{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af375836-b870-458b-87d1-4e00565977eb",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-07-11T15:36:31.385695Z",
     "iopub.status.busy": "2024-07-11T15:36:31.385368Z",
     "iopub.status.idle": "2024-07-11T15:36:31.389515Z",
     "shell.execute_reply": "2024-07-11T15:36:31.388754Z",
     "shell.execute_reply.started": "2024-07-11T15:36:31.385660Z"
    },
    "id": "af375836-b870-458b-87d1-4e00565977eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install -U langchain langchain_community pypdf sentence_transformers chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e-Oy-FOJgB_A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e-Oy-FOJgB_A",
    "outputId": "5512085a-528b-4778-fc63-50bf0b82d2be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\gold-yolo\\lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n",
      "d:\\Anaconda\\envs\\gold-yolo\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "langchain                     0.2.8\n",
      "langchain_community           0.2.7\n",
      "pypdf                         4.3.0\n",
      "sentence_transformers         3.1.0\n",
      "chromadb                      0.4.15\n"
     ]
    }
   ],
   "source": [
    "import langchain, langchain_community, pypdf, sentence_transformers, chromadb\n",
    "\n",
    "for module in (langchain, langchain_community, pypdf, sentence_transformers, chromadb):\n",
    "    print(f\"{module.__name__:<30}{module.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2c72b8-ee12-4130-af88-699998aa230c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:03.664713Z",
     "iopub.status.busy": "2024-07-21T13:22:03.664452Z",
     "iopub.status.idle": "2024-07-21T13:22:04.061711Z",
     "shell.execute_reply": "2024-07-21T13:22:04.061242Z",
     "shell.execute_reply.started": "2024-07-21T13:22:03.664688Z"
    },
    "id": "1e2c72b8-ee12-4130-af88-699998aa230c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "841d2b02-ad06-40d2-b11f-c7adccec6ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:04.062389Z",
     "iopub.status.busy": "2024-07-21T13:22:04.062201Z",
     "iopub.status.idle": "2024-07-21T13:22:04.064861Z",
     "shell.execute_reply": "2024-07-21T13:22:04.064565Z",
     "shell.execute_reply.started": "2024-07-21T13:22:04.062376Z"
    },
    "id": "841d2b02-ad06-40d2-b11f-c7adccec6ca2"
   },
   "outputs": [],
   "source": [
    "# Â¶ÇÊûúÂ∑≤Áªè‰∏ãËΩΩÂà∞Êú¨Âú∞ÔºåÂèØ‰ª•ÊõøÊç¢‰∏∫Êú¨Âú∞Ë∑ØÂæÑ\n",
    "# EMBEDDING_MODEL_PATH = 'BAAI/bge-large-zh-v1.5'\n",
    "EMBEDDING_MODEL_PATH = 'BAAI/bge-large-en-v1.5'\n",
    "dt = '20240929'\n",
    "version = 'baseline'\n",
    "\n",
    "# output_dir = os.path.join('my_outputs', f'{version}_{dt}')\n",
    "output_dir = r''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e81e3-4c82-4842-aef5-7592caaf1d39",
   "metadata": {
    "id": "cf7e81e3-4c82-4842-aef5-7592caaf1d39"
   },
   "source": [
    "# ÊñáÊ°£Â§ÑÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da15f02e-3131-43fb-81c5-f4da615c449b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:04.065430Z",
     "iopub.status.busy": "2024-07-21T13:22:04.065320Z",
     "iopub.status.idle": "2024-07-21T13:22:04.080425Z",
     "shell.execute_reply": "2024-07-21T13:22:04.079979Z",
     "shell.execute_reply.started": "2024-07-21T13:22:04.065419Z"
    },
    "id": "da15f02e-3131-43fb-81c5-f4da615c449b"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0PNdWJAALMni",
   "metadata": {
    "id": "0PNdWJAALMni"
   },
   "source": [
    "## ÊñáÊ°£Âä†ËΩΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6920e29-bc7d-4635-be06-d151eaf0e100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:04.081018Z",
     "iopub.status.busy": "2024-07-21T13:22:04.080903Z",
     "iopub.status.idle": "2024-07-21T13:22:05.275067Z",
     "shell.execute_reply": "2024-07-21T13:22:05.274519Z",
     "shell.execute_reply.started": "2024-07-21T13:22:04.081006Z"
    },
    "id": "e6920e29-bc7d-4635-be06-d151eaf0e100"
   },
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"data_test/GeoChat.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03f6f2a8-d1d9-42ad-9eee-cd5fe74c312f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:05.275745Z",
     "iopub.status.busy": "2024-07-21T13:22:05.275596Z",
     "iopub.status.idle": "2024-07-21T13:22:05.377362Z",
     "shell.execute_reply": "2024-07-21T13:22:05.376902Z",
     "shell.execute_reply.started": "2024-07-21T13:22:05.275733Z"
    },
    "id": "03f6f2a8-d1d9-42ad-9eee-cd5fe74c312f"
   },
   "outputs": [],
   "source": [
    "qa_df = pd.read_excel(os.path.join(output_dir, 'question_answer.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ec659-4ad7-4e1f-b1ea-3477bf97fde3",
   "metadata": {
    "id": "841ec659-4ad7-4e1f-b1ea-3477bf97fde3"
   },
   "source": [
    "## ÊñáÊ°£ÂàáÂàÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74fe856a-7c19-4c3c-bb30-7abfa6298f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:05.378932Z",
     "iopub.status.busy": "2024-07-21T13:22:05.378773Z",
     "iopub.status.idle": "2024-07-21T13:22:05.381649Z",
     "shell.execute_reply": "2024-07-21T13:22:05.381345Z",
     "shell.execute_reply.started": "2024-07-21T13:22:05.378919Z"
    },
    "id": "74fe856a-7c19-4c3c-bb30-7abfa6298f74"
   },
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_docs(documents, filepath, chunk_size=400, chunk_overlap=40, seperators=['\\n'], force_split=False):\n",
    "    if os.path.exists(filepath) and not force_split:\n",
    "        print('found cache, restoring...')\n",
    "        return pickle.load(open(filepath, 'rb'))\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=seperators\n",
    "    )\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "    for chunk in split_docs:\n",
    "        chunk.metadata['uuid'] = str(uuid4())\n",
    "\n",
    "    pickle.dump(split_docs, open(filepath, 'wb'))\n",
    "\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fh0N6CwXe-Ho",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fh0N6CwXe-Ho",
    "outputId": "1f1d5b00-f1cc-451b-827c-f834a59191b3"
   },
   "outputs": [],
   "source": [
    "# splitted_docs = split_docs(documents, os.path.join(output_dir, 'split_docs.pkl'), chunk_size=500, chunk_overlap=50)\n",
    "splitted_docs = split_docs(documents, os.path.join(output_dir, 'qqq.pkl'), chunk_size=500, chunk_overlap=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10019450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': '805948dc-9161-4357-b2b7-bb88784386f5'}, page_content='GeoChat\\n : Grounded Large Vision-Language Model for Remote Sensing\\nKartik Kuckreja1, 2* Muhammad Sohail Danish1*Muzammal Naseer1\\nAbhijit Das2Salman Khan1, 3Fahad Shahbaz Khan1, 4\\n1Mohamed bin Zayed University of AI,2Birla Institute of Technology & Science, Hyderabad\\n3Australian National University,4Link ¬®oping University\\nkartik.kuckreja@mbzuai.ac.ae, muhammad.sohail@mbzuai.ac.ae\\nAbstract\\nRecent advancements in Large Vision-Language Mod-\\nels (VLMs) have shown great promise in natural image do-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': 'c2fc0cac-d021-4507-aa26-3d023e74b5e1'}, page_content='mains, allowing users to hold a dialogue about given vi-\\nsual content. However, such general-domain VLMs perform\\npoorly for Remote Sensing (RS) scenarios, leading to inac-\\ncurate or fabricated information when presented with RS\\ndomain-specific queries. Such a behavior emerges due to\\nthe unique challenges introduced by RS imagery. For exam-\\nple, to handle high-resolution RS imagery with diverse scale\\nchanges across categories and many small objects, region-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': '37163d64-7378-4168-9998-b89240a20c84'}, page_content='level reasoning is necessary alongside holistic scene inter-\\npretation. Furthermore, the lack of domain-specific mul-\\ntimodal instruction following data as well as strong back-\\nbone models for RS make it hard for the models to align\\ntheir behavior with user queries. To address these lim-\\nitations, we propose GeoChat - the first versatile remote\\nsensing VLM that offers multitask conversational capabil-\\nities with high-resolution RS images. Specifically, GeoChat'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': '424985f5-1269-46d5-8038-0917fe4b4067'}, page_content='can not only answer image-level queries but also accepts\\nregion inputs to hold region-specific dialogue. Further-\\nmore, it can visually ground objects in its responses by re-\\nferring to their spatial coordinates. To address the lack\\nof domain-specific datasets, we generate a novel RS mul-\\ntimodal instruction-following dataset by extending image-\\ntext pairs from existing diverse RS datasets. We establish\\na comprehensive benchmark for RS multitask conversations'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': '8d02ff70-fd24-4093-89d1-307e84228ab3'}, page_content='and compare with a number of baseline methods. GeoChat\\ndemonstrates robust zero-shot performance on various RS\\ntasks, e.g., image and region captioning, visual question\\nanswering, scene classification, visually grounded conver-\\nsations and referring detection. Our code is available here.\\n1. Introduction\\nIn the natural image domain, the abundance of aligned\\nimage-text data sourced from web imagery or manual anno-\\n*Equally contributing first authors.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': '25ecb74a-918e-49ef-9502-04ae84911b10'}, page_content='\\nGrounded Image CaptioningUSER:Describetheimageindetail.ASSISTANT:Theimagedepictsascenewithmultiplebuildings,houses,trees,andsmallvehicles.Thereare10tenniscourtsvisibleintheimage‚Ä¶..Visual QAUSER:Howmanytenniscourtsarevisible?ASSISTANT:10tenniscourtsarepresentintheimage.Region Based CaptioningUSER:Whatistheobjectpresenthere?ASSISTANT:Awhitesmallcar.Multi-turn ConversationUSER:Howmanytenniscourtsarethereintheimage?ASSISTANT:‚Ä¶‚Ä¶‚Ä¶‚Ä¶..USER:Canyoudescribethesurroundingsofthetenniscourts?ASSISTANT:Thetenniscourtsaresurroundedbymultiplebuildingsandhouses.Therearealsocarsparkednearby.USER:‚Ä¶‚Ä¶ASSISTANT:‚Ä¶..Referring ExpressionUSER:Tenniscourtatthebottomleft.ASSISTANT:Tenniscourtatthebottomleft.Scene ClassificationUSER:Isthisaruraloranurbanarea?ASSISTANT:Thisisanaerialviewofanurbanarea.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': '33890465-ae6c-42b4-85cf-b5d2e06849fc'}, page_content='Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': '30143afb-0d70-4766-87d8-f491ba2e7077'}, page_content='tom). This makes it the first RS VLM with grounding capability.\\ntations facilitate effective self-supervised vision-language\\nmodeling, as demonstrated by multimodal GPT-4 [23] and\\nopen-source initiatives like LLaV A [19]. These vision-\\nlanguage models (VLMs), developed through generative\\npretraining and instruction-tuning, exhibit robust zero-shot\\ntask completion across various user-oriented multimodal\\ntasks. The resulting capabilities open the door to the de-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 0, 'uuid': 'a6e3e714-66ad-463c-a716-cbbeec985ebb'}, page_content='velopment of versatile multimodal conversational assistants\\nwith broad applications in real-world scenarios [12].\\nHowever, general-domain VLMs designed for natural\\nimages, exhibit poor performance when presented with re-\\n1\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n27831'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'd71dac09-bdc9-4f37-8e0c-75306aff5dbf'}, page_content='motely sensed visual imagery. The performance disparity\\narises primarily from the distinct nature of content found\\nin remote sensing image-text pairings compared to the pub-\\nlicly available web data. As a result, general-domain VLMs\\ncan provide inaccurate information or hallucinate when pre-\\nsented with spatial images from RS sensors. Although there\\nhas been significant progress in the field of remote sensing\\nvisual question answering (VQA) [39, 41], earlier meth-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': '227dfebb-21ab-41c2-9236-45b06554f901'}, page_content='ods have framed the task as a classification problem. Here,\\nthe model chooses answers from predetermined responses\\nfound in the training data. It limits their applicability to\\nopen-ended answer generation and instruction-following.\\nIn this paper, we introduce GeoChat, an attempt to\\nextend multimodal instruction-tuning to the remote sens-\\ning domain for training a multitask conversational assis-\\ntant. However, remote-sensing domain lacks a multimodal'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': '2a9bf5a3-1ce6-4fa5-9005-7fdbba289f9c'}, page_content='instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'fb598345-793b-48e1-9800-48353185955b'}, page_content='cation [5] and SAMRS for object detection [31].\\nA crucial capability of GeoChat is the unification of mul-\\ntiple image and region-level reasoning tasks for RS imagery\\nwithin a single pipeline (see Fig. 1). We achieve this via\\ndistinct task tokens that help suitably direct the model‚Äôs re-\\nsponses according to user requirements. In addition, the\\nmodel uses spatial location representations in its inputs to\\nseamlessly reason about local regions and can also generate'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'bd6a14fa-35c8-4e2f-8fac-d1f47db3a818'}, page_content='object locations in its responses to visually ground objects.\\nThis enables a diverse set of tasks possible with GeoChat\\nincluding referring expression detection, image/region cap-\\ntioning, scene classification, natural language conversations\\nand VQA, besides visually grounded conversations.\\nIn summary, this work has the following contributions:\\n‚Ä¢RS multimodal instruction following dataset . We present\\na novel data generation pipeline, to leverage existing ob-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'b067adf7-31fb-496f-924b-e2806dd788cb'}, page_content='ject detection dataset [31] to create short descriptions of\\nthe images, followed by using Vicuna-v1.5 [7] to cre-\\nate conversations using the generated text alone. Further,\\nwe add visual question-answering and scene classification\\nabilities using their corresponding datasets [5, 20]. This\\nresults in a total of 318kinstruction pairs for RS domain.\\n‚Ä¢GeoChat . Leveraging our dataset, we finetune LLaV A-\\n1.5 [14] to create the remote sensing-domain vision-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': '9ffea684-e225-4c78-bfa6-80d144a39bb7'}, page_content='language model - GeoChat. Our LoRA [11] fine-tuning is\\nefficient and avoids forgetting the necessary context em-\\nbedded in fully-tuned LLaV A model, whose MLP pro-\\njection is trained to align images into the word embed-\\nding space of the LLM (Vicuna-v1.5 [7]). This allowsGeoChat to retain the conversation and instruction follow-\\ning abilities of LLaV A and extend its domain-knowledge\\nto remote sensing tasks.\\n‚Ä¢ We also address the lack of evaluation benchmarks to as-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'd9dc2c4d-0a58-42ca-a2bd-874acc22baea'}, page_content='sess the capability of existing VLMs on remote-sensing\\nconversations. To this end, we setup evaluation proto-\\ncols for conversation grounding in RS, as well as a setup\\na suite of tasks to allow comparisons with future efforts\\nin this direction. We show various supervised as well as\\nzero-shot evaluations for different remote sensing tasks,\\nincluding image captioning, visual question answering\\nand scene classification to demonstrate the generalisabil-\\nity of GeoChat conversational VLM.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'cf678153-859c-4df6-8b0a-351d610126f6'}, page_content='ity of GeoChat conversational VLM.\\n2. Related Work\\nLarge Vision-Language Models. The typical architecture\\nof instruction-following Vision Language Models (VLMs)\\nconsists of utilising a pre-trained visual backbone[9] to en-\\ncode visual data, a large language model [7] for interpreting\\nuser instructions and generating responses, and a vision-\\nlanguage cross-modal connector, e.g., a linear projection\\nlayer [18, 42] or an MLP [17], for fusing visual information'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'f4a6cb93-c6de-4973-8c54-53ab68d4b951'}, page_content='with language models. The results achieved with VLMs\\nshow great promise; for example, LLaV A [18], Instruct-\\nBLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable\\ngains in language instruction following and visual reasoning\\nability for natural scenes. More recent studies have shown\\nthat these models can be adapted to other domains such as\\nvideos [22], biomedical [14, 30] and remote sensing [12].\\nRemote Sensing VLMs. The application of generalized'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': '6f43958e-0330-4304-8473-93794cd37bc6'}, page_content='VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': '094e8a64-c881-4f68-a1b2-45f426f4d9b9'}, page_content='age retrieval, captioning images, and answering visual ques-\\ntions that call for both visual and linguistic knowledge. Al-\\nthough there has been progress in vision language models\\nfor remote sensing tasks, such as image captioning [43],\\nzero-shot classification [16] and visual question answering\\n[3, 39], these models can only perform a specific task they\\nare trained for, lack conversational capability and do not\\npossess generic semantic knowledge about the remote sens-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 1, 'uuid': 'd22efc72-1bb0-4ea2-932f-f35dbc2eaa31'}, page_content='ing images. A major gap exists in the remote sensing do-\\nmain towards developing general-purpose models to solve\\nall tasks together, while also maintaining conversation abil-\\nities. While RSGPT [12] is an initial effort that has shown\\ngood conversation ability along with solving multiple tasks,\\nit requires finetuning the model for each task separately,\\nwhich makes it cumbersome and not generalizable. Fur-\\n2\\n27832'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': 'e85ef725-1e7f-4182-8e79-d0ac232609a2'}, page_content='ther, RSGPT cannot work for region-level reasoning or vi-\\nsual grounding, which our work aims to address.\\n3. GeoChat: Grounded Remote Sensing VLM\\nVisually grounded conversations for remote sensing aim to\\ngenerate textual responses interleaved with corresponding\\nobject locations. Further, a user can also provide visual\\nprompts (e.g., a bounding box) besides natural language\\nquestions, and the model should be able to answer questions\\nabout the specified Region of Interest (RoI). Such seamless'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': '106df0eb-bfd9-4a85-98f4-06ea19411584'}, page_content='interplay between visual and language modalities necessi-\\ntate a deep comprehension of linguistic constructions that\\ndenote particular objects or elements in a visual scene.\\nAs mentioned above, GeoChat is the first model capable\\nof holding visually grounded conversations about remotely\\nsensed images. By construction, GeoChat can address not\\nonly the challenging task of visually grounded conversa-\\ntions, but can also perform a spectrum of other spatial rea-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': '637f112b-2a46-4628-99b8-3a74920123ee'}, page_content='soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': '01501c61-faf9-4056-aa0a-24f910e2c104'}, page_content='The goal is to perform conversation-based tasks at a holistic\\nlevel with image-wide context, such as visual question an-\\nswering (VQA), scene classification and image captioning.\\nb) Region-Level Conversation Tasks. This task in-\\nvolves providing spatial box locations bin the input to\\nGeoChat besides xandq. Region locations bguide the\\nmodel‚Äôs attention to specific regions within the image, so\\nthat the model can perform tasks such as region-level cap-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': '2ab3ee6d-aefa-43ec-8e0c-fa4a3a4ef40c'}, page_content='tioning, region-specific VQA or multi-turn conversation.\\nc) Grounded Conversation Tasks. With the use of spe-\\ncial tokens, termed as task-specification tokens t, GeoChat\\ncan be guided to provide object locations at different gran-\\nularities, while maintaining conversation abilities. It helps\\nin tasks including grounded image captioning/conversation,\\nobject grounding and referring expression detection.\\n3.1. GeoChat Architecture\\nGeoChat follows the architecture as of LLaV A-v1.5 [17],'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': 'd329ad38-5f67-425c-9d12-82eadad757e9'}, page_content='which consists of three core components, i) Global Im-\\nage encoder, ii) an MLP adaptor (two linear layers) and\\niii) LLM. Different to LLaV A, we add specific task prompt\\nthat indicates the type of task desired from the model i.e.,\\ngrounding, image-level or region-level conversations. Ad-\\nditionally, we allow spatial positions within both inputs and\\noutputs, enabling visual prompts as inputs and grounded\\nobjects in GeoChat outputs. Notably, the original LLaV A'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': 'dbeac12f-0a8e-40fe-84a1-d93ab3d8464b'}, page_content='model cannot perform object grounding or accept regionData Size Response formatting prompts\\nDetailed Description 30k Describe the image in detail.\\nMulti-Round Conversation 65k -\\nComplex Questions 10k -\\nRSVQA-LRBEN[20] 56k Answer the question using a single word or phrase.\\nNWPU-RESISC-45[5] 31.5k\\nFloodnet[25] 4k\\nGrounding Description 25k [grounding] Describe the image in detail.\\nRegion Captioning 40k [identify] {bxleft, bytop, bxright, bybottom|Œ∏}'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': 'a82c1eab-01fd-422b-abcc-fc5b1215153d'}, page_content='Referring Expression 45k [refer] < p > Object < /p >\\nTable 1. Instruction following data used to train GeoChat. Instruc-\\ntion types and format are shown. We use a 308k set for training\\nand a separate 10k instruction-set for testing.\\ninputs. Further, the original LLaV A can not reason about\\nremote sensing images which is enabled via our domain-\\nspecific dataset. We describe each component in the archi-\\ntecture as follows:\\nTask Token: The unique quality of GeoChat is its abil-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': '054fbe4c-1588-4a11-bea9-ec7edfe70ded'}, page_content='ity to easily switch between different types of remote sens-\\ning visual interpretation tasks. To eliminate uncertainty\\namong tasks, our approach assigns a unique task identifi-\\ncation to each one. We suggest three distinct task identities,\\nt‚àà{grounding, identify, refer }, each for grounded con-\\nversations, region captioning and referring expression com-\\nprehension. As for the case of visual question answering\\nand scene classification, we directly ask the model to out-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': '042843e2-cc9b-4679-b2f8-4efd552fbd0d'}, page_content='put the answer in a single word or phrase, as shown in Ta-\\nble 1. Our approach does not employ any task identifica-\\ntion tokens for vision-irrelevant commands. This unified\\napproach is supported by a modular design that efficiently\\nintegrates spatial data, giving the model flexibility in its rea-\\nsoning about visual content.\\nSpatial Location Representation. Our model must pre-\\ncisely identify the spatial position of the referenced items'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': 'cb067fb7-ef34-4ea3-93b8-81f9947b1dd7'}, page_content='for tasks such as grounded conversations, referring expres-\\nsion generation, and comprehension. To this end, we repre-\\nsent the box locations in a textual format to express the ge-\\nographical position: b={bxleft, bytop, bxright, bybottom|Œ∏}.\\nHere, bxleft, bytopdenote the top left corner point of box\\nwhile the bxright, bybottom represent the bottom right corner\\ncoordinates. The angle Œ∏represents the angle of rotation for\\nthe bounding box, from the lower edge. Numerical values'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': 'faa82d57-2e4c-4ade-8839-63cd3c9bb14f'}, page_content='normalised within the interval [0, 100] are used to represent\\nthe x and y coordinates. Region locations in this format are\\nused to interact with the model via its inputs and outputs.\\nVisual Backbone. GeoChat adapts the pretrained vision\\nbackbone of CLIP-ViT(L-14) [28], which has an input res-\\nolution of 336 √ó336. This results in effectively 576 patches\\nper image. Since this resolution is not sufficient to un-\\nderstand details presented in remote sensing imagery (e.g.,'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 2, 'uuid': '7a2d75df-ef26-44aa-929a-249cd555b9d1'}, page_content='small objects and object details), we interpolate the posi-\\ntional encoding in the transformer-based CLIP [28] model\\nto scale with input image sizes of 504 √ó504. Although this\\n3\\n27833'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': '52d0e8b2-9a45-40ac-aaa7-b845fe0decfd'}, page_content='Large Language Model\\nüî•MLP Adaptor \\n‚ùÑCLIP-ViT\\n‚ùÑ\\n<im_end> [task specific prompts] <im_start>[USER]: [identify] What is present in this area?[USER]:[refer] Football ground on the right side, surrounded by trees.\\nReferring Expression\\nImage Description\\nRegion  captioning\\nVisual QA\\nSure!'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': 'e5e75933-606c-46ad-88e1-8f30cbf6ab17'}, page_content='Region  captioning\\nVisual QA\\nSure!\\nA basketball field at the bottom of the image.[USER]:Can you please describe the image in detail?The image is an aerial view of a sports complex, with twofootball grounds at the center and one basketball ground at the bottom. The football grounds are surrounded by trees‚Ä¶‚Ä¶.[USER]: How many football grounds and basketball courts are present in the image?\\nThere are two football grounds and one basketball ground present.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': '94b6816d-3791-43e5-a3e3-6a3068a63c8c'}, page_content=\"<System Message>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to thehuman's questions.. Figure 2. An overview of GeoChat - the first grounded large vision-language model for remote sensing. Given an image input together\\nwith a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\"),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': 'b4e28466-3582-4ae6-b986-09e7589604da'}, page_content='A multi-layer perceptron (MLP) is used to adapt vision-tokens to language space suitable for input to a Large Language Model (Vicuna\\n1.5). Besides visual inputs, region locations can also be input to the model together with task-specific prompts that specify the desired task\\nrequired by the user. Given this context, the LLM can generate natural language responses interleaved with corresponding object locations.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': 'ef7dfbfb-f407-4baa-a301-34a23b37e63b'}, page_content='GeoChat can perform multiple tasks as shown on top e.g., scene classification, image/region captioning, VQA and grounded conversations.\\nleads to an increase in the number of patches to almost dou-\\nble (i.e., 1296 per image), this enhanced resolution allows\\nus to handle larger image sizes and also supports better vi-\\nsual grounding in high-resolution RS images.\\nMLP Cross-modal Adaptor. From the frozen CLIP-\\nViT[28], we project the output tokens ( ‚ààR1296√ó1024) with'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': 'ffb10c2c-e465-4e41-9e07-279e85b62f9b'}, page_content='dimensions 1024 onto the language model space, using an\\nMLP adaptor with one hidden layer. The adaptor has an\\ninput dimensionality of 1024 and outputs a vector of size\\n4096, corresponding to the input size of the LLM [7]. A\\nGeLU [10] is used as the activation function.\\nLarge Language Model. The open source Vicuna-\\nv1.5(7B) [7] large language model is utilised as the founda-\\ntion for GeoChat. The language model functions as a single\\ninterface for diverse vision-language inputs in our frame-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': 'd0451443-62fd-49f5-95b2-a0dc418d861f'}, page_content='work. To accomplish different vision-language tasks, we\\ndirectly depend on the Vicuna-v1.5(7B) [7] language to-\\nkens. We explicitly interact with the language model to\\nconstruct textual representations of bounding boxes to ex-\\npress their spatial coordinates for the visual grounding tasks\\nthat require the production of spatial locations. Similarly,\\nthe safe, aligned and effective behavior of LLM is ensured\\nvia system prompts appended together with given inputs. A'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': '7d8f01da-3650-484f-a4b5-400f5cf48664'}, page_content='[USER] <im_start> Image Features <im_end> [Task Identifier] [ASSISTANT]Figure 3. Multi-task instruction template for GeoChat.\\nLow-Rank Adaptation (LoRA) [11] based strategy is used\\nfor fine-tuning the LLM. While training, instead of finetun-\\ning all of the weights that comprise the weight matrix of the\\npre-trained Vicuna-v1.5[7], we finetune two smaller matri-\\nces in LoRA [11] that approximate the original larger ma-\\ntrix. After that, the fine-tuned adaptor is fed into the pre-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': 'bfede987-8b5e-41d1-b378-3c4d9c7776ef'}, page_content='trained model and utilised for inference. The LoRA adap-\\ntation ensures faster training and avoids forgetting original\\nknowledge embedded in the LLM trained and fine-tuned on\\ngeneric natural language instructions. This is an important\\nfeature since it allows the model to bring in external con-\\ntext about generic object types, landmarks and affordances\\nin the remote-sensing reasoning framework of GeoChat.\\n3.2. Training Details\\nTo enhance the effectiveness of our model on general visual'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 3, 'uuid': '2a86211b-46d4-47bd-a3f7-665a8de20246'}, page_content='tasks and optimize training efficiency, we employ a strat-\\negy that involves initializing the network with pre-trained\\nweights and fine-tuning specific segments for remote sens-\\n4\\n27834'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': 'a5a5f5ef-a0a5-40f1-80d3-efcf925cfed4'}, page_content='MultiTurnConversationUSER:Whereisthewhiteshippresent?ASSISTANT:Whiteshipispresentatthetopleft.USER:Whatisthecoloroftheshipanchoredattheharbor?ASSISTANT:Theshipanchoredattheharborisgreyincolor.Complex ReasoningUSER:Whatdoesthepresenceof4harborsand2shipsindicate?ASSISTANT:Thepresenceof4harborsand2shipsconstituteabustlingmaritimeenvironment.Thepresenceofsuchharborsmayindicateawaterareawiththepossibilityofexport/importsofgoodsaswellastourism.Detailed DescriptionUSER:PleaseDescribetheimageindetail.ASSISTANT:Thissatelliteimageshowsdepictsariverscenewithmultipleshipsandharborspresent.Thereareatotaloftwoshipsandfourharbors.Thegreyshipatthebottomrightisanchoredtotheharbor‚Ä¶‚Ä¶‚Ä¶..Grounded DescriptionUSER:[grounding]Describetheimageindetail.ASSISTANT:Thissatelliteimagedepictsagreyshipanchoredatalargeharbor.Thereisanotherharboratthetopright,whichisadjacenttosometrees.Thereisanotherwhiteshipclosetoaharborattheright.LLM'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': '712afef9-3566-474b-bf17-675d58097d79'}, page_content='\\nObject Attributes and Relationships1.Ship{white,small,top-left},{grey,small,bottom-right}2.Harbor{small,right},{large,bottom-left},{small,topright},{small,left}3.Trees{top-right}.Referring Expression1.SmallGreyshipanchoredatlargeharboronbottom-right2.SomeTreesatthetop-right.3.Whiteshipatthetopofasmallharborontheright.Region Captioning1.Objectlocatedat{}.2.Sizeofobjectat.{}issmall.Figure 4. Types of annotations available in the GeoChat instruction-set. Top-row: For a given RS image, we obtain object attribute and'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': '3ec8d1fa-5d9e-46d4-9c4f-4eaa5b4bfc15'}, page_content='relationship information, referring expressions and region captions along with their corresponding region annotations (shown over the\\nimage). Bottom-row: This structured information is used to create the rich instruction-set with a total of 318k image-instruction pairs.\\ning related tasks. We use a pre-trained CLIP-ViT(L-14)\\nencoder[28],trained on large amounts of textual and visual\\ndata, a pretrained MLP adaptor[17], pretrained on a 558K\\nsubset of the LAION-CC-SBU [26] dataset with BLIP [15]'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': 'ea517f2f-e86b-47c5-916a-437af825d149'}, page_content='captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': 'c83dbd1d-fbe4-4d51-9579-52501c8958a6'}, page_content='cally provide system instructions as prompts that ask Vi-\\ncuna [7] to generate multi-round question and answer pairs\\nin a manner as if it could visualize the image (although it\\nonly has access to the text). This is achieved by providing\\nfew-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied. Specifically, from our short descriptions'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': '0286b743-2dfa-4d3c-a394-cb4a3a5acec5'}, page_content='created using the below pipeline, we randomly sample 65k\\nimages to create multi-round conversations, 10k images to\\ngenerate complex question answers and 30k images to gen-\\nerate detailed descriptions for the given short descriptions.\\nIn combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs\\nfor training and 10k for testing. Next, we outline the\\ninstruction-set creation process.\\nDataset Category # Classes # Images Image Size'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': 'b3ff2467-9a43-49e1-ab85-bda122797ba6'}, page_content='Dataset Category # Classes # Images Image Size\\nDOTA Object Detection 18 17,480 1024 √ó1024\\nDIOR Object Detection 20 23,463 800 √ó800\\nFAIR1M Object Detection 37 64,147 600 √ó600\\nLRBEN(rsvqa) Visual Question Answering - 600 256 √ó256\\nFloodnet Visual Question Answering - 4056 3000 √ó4000\\nNWPU-RESISC-45 Scene Classification 45 31,500 256 √ó256\\nTable 2. List of datasets used to creat our remote-sensing instruc-\\ntion set for GeoChat VLM training. We include object detection,'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': '0125040e-62c0-42d3-81cd-7e171b76480e'}, page_content='visual question answering and scene classification datasets with\\nvarying image sizes and types of classes to ensure diversity.\\nConstituent Datasets: In the compilation of our in-\\nstruction set, we incorporate three distinct types of datasets,\\nencompassing the ones designed for object detection,\\nscene classification, and visual question answering (VQA).\\nSpecifically, we integrate three object detection (DOTA\\n[35], DIOR [6], and FAIR1M [27] which together form'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 4, 'uuid': 'c4ac6f83-bcba-4a7f-b19e-7766a3de786a'}, page_content='the SAMRS [31] dataset), one scene classification (NWPU-\\nRESISC-45 [5]), one VQA (LRBEN[20]), and one flood\\ndetection [25] VQA dataset (see Table 2). The object detec-\\ntion datasets allow region-level reasoning capability as they\\noffer segmentation masks along with bounding boxes.\\nAddition of Missing Classes: Although a wide vari-\\nety of object classes are included in the object detection\\ndatabases, several essential categories like buildings, roads,\\n5\\n27835'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': 'ccc1f456-e73f-42f3-a160-ffb0acecc545'}, page_content='Give bounding box for the Boeing 747 at the bottom left.\\nSure!Inthesatelliteimage,therearefourbaseballfieldslocatednearsomebuildingsatthecenterofthescene.Thebaseballfieldsarearrangedinacloseproximitytoeachother.\\n[grounding] Please describe the image in detail.\\nYes, the road at the center is completely flooded.\\nIs the whole road flooded?\\nHow many buildings are flooded?Three buildings are flooded and are present at the top left.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': '2d4b64d7-8302-4f94-b5e3-1f321a10c54c'}, page_content='Figure 5. Qualitative results of GeoChat. ( left-right ) Results are shown on grounding, referring object detection, and disaster/damage\\ndetection. The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior. The\\nmodel can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left).'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': 'eb304208-b52b-4226-874c-01a2e3a10f36'}, page_content='The model can also specify object types, object counts, object attributes and object relationships.\\nAttribute Example\\na1 category (e.g. ‚Äúplane, ship‚Äù)\\na2 color (e.g. ‚Äúgray, white‚Äù)\\na3 relative size (e.g. ‚Äúsmall, large‚Äù)\\na4 relative location (e.g. ‚Äútop right, bottom‚Äù)\\na5 relation (e.g. ‚Äúparked at, driving through‚Äù)\\nTable 3. List of attributes collected for objects. Attributes are used\\nto obtain referring expressions e.g., small-sized plane to the left.Categories Example'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': '1c477cd0-505e-4fc2-9341-925366306d0a'}, page_content='Ships and Harbors (e.g. ‚Äúanchored at, parked at‚Äù)\\nTrack Field and Soccer Field (e.g. ‚ÄúSurrounded by, Inside‚Äù)\\nVehicles, Bridge, Road, Roundabout (e.g. ‚Äúpassing through, passing through‚Äù)\\nVehicles and Building (e.g. ‚Äúparked‚Äù)\\nAirport and Plane (e.g. ‚Äúparked‚Äù)\\nShip and Helipad (e.g. ‚Äúon, contains‚Äù)\\nTable 4. Example of relationships between different objects used in\\nthe proposed instruction dataset.\\nand trees are missing. To address this, we propose to uti-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': 'a9b32226-617f-4e75-a77f-c6f40f31acaa'}, page_content='lize ViTAE-RVSA [32] model, pre-trained on the LoveDA\\ndataset [33], which encompasses the required important\\nclasses. The model [32] is used to infer these classes on\\nthe SAMRS [31] dataset, yielding pseudo labels. To miti-\\ngate potential noise in these predictions, we remove the pre-\\ndictions of ViTAE-RVSA [32] for which we already have\\nground truth from the SAMRS [31] dataset to refine the re-\\nsults.\\nAttribute extraction: For referring expression annota-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': '11d94f3e-6b27-469b-8f2f-a394461b19d8'}, page_content='tions, it is important to derive a variety of attributes in RS\\nimages. To this end, we have selected five distinct types of\\nattributes, as outlined in Table 3. Object category informa-\\ntion can be directly obtained from the SAMRS dataset. For\\ncolor extraction, we use the K-Means clustering algorithm.\\nSpecifically, we extract the object‚Äôs pixels from the image\\nusing ground-truth box and cluster them into Kgroups. The\\ncenter of the largest cluster is then selected as the object‚Äôs'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': 'df69d168-4c98-4b1f-8eb0-5bf903d2049f'}, page_content='color. To specify the relative size of the object, we catego-\\nrize objects into three sizes: small, normal, and large. This\\ncategorization is determined by measuring the area of all in-stances of a class in the entire dataset and assigning the 80th\\npercentile as the large label. Similarly, the 20thpercentile\\nis designated as small size, with the remaining falling into\\nthe normal category. To determine the object‚Äôs relative po-\\nsition within the images, we partition the entire image into'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': 'ceb5217e-a212-40ce-acc9-8fb6645d7f58'}, page_content='a 3√ó3 grid, defining regions such as Top Right, Top, Top\\nLeft, Left, Center, Right, Bottom Right, Bottom Left, and\\nBottom. Based on the object‚Äôs center pixel coordinates, we\\nassign its relative position accordingly.\\nTo define the relation between objects in a given image,\\nwe group different objects based on their distance between\\nthe bounding boxes, and for each sub-graph, we assign dif-\\nferent relationships between objects based on their class la-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 5, 'uuid': '2974c710-db73-4a39-9600-38bec3b8a782'}, page_content='bels. Table 4 presents various examples of object relation-\\nships. To establish relationships like ‚Äúsurrounded by,‚Äù we\\ncross-reference pixel-level coordinates to verify if one ob-\\nject is entirely contained within another object.\\nExpression Generation: To emulate natural language\\nexpressions, we employ predefined textual templates based\\non [40]. The phrase template encompasses the attributes\\n{a1, . . . , a5 }from Table 3. The expression for a group of\\n6\\n27836'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '6ee0362f-2585-43ad-89b9-5d78d19b9576'}, page_content='Model UCMerced AID\\nQwen-VL [1] 62.90 52.60\\nMiniGPTv2 [4] 4.76 12.90\\nLLaV A-1.5 [17] 68.00 51.00\\nGeoChat 84.43 72.03\\nTable 5. Zero-shot scene classification accuracy comparison on\\nAID [34] and UCMerced [36] datasets. In comparison to other\\ngeneric VLMs, GeoChat performs favorably well.\\nobjects of the same class is formulated as:\\n‚ÄùThe/A ‚ü®a3‚ü© ‚ü®a2‚ü©a1‚ü®in/on the a4‚ü©.‚Äù\\nAttributes that may be absent are enclosed in ‚å©‚å™, and at-\\ntributes {a2, a3}can be arranged in any sequence.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': 'c6f26a2f-9aed-48ad-aa0d-6a7a2ef8f0cd'}, page_content='tributes {a2, a3}can be arranged in any sequence.\\nSimilarly, the sentence template incorporates the rela-\\ntional attributes a5 to establish connections between two\\nobjects through this structure:\\n‚ÄùThe/A ‚ü®ai3‚ü© ‚ü®ai2‚ü©ai1ai5aj1‚ü®in/on the aj4‚ü©.‚Äù\\nHere, the indicies iandjrepresent the ithandjthobject.\\nVisual Grounding: Although referring expression\\ndatasets are available in the natural image domain [37, 38],\\nthey lack for the remote sensing domain. To this end, we'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '56288e4f-caba-4fca-9af7-c6518459ae87'}, page_content='use our short descriptions as referring expressions to cre-\\nate three different kinds of question answering pairs, i.e.\\ngrounding image description, referring expression, and re-\\ngion level captioning, as described in Table 1.\\n5. Experiments\\n5.1. Implementation Details\\nWe initialize the weights of our model with the pretrained\\nCLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA\\n[11] finetuning. Utilizing LoRA, we refine the parameters\\nWqandWvthrough low-rank adaptation, with a designated'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '3e17f0b1-7136-4312-85db-68f4bed9d275'}, page_content='rankrset to 64 in our implementation. The model under-\\ngoes training consistently at an image resolution of 504 √ó\\n504 throughout the whole process. Each training step incor-\\nporates specifically crafted multi-modal instructional tem-\\nplates designed for a variety of vision-language tasks during\\nthe training process. We use AdamW [21] optimizer with a\\ncosine learning rate scheduler to train our model. We keep\\nthe global batch size as 144. We train our model in two'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '81bd261d-09f2-4453-929a-9d2496989a29'}, page_content='stages, first, we train using all of our datasets for 1 epoch,\\ncorrespondingly 2144 steps, followed by stage 2, where we\\nonly train on the grounding dataset for 1600 more steps(at\\n128 batch size).\\n5.2. Scene Classification\\nDatasets for evaluation. For scene classification, we eval-\\nuate our model using AID [34] and UCMerced [36]. AIDMethod Presence Comparison Rural/Urban Avg. Accuracy\\nLLaV A-1.5[17] 55.46 68.20 59.00 62.77\\nQwen-vl-Chat [1] 38.57 67.59 61.00 55.35'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '51580020-2258-4d2f-a38e-400da7ab9920'}, page_content='Qwen-vl-Chat [1] 38.57 67.59 61.00 55.35\\nMiniGPTv2 [4] 55.16 55.22 39.00 54.96\\nRSVQA[20] 87.47 81.50 90.00 86.32\\nEasyToHard[39] 90.66 87.49 91.67 89.94\\nBi-Modal[2] 91.06 91.16 92.66 91.63\\nSHRNet [41] 91.03 90.48 94.00 91.84\\nRSGPT[12] 91.17 91.70 94.00 92.29\\nGeoChat 91.09 90.33 94.00 90.70\\nTable 6. Comparisons with general zero-shot (top) and RS-VQA\\nspecialized (middle) models on RSVQA-LRBEN [20] dataset for\\nVQA task. [1, 4, 17] are evaluated in zero-shot setting. GeoChat'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '8db34f6a-4819-4a24-9cbe-3d447ab9b24d'}, page_content='outperforms other zero-shot models and performs competitively to\\nSoTA-supervised models like RSGPT which are specifically fine-\\ntuned on target dataset (while ours is a generic model not specifi-\\ncally finetuned on target dataset).\\n[34] is a large-scale aerial image collection compiled from\\nGoogle Earth imagery, with 30 classes, such as a river,\\ndense residential area, etc. The images are labeled by spe-\\ncialists in the field of remote sensing image interpretation.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '743e5079-821b-4534-acaa-2689bb5ca548'}, page_content='In total, the AID [34] dataset has 10,000 images within 30\\nclasses. The images have been taken from different coun-\\ntries as well as different weather conditions. For evaluation,\\nwe use a 20% split of the AID [34] dataset. UCMerced [36]\\nis a Land Use scene classification dataset, with 2,100 im-\\nages and 21 classes. Each image is of size 256 √ó256. We\\nuse the whole UCMerced [36] dataset as a zero-shot test set.\\nResults. We prompt the models with all of the'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '5b27f9cd-6f1d-4c8a-b117-054b458a7b25'}, page_content='Results. We prompt the models with all of the\\nclasses and prompt to classify the image using just one\\nword/phrase. For example, we input a prompt like ‚ÄùClas-\\nsify the image within one of the given classes: dense\\nresidential area, . . . , school. Answer with one\\nword or short phrase.‚Äù . We calculate zero-shot accu-\\nracy on both AID and UCMerced. GeoChat significantly\\noutperforms other VLM‚Äôs with an accuracy of 84.43% on\\nUCMerced [36] and 72.03% on AID [34], as presented in'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '2352554c-6dc9-48dc-b229-5e3b502d30e4'}, page_content='Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol-\\nlow the instructions provided for this specific task and re-\\nturns unrelated classes that are not a part of the dataset. It‚Äôs\\naccuracy is close to 5% if we pass the answers from Vicuna-\\nv1.5 [7] and ask it to check if the output sentence refers to\\nthe ground truth class or not. In comparison, Qwen-VL and\\nLLaVa-1.5 perform well in instruction following, but fall\\nshort to GeoChat, due to lack of domain knowledge.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 6, 'uuid': '56c2211c-158d-4f9f-b943-9bf6e8daab58'}, page_content='5.3. Visual Question Answering\\nDatasets for evaluation. RSVQA-HRBEN [20] comprises\\n10,569 high-resolution photos and 1,066,316 question-\\nanswer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-\\nvided into training, validation, test 1, and test 2 sets, re-\\nspectively. This dataset has three question types: presence,\\n7\\n27837'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '240047c1-0ba5-4a5f-af4e-eb88226410d5'}, page_content='Model Small Medium Large Single-object grounding Multi-object grounding [refer] [grounding] Overall\\nMiniGPTv2 [4] 1.7 9.9 21.9 9.1 3.6 8.2 2.6 7.6\\nGeoChat 2.9 13.6 21.7 16.0 4.3 10.5 11.8 10.6\\nTable 7. Performance (acc@0.5%) comparison of GeoChat on our benchmark. Small, medium and large refer to the size of the objects\\nbased on the bounding box area. Single/multi-object refer to how many objects the question asks the model to predict. [refer] : object'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': 'a01e4504-ca5e-414a-b72b-43fcadd41b25'}, page_content='referenced using one attribute from a2, a3 or a4 in Table 3. [grounding] : objects referenced using a combination of attributes from a1-a5\\nin Table 3. Overall, GeoChat outperforms the baseline, but there is still significant room for further improvement on this complex task.\\nModel Presence Comparison Average Accuracy\\nQwen-VL[1] 66.44 60.41 63.06\\nLLaV A-1.5[17] 69.83 67.29 68.40\\nMiniGPTv2[4] 40.79 50.91 46.46\\nGeoChat 58.45 83.19 72.30\\nTable 8. Comparison with other general ZS model‚Äôs on RSVQA-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '37c378f9-8660-4b13-8b66-0d9b68163c46'}, page_content='HRBEN [20] dataset for visual qa. All models here have not been\\ntrained on the target dataset. GeoChat performs favorably well\\ncompared to generic VLMs.\\ncomparison, and count. For evaluation, we use the test set-2\\nfor RSVQA-HRBEN [20] with 47k question answer pairs.\\nRSVQA-LR [20] is made up of 772 low-resolution im-\\nages and 77,232 question-answer pairs, with 77.8%, 11.1%,\\nand 11.1% used for training, validation, and testing, re-\\nspectively. There are four different categories of questions:'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '20ffe3c5-f23d-4eb4-bfaf-499a72fc8824'}, page_content='presence, comparison, rural/urban, and count. We omitted\\narea and count questions during evaluation because the re-\\nsponses are numerical and quantifiable into numerous cat-\\negories. In the RSVQA-LRBEN [20] dataset, for exam-\\nple, counting questions are quantified into five categories:\\n0, between 1 and 10, between 11 and 100, between 101 and\\n1000, and greater than 1000. For evaluation, we use the test\\nset of RSVQA-LRBEN [20] with 7k question-answer pairs.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': 'f7d0f6a9-deb0-4c17-b77a-7b589d2d8915'}, page_content='Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '2124535a-a4d8-4192-93d9-cc2857e75cd4'}, page_content='15.9% on LLaV A-v1.5 [17], as shown in Table 8.\\n5.4. Visual Grounding\\nDatasets for evaluation. For the evaluation of grounding\\ntasks, we propose a new benchmark that contains different\\nreferring and grounding tasks. We use the validation set\\nfrom [31] and used the same dataset creation pipeline as in\\nSec. 4 to construct the test benchmark. There are a total\\nof 7593 [refer], 560 [grounding], and 495 grounding de-\\nscription questions, as well as 2793 for region captioning.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '1ce24cb0-e65f-4e94-bb50-2741f4dfa0bc'}, page_content='We use accuracy@0.5 as the evaluation metric. Accuracy is\\ncalculated if the predicted box has an overlap of more thanModel acc@0.5 acc@.25 METEOR\\nMiniGPTv2[4] 10.8 30.9 16.4\\nGeoChat 11.7 33.9 48.9\\nTable 9. Results on grounding description task.\\nModel ROUGE-1 ROUGE-L METEOR\\nMiniGPTv2[4] 32.1 31.2 10.0\\nGeoChat 87.3 87.2 83.9\\nTable 10. Region level captioning performance.\\n0.5 IoU with the ground-truth box.\\nResults. Table 7 shows the performance of our method'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '58eff65a-541e-4bf4-8074-b921545dd641'}, page_content='and MiniGPT-4-v2 [4] on the proposed benchmark. Over-\\nall, the model performance is low on small objects or when\\nit has to predict multiple boxes. Compared to MiniGPT-4-\\nv2[4], our model works better on medium size images. On\\nthe grounding description task, we calculate both, the IoU\\nfor the multiple bounding boxes generated as well as the\\ntext answer generated. Our model provides a better descrip-\\ntion with slightly better box accuracy than MiniGPT-4-v2'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '95dfeb25-0568-457a-b7bc-9553ce1f5274'}, page_content='[4] (Table 9). As for region-level captioning, we evaluate\\nboth models based on the text accuracy with ground truth\\nregion-level captions (Table 10). Our model significantly\\noutperforms MiniGPT-4-v2 in terms of ROUGE and ME-\\nTEOR score.\\n6. Conclusion\\nAlthough recent advancements in large Vision-Language\\nModels (VLMs) have shown promise in nature image do-\\nmains, their performance in Remote Sensing (RS) scenar-\\nios is still limited due to the unique domain-specific chal-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': 'eaaff66d-e614-4a89-ace1-eeeee146d1cf'}, page_content='lenges. Addressing this gap, we present GeoChat, the\\nfirst unified remote sensing VLM that excels in multitask\\nconversational capabilities with high-resolution RS images.\\nGeoChat not only answers image-level queries but also en-\\ngages in region-specific dialogue, grounding responses with\\nprecise spatial coordinates. We create a novel RS mul-\\ntimodal instruction-following dataset comprising of 318k\\nimage-instruction pairs with a diverse multitask format.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 7, 'uuid': '62bea293-9d52-4aea-a37d-eb283b5b2bd0'}, page_content='GeoChat achieves robust zero-shot performance across var-\\nious RS tasks including scene classification, VQA, multi-\\nturn dialogue, visual grounding and referring object detec-\\ntion, thus establishing a comprehensive benchmark.\\n8\\n27838'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': '9d89dea7-13f9-4c5a-b4a8-86bf591cdd66'}, page_content='References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': '2fe8a064-ddde-45de-96ff-51dc00c9b662'}, page_content='Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': 'eedade6d-d3fd-4bc0-8de0-6a2e55569d61'}, page_content='Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 7, 8\\n[5] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-\\ning image scene classification: Benchmark and state of the\\nart.Proceedings of the IEEE , 105(10):1865‚Äì1883, 2017. 2,\\n3, 5\\n[6] Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': '70faf861-235e-41a5-a6cb-25064b5ec1d8'}, page_content='Lang, Yanqing Yao, and Junwei Han. Anchor-free oriented\\nproposal generator for object detection. IEEE Transactions\\non Geoscience and Remote Sensing , 60:1‚Äì11, 2022. 5\\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\\nXing. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality, 2023. 2, 4, 5, 7\\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': 'f61d89ab-02c9-454e-8842-2e78a2520e27'}, page_content='Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\\nFung, and Steven Hoi. Instructblip: Towards general-\\npurpose vision-language models with instruction tuning,\\n2023. 2\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. ICLR , 2021. 2'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': 'c6345f79-af83-444c-9873-66351f7252c9'}, page_content='scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': '4fb0f285-bf87-4329-af36-26198b987bc8'}, page_content='Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': 'e1592506-175f-48a4-b396-2c6ee34f6f6f'}, page_content='and-vision assistant for biomedicine in one day. arXiv\\npreprint arXiv:2306.00890 , 2023. 2\\n[15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\\nBlip: Bootstrapping language-image pre-training for uni-\\nfied vision-language understanding and generation. In In-\\nternational Conference on Machine Learning , pages 12888‚Äì\\n12900. PMLR, 2022. 5\\n[16] Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou. Rs-clip:\\nZero shot remote sensing scene classification via contrastive'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': '9588d12d-a7c6-49db-a06e-02092a4ae1a7'}, page_content='vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': 'b75373ce-9d75-4d18-aaaf-10589779bedf'}, page_content='Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': 'e253846a-71de-4252-b554-4da123b3a98e'}, page_content='understanding via large vision and language models. arXiv\\npreprint arXiv:2306.05424 , 2023. 2\\n[23] OpenAI. Gpt-4 technical report, 2023. 1\\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748‚Äì8763. PMLR, 2021. 7'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': '774e7322-1726-41ac-ad7f-c62f0d698765'}, page_content='8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 8, 'uuid': '895ea578-591e-457a-8049-fefa6cc7a4b3'}, page_content='arXiv preprint arXiv:2111.02114 , 2021. 5\\n9\\n27839'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': 'bae3d30e-8386-49ae-80e4-61f9e1036b21'}, page_content='[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping\\nWang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao\\nXu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun\\nFu. Fair1m: A benchmark dataset for fine-grained object\\nrecognition in high-resolution remote sensing imagery. IS-\\nPRS Journal of Photogrammetry and Remote Sensing , 184:\\n116‚Äì130, 2022. 5\\n[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui.\\nLearning to rank question answer pairs with holographic dual'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': '24283ff2-74d3-43ed-89d6-d15a52483a26'}, page_content='lstm architecture. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval , pages 695‚Äì704. ACM, 2017. 3, 4, 5\\n[29] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham\\nCholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin,\\nEric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards\\naccurate and lightweight fully transparent gpt. arXiv preprint\\narXiv:2402.16840 , 2024. 2\\n[30] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mul-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': 'dffabc3e-22d7-4d85-aae0-508650d2a0e7'}, page_content='lappilly, Hisham Cholakkal, Rao Muhammad Anwer,\\nSalman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan.\\nXraygpt: Chest radiographs summarization using large med-\\nical vision-language models. arXiv: 2306.07971 , 2023. 2\\n[31] Di Wang, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei\\nZhang. Scaling-up remote sensing segmentation dataset with\\nsegment anything model. In arxiv , 2023. 2, 5, 6, 8\\n[32] Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du,\\nDacheng Tao, and Liangpei Zhang. Advancing plain vision'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': '2bd164ad-af46-4f8c-b077-f21b75d1bc59'}, page_content='transformer toward remote sensing foundation model. IEEE\\nTransactions on Geoscience and Remote Sensing , 61:1‚Äì15,\\n2023. 6\\n[33] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and\\nYanfei Zhong. LoveDA: A remote sensing land-cover dataset\\nfor domain adaptive semantic segmentation, 2021. 6\\n[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang\\nBai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid:\\nA benchmark data set for performance evaluation of aerial'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': '3556a521-8749-4771-96b3-fe1f968c2d6b'}, page_content='scene classification. IEEE Transactions on Geoscience and\\nRemote Sensing , 55(7):3965‚Äì3981, 2017. 7\\n[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-\\nlongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-\\npei Zhang. Dota: A large-scale dataset for object detection in\\naerial images. In The IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , 2018. 5\\n[36] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': '7c42c969-4b9b-4d90-9428-492913954435'}, page_content='tial extensions for land-use classification. In Proceedings of\\nthe 18th SIGSPATIAL international conference on advances\\nin geographic information systems , pages 270‚Äì279, 2010. 7\\n[37] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\\nmaier. From image descriptions to visual denotations: New\\nsimilarity metrics for semantic inference over event descrip-\\ntions. Transactions of the Association for Computational\\nLinguistics , 2:67‚Äì78, 2014. 7'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': '2c17ae39-4bfb-4c8f-ace5-b4e475dd5de2'}, page_content='Linguistics , 2:67‚Äì78, 2014. 7\\n[38] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\\nand Tamara L Berg. Modeling context in referring expres-\\nsions. In Computer Vision‚ÄìECCV 2016: 14th European\\nConference, Amsterdam, The Netherlands, October 11-14,\\n2016, Proceedings, Part II 14 , pages 69‚Äì85. Springer, 2016.\\n7[39] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang\\nZhu. From easy to hard: Learning language-guided curricu-\\nlum for visual question answering on remote sensing data.'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': '4dbd5e8d-b98a-403f-a776-1206374f67ae'}, page_content='IEEE Transactions on Geoscience and Remote Sensing , 60:\\n1‚Äì11, 2022. 2, 7\\n[40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring\\ndata and models for visual grounding on remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 61:\\n1‚Äì13, 2023. 6\\n[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua\\nChen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-\\ntial hierarchical reasoning network for remote sensing visual\\nquestion answering. IEEE Transactions on Geoscience and'),\n",
       " Document(metadata={'source': './data_test/GeoChat.pdf', 'page': 9, 'uuid': 'abd8593c-5af6-4e20-92c0-1eb2337190d2'}, page_content='Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_docs\n",
    "# len(splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220dbc3a-fceb-4e49-a3f1-01e16660b2a6",
   "metadata": {
    "id": "220dbc3a-fceb-4e49-a3f1-01e16660b2a6"
   },
   "source": [
    "## ÂêëÈáèÂåñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7124830-dba9-4609-aedb-f81f4e388fa4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514,
     "referenced_widgets": [
      "8ae8db517ad14268a8f7864a01b8191c",
      "a76676eca3684c3eb4a2a1d896c3a966",
      "04d57f5f4e694641a38239de125e446d",
      "92e13d95804a45d4a636dfbdec0772c2",
      "a761689717ac4ec89c218a99fce3eb27",
      "156f58b920b143c4b2147878f84a2809",
      "d11d56deec8547c48d7bfec194fead9b",
      "ed00c82e5220472d822e8dee4d773608",
      "db0cded2a2e14f7c821eb10ea54a6b80",
      "ce09b1931e5146d6b4a592afc8146bd6",
      "ab1782bb839b490f890a7b78c40cb9f9",
      "fc64c338a9234e2b95f2eb7193a8e976",
      "b51bb55a9e12441fa1cc8bf46fcee318",
      "cd7dd87b8b924da890595c26a47456e9",
      "04b4df7da6054cebb1b4a7361d904517",
      "b730a3024024445eb7b93ce25cf4a8bf",
      "525fb3b924614e74937ae9cd26711527",
      "78362c0f00114573bdd9ef5883815140",
      "8df9bde6169b46a498949108e00fbaf8",
      "b9d1345184cf4f35a7f107079bad50fc",
      "caa7636b326743e0a0b9075d029bf908",
      "c66e0318f9b444af84686d737555ace6",
      "a9798a3378554f408c7bd016abcb1d26",
      "c8f8237bbaea49d6824ca103d3a66bba",
      "fc9daa43549645708c23293195087cdb",
      "0e8bdeab87ba4aa4863bc8cb1f5d05e2",
      "0fa3d8cc67f245679850c5b9e9ba7b6b",
      "f7f88ad7f22e4076bcc3a4f5a3348883",
      "b627f32931c343e3927a930a82c2e180",
      "1afffc47ddfa4095bcdf42fef01b1771",
      "c21838ae58a642cd92d5bed9e2d64410",
      "fab84da34e7b4acaac607f3ebf41fc5e",
      "6d56498445964a739f55bd5914671611",
      "4bf9408046b24e70bcbc434ce81f1586",
      "b3e6e10a9c0945a2b29c3455a6db590e",
      "d8bf0fbdcbc24b938ccc3f5525496c8b",
      "68549cda864b414c9cc9c69c52c1c7bb",
      "3228b0e6be2146928c6dc0fbd5d2636e",
      "cab762281bc642b689349908f8c8d509",
      "a2eb0ac561e045ec9cb44586da815516",
      "9e6053b8a79841c1a2c75c110ed2e2c1",
      "173f5ea18ee643eda658ed6ec7fb8888",
      "385a708cb16f4de9bf83796f6c321d34",
      "93c6143ee1e34a589f102307fce2be46",
      "05ddafa8e53b4323a3b87773d8c52d7b",
      "cc3634bb596c4730a6ff72fbf6093e2b",
      "44f1502b9d8c44bdb8e67823df3b61df",
      "d7b73b320a214a1c8564cb87c9fc7c1f",
      "ce2e09ea7a584e87828f247c01e710ef",
      "b63739062c34496d9e18d64e9d572c04",
      "3a24ba6d66704525a0f65044d36c3116",
      "0c24bf2912564885944b15fe875b9e86",
      "2a1fa9e5abcc40a5b27565b86eba4f2a",
      "f4705f680041401f9ded40e945e5e72e",
      "64c85d2d6d164b579de82467cd0fc5a9",
      "d1c23ba161fe48deb887bef9a696e98c",
      "ee2051c6fa9c4c62abffb2c41ad6cbf9",
      "e41b27af5bdb4917b5c176b14f1d38f1",
      "2bd0a1cdd3a0493db3a2b7c9ea999614",
      "1bb07d56deff47b89d80c88460420a28",
      "e8484544ce2c4b82b306676a3c73f9bf",
      "70c9685019ed480fa6b1a30b224d3d0a",
      "f4f24a8906f7451abfe8cea83dbc6c2d",
      "d529d59457d14794862a5c98602eeb51",
      "5167b90dfe31453f98f7a97c1bf45e77",
      "36f12265773f4c95b0665bfaf3a383db",
      "d4250feb91a547b8ba55a716ed72bc41",
      "fe7e1ce77a8e459e817b5c3a061dc3b9",
      "cb2afcae4aeb45c4a097532ff5fe0660",
      "0367de25b0804a079fef3a9fa4a1b2f2",
      "044f48908671438ab65b8040a4b09f40",
      "c6701afdd6f24032acfa724c5d1b5ca3",
      "a6267c3585994150b0119e18f92b253c",
      "2528a771e3174761ac4fdbf5bb737722",
      "274c88624c3449799ec984186146b688",
      "8a391251a1c54dbba308f304d9d0b6dd",
      "fe47f740059d4216b018d9439bc9aafa",
      "4e2cca81afd544b5af33bad236a574cc",
      "2312778eaac94a2696f435c8443b58f7",
      "2b252be54a4e4758b9e039ce0d16ca3c",
      "354eacb8b3ff436aa6e9c98a0392a478",
      "4dd431b7458e4cfdb0b45ad6f4560574",
      "12126a45430f407d8c903f408679df7d",
      "432e5082c3424903a2e0fcf857947d6f",
      "269f2ddeec3e4ae6a9d13a46e2a831a1",
      "05a457d2c35b403e9ad2d0cdee3b617d",
      "87abda91a5274305b493028d82c72e22",
      "6ef4117d7a3045bbbab01f71b6d9c828",
      "0312233d069f47c689d98e7e0fa7da5b",
      "94d281a28c64465294758a200817ada1",
      "6efb43f11ea045038eade7ea773cdd8e",
      "3bc9f26eda0e4f818e6f23df31195c5e",
      "b2bb7e0af2d442a3974e8471c1765d54",
      "474dde96127645e38017b38b11a78a01",
      "78b34247c0ec43a6acf54a0fc913dae3",
      "b318510199794db1907adce0cdc007ce",
      "956e4999e48e4d30bc971154d70c1ead",
      "683160f1e0ca40599b35d8ba784bc10a",
      "bdfb41f5379d4c12872ad647c51937e8",
      "549d665ac28c4d4bb3be5593c4653191",
      "819646ccaab2464d8688a31e4f345d96",
      "1199feec159e44c3a5ebdb582e687950",
      "173b837173314d86b948673d376f5c3c",
      "b0dec5996bcf42f3a9b798c07180513d",
      "b742e09a17024e9ea54cbe81bb328d8c",
      "f3e5aeafad9b470bbb54880644519aa8",
      "e6024d09af914986bec1834c773f1627",
      "fd7a02cc1f884940bb98c5bed786d5a1",
      "55bf13ec08794f5c821fe62b1104c89b",
      "425ebec3c1ca4393ab7bb6f45b694d82",
      "9c6d6ecd27ec456f8fe894c8ab31fab1",
      "a14c32c2257e40c7ab724e3de4a0e954",
      "c1e1ac7befd34eefb7463d85ded81796",
      "03ad8540620b4a0b977503df9550e03e",
      "4d7be10c64b94c98965305f696f9398a",
      "852f2146db4743888460097cec7bf938",
      "779a96631812433ab007fcc32bda72a3",
      "6a6de690242042c788e7bf4e93cbc256",
      "767ff3484a4d45db9c32932a892521a5",
      "38a531c0e0454085b6a8bad2b6fe162b",
      "5946f069abe14153a0929aafca9bbdfc"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:05.389230Z",
     "iopub.status.busy": "2024-07-21T13:22:05.389114Z",
     "iopub.status.idle": "2024-07-21T13:22:05.398475Z",
     "shell.execute_reply": "2024-07-21T13:22:05.397968Z",
     "shell.execute_reply.started": "2024-07-21T13:22:05.389219Z"
    },
    "id": "f7124830-dba9-4609-aedb-f81f4e388fa4",
    "outputId": "6cc82529-a1db-4a23-9514-2fc4b5a6c2f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'device: {device}')\n",
    "\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_PATH,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "663ef1a4-5866-4f6b-8d9d-4724f62142cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:07.755864Z",
     "iopub.status.busy": "2024-07-21T13:22:07.755656Z",
     "iopub.status.idle": "2024-07-21T13:22:07.758822Z",
     "shell.execute_reply": "2024-07-21T13:22:07.758495Z",
     "shell.execute_reply.started": "2024-07-21T13:22:07.755850Z"
    },
    "id": "663ef1a4-5866-4f6b-8d9d-4724f62142cb"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def get_vector_db(docs, store_path, force_rebuild=False):\n",
    "    if not os.path.exists(store_path):\n",
    "        force_rebuild = True\n",
    "\n",
    "    if force_rebuild:\n",
    "        vector_db = Chroma.from_documents(\n",
    "            docs,\n",
    "            embedding=embeddings,\n",
    "            persist_directory=store_path\n",
    "        )\n",
    "    else:\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=store_path,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41821b04-afc4-4a9b-98d2-f95f7bf83f74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:07.773482Z",
     "iopub.status.busy": "2024-07-21T13:22:07.773355Z",
     "iopub.status.idle": "2024-07-21T13:22:08.327108Z",
     "shell.execute_reply": "2024-07-21T13:22:08.326541Z",
     "shell.execute_reply.started": "2024-07-21T13:22:07.773470Z"
    },
    "id": "41821b04-afc4-4a9b-98d2-f95f7bf83f74"
   },
   "outputs": [],
   "source": [
    "vector_db = get_vector_db(splitted_docs, store_path=os.path.join(output_dir, 'chromadb', 'bge_large_v1.5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CKF0J7duLRMw",
   "metadata": {
    "id": "CKF0J7duLRMw"
   },
   "source": [
    "# Ê£ÄÁ¥¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9ba0818-d608-4e87-aa9e-aced270a113c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:07.759389Z",
     "iopub.status.busy": "2024-07-21T13:22:07.759238Z",
     "iopub.status.idle": "2024-07-21T13:22:07.772903Z",
     "shell.execute_reply": "2024-07-21T13:22:07.772468Z",
     "shell.execute_reply.started": "2024-07-21T13:22:07.759377Z"
    },
    "id": "f9ba0818-d608-4e87-aa9e-aced270a113c"
   },
   "outputs": [],
   "source": [
    "def retrieve(vector_db, query: str, k=5):\n",
    "    return vector_db.similarity_search(query, k=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c6f3c-5777-4aa9-bc60-a3ee23050506",
   "metadata": {
    "id": "566c6f3c-5777-4aa9-bc60-a3ee23050506"
   },
   "source": [
    "## ËÆ°ÁÆóÊ£ÄÁ¥¢ÂáÜÁ°ÆÁéá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b03e3382-39e9-4932-a265-69b811041629",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:08.327815Z",
     "iopub.status.busy": "2024-07-21T13:22:08.327571Z",
     "iopub.status.idle": "2024-07-21T13:22:08.330973Z",
     "shell.execute_reply": "2024-07-21T13:22:08.330592Z",
     "shell.execute_reply.started": "2024-07-21T13:22:08.327802Z"
    },
    "id": "b03e3382-39e9-4932-a265-69b811041629"
   },
   "outputs": [],
   "source": [
    "test_df = qa_df[(qa_df['dataset'] == 'test') & (qa_df['qa_type'] == 'detailed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ad650ce-43c4-4bbc-b4b3-809a5269b545",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:08.332774Z",
     "iopub.status.busy": "2024-07-21T13:22:08.332444Z",
     "iopub.status.idle": "2024-07-21T13:22:08.344726Z",
     "shell.execute_reply": "2024-07-21T13:22:08.344334Z",
     "shell.execute_reply.started": "2024-07-21T13:22:08.332758Z"
    },
    "id": "3ad650ce-43c4-4bbc-b4b3-809a5269b545",
    "outputId": "93832afb-ae63-4b3d-d5fe-bbab4bb7b264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32c3ad14-b217-44aa-bdb9-909b9d559668",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "14963d0f85154fbf9ab49ef84840f921",
      "0bbf9d684d77494794eb50139323f82b",
      "25d2069884884c5d9b145cbbc6d454f2",
      "85b79c6c8ebb440eb8ff423dbc75c183",
      "6da130e16c28435d983f5b817c406265",
      "b94356d13b624b87afe94eae90c8bb73",
      "ade929269f854be2bed144b486671d6c",
      "8dba47c2b2dc41cda3afc4918321f605",
      "cf393ed116a248e9a924daa4ff24da64",
      "608807c98d464ba987fe6c08e6e8958c",
      "51ae5239814d484386f1f776b24b1c6a"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:08.345367Z",
     "iopub.status.busy": "2024-07-21T13:22:08.345209Z",
     "iopub.status.idle": "2024-07-21T13:22:11.554954Z",
     "shell.execute_reply": "2024-07-21T13:22:11.554519Z",
     "shell.execute_reply.started": "2024-07-21T13:22:08.345352Z"
    },
    "id": "32c3ad14-b217-44aa-bdb9-909b9d559668",
    "outputId": "c152f51e-4faa-42ef-ba26-ce03b9ffa261"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:06<00:00, 15.87it/s]\n"
     ]
    }
   ],
   "source": [
    "top_k_arr = list(range(1, 9))\n",
    "hit_stat_data = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    question = row['question']\n",
    "    true_uuid = row['uuid']\n",
    "    chunks = retrieve(vector_db, question, k=max(top_k_arr))\n",
    "    retrieved_uuids = [doc.metadata['uuid'] for doc in chunks]\n",
    "\n",
    "    for k in top_k_arr:\n",
    "        hit_stat_data.append({\n",
    "            'question': question,\n",
    "            'top_k': k,\n",
    "            'hit': int(true_uuid in retrieved_uuids[:k])\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eef0eedb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 8, 'source': './data_test/GeoChat.pdf', 'uuid': '4fb0f285-bf87-4329-af36-26198b987bc8'}, page_content='Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-'),\n",
       " Document(metadata={'page': 9, 'source': './data_test/GeoChat.pdf', 'uuid': '4dbd5e8d-b98a-403f-a776-1206374f67ae'}, page_content='IEEE Transactions on Geoscience and Remote Sensing , 60:\\n1‚Äì11, 2022. 2, 7\\n[40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring\\ndata and models for visual grounding on remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 61:\\n1‚Äì13, 2023. 6\\n[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua\\nChen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-\\ntial hierarchical reasoning network for remote sensing visual\\nquestion answering. IEEE Transactions on Geoscience and'),\n",
       " Document(metadata={'page': 9, 'source': './data_test/GeoChat.pdf', 'uuid': 'bae3d30e-8386-49ae-80e4-61f9e1036b21'}, page_content='[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping\\nWang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao\\nXu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun\\nFu. Fair1m: A benchmark dataset for fine-grained object\\nrecognition in high-resolution remote sensing imagery. IS-\\nPRS Journal of Photogrammetry and Remote Sensing , 184:\\n116‚Äì130, 2022. 5\\n[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui.\\nLearning to rank question answer pairs with holographic dual'),\n",
       " Document(metadata={'page': 9, 'source': './data_test/GeoChat.pdf', 'uuid': '2c17ae39-4bfb-4c8f-ace5-b4e475dd5de2'}, page_content='Linguistics , 2:67‚Äì78, 2014. 7\\n[38] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\\nand Tamara L Berg. Modeling context in referring expres-\\nsions. In Computer Vision‚ÄìECCV 2016: 14th European\\nConference, Amsterdam, The Netherlands, October 11-14,\\n2016, Proceedings, Part II 14 , pages 69‚Äì85. Springer, 2016.\\n7[39] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang\\nZhu. From easy to hard: Learning language-guided curricu-\\nlum for visual question answering on remote sensing data.'),\n",
       " Document(metadata={'page': 8, 'source': './data_test/GeoChat.pdf', 'uuid': '9588d12d-a7c6-49db-a06e-02092a4ae1a7'}, page_content='vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.'),\n",
       " Document(metadata={'page': 9, 'source': './data_test/GeoChat.pdf', 'uuid': '2bd164ad-af46-4f8c-b077-f21b75d1bc59'}, page_content='transformer toward remote sensing foundation model. IEEE\\nTransactions on Geoscience and Remote Sensing , 61:1‚Äì15,\\n2023. 6\\n[33] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and\\nYanfei Zhong. LoveDA: A remote sensing land-cover dataset\\nfor domain adaptive semantic segmentation, 2021. 6\\n[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang\\nBai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid:\\nA benchmark data set for performance evaluation of aerial'),\n",
       " Document(metadata={'page': 8, 'source': './data_test/GeoChat.pdf', 'uuid': '9d89dea7-13f9-4c5a-b4a8-86bf591cdd66'}, page_content='References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE'),\n",
       " Document(metadata={'page': 8, 'source': './data_test/GeoChat.pdf', 'uuid': '70faf861-235e-41a5-a6cb-25064b5ec1d8'}, page_content='Lang, Yanqing Yao, and Junwei Han. Anchor-free oriented\\nproposal generator for object detection. IEEE Transactions\\non Geoscience and Remote Sensing , 60:1‚Äì11, 2022. 5\\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\\nXing. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality, 2023. 2, 4, 5, 7\\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcaa74a3-3873-4c9e-ae8a-47f4e940b724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:11.555548Z",
     "iopub.status.busy": "2024-07-21T13:22:11.555422Z",
     "iopub.status.idle": "2024-07-21T13:22:11.558235Z",
     "shell.execute_reply": "2024-07-21T13:22:11.557931Z",
     "shell.execute_reply.started": "2024-07-21T13:22:11.555535Z"
    },
    "id": "dcaa74a3-3873-4c9e-ae8a-47f4e940b724"
   },
   "outputs": [],
   "source": [
    "hit_stat_df = pd.DataFrame(hit_stat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f85cef68-c3ef-49d0-b9ff-9009953d5685",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:11.558967Z",
     "iopub.status.busy": "2024-07-21T13:22:11.558707Z",
     "iopub.status.idle": "2024-07-21T13:22:11.574837Z",
     "shell.execute_reply": "2024-07-21T13:22:11.574379Z",
     "shell.execute_reply.started": "2024-07-21T13:22:11.558955Z"
    },
    "id": "f85cef68-c3ef-49d0-b9ff-9009953d5685",
    "outputId": "47fd9b04-2653-46ad-fa7a-e55b400f4ade"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>top_k</th>\n",
       "      <th>hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>What is the distribution of RSVQA-LR [20] for ...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>What kind of remote sensing model is GeoChat?</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>What is the size of each image in the UCMerced...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>What are the attributes that may be absent in ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>Who are the authors of the LoRA paper?</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              question  top_k  hit\n",
       "534  What is the distribution of RSVQA-LR [20] for ...      7    1\n",
       "591      What kind of remote sensing model is GeoChat?      8    1\n",
       "500  What is the size of each image in the UCMerced...      5    1\n",
       "415  What are the attributes that may be absent in ...      8    0\n",
       "653             Who are the authors of the LoRA paper?      6    1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_stat_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b07dd842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: seaborn in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=1.2 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from seaborn) (3.7.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from pandas>=1.2->seaborn) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\envs\\gold-yolo\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "48438b40-6e03-4459-81ce-31a41867bad8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:11.575377Z",
     "iopub.status.busy": "2024-07-21T13:22:11.575255Z",
     "iopub.status.idle": "2024-07-21T13:22:11.920473Z",
     "shell.execute_reply": "2024-07-21T13:22:11.919973Z",
     "shell.execute_reply.started": "2024-07-21T13:22:11.575364Z"
    },
    "id": "48438b40-6e03-4459-81ce-31a41867bad8"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a63797c7-4151-4f55-8e5d-080c34265393",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:11.921495Z",
     "iopub.status.busy": "2024-07-21T13:22:11.920978Z",
     "iopub.status.idle": "2024-07-21T13:22:11.927266Z",
     "shell.execute_reply": "2024-07-21T13:22:11.926849Z",
     "shell.execute_reply.started": "2024-07-21T13:22:11.921481Z"
    },
    "id": "a63797c7-4151-4f55-8e5d-080c34265393",
    "outputId": "7687dcf9-54ac-4ed8-f2ba-b15cd9198aef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_k</th>\n",
       "      <th>hit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   top_k   hit\n",
       "0      1  0.42\n",
       "1      2  0.56\n",
       "2      3  0.62\n",
       "3      4  0.65\n",
       "4      5  0.71\n",
       "5      6  0.73\n",
       "6      7  0.74\n",
       "7      8  0.74"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hit_stat_df.groupby('top_k')['hit'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0b086d1-6cec-4743-8df6-2ab3b1593689",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T13:22:11.929189Z",
     "iopub.status.busy": "2024-07-21T13:22:11.929035Z",
     "iopub.status.idle": "2024-07-21T13:22:12.047934Z",
     "shell.execute_reply": "2024-07-21T13:22:12.047474Z",
     "shell.execute_reply.started": "2024-07-21T13:22:11.929177Z"
    },
    "id": "b0b086d1-6cec-4743-8df6-2ab3b1593689",
    "outputId": "add763c5-3aee-4724-c9ce-8ffc51994a5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='top_k', ylabel='hit'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x='top_k', y='hit', data=hit_stat_df.groupby('top_k')['hit'].mean().reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7925564a-7d30-4914-baaf-4a00abb7686d",
   "metadata": {
    "id": "7925564a-7d30-4914-baaf-4a00abb7686d"
   },
   "source": [
    "# ÈóÆÁ≠î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f289897a4f3bf47b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T15:08:15.067283Z",
     "start_time": "2024-07-23T15:08:15.065512Z"
    }
   },
   "outputs": [],
   "source": [
    "## ‰ΩøÁî®LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78d762ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install ollama\n",
    "# !ollama serve\n",
    "# !ollama pull qwen2:7b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b47b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama serve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3660f166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull qwen2:7b-instruct\n",
    "# !pip install langchain==0.2.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_o62oZuZ-6qi",
   "metadata": {
    "id": "_o62oZuZ-6qi"
   },
   "source": [
    "### ÊµÅÂºèÂ§ÑÁêÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1796551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÊàëÊòØÈòøÈáå‰∫ëÂºÄÂèëÁöÑ‰∏ÄÊ¨æË∂ÖÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÂè´ÈÄö‰πâÂçÉÈóÆ„ÄÇ‰Ωú‰∏∫‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÁöÑ‰∏ªË¶ÅËÉΩÂäõÊòØÁîüÊàê‰∏éÁªôÂÆöËØçËØ≠Áõ∏ÂÖ≥ÁöÑÈ´òË¥®ÈáèÊñáÊú¨„ÄÇÊàëÂèØ‰ª•ÂõûÁ≠îÂêÑÁßçÈóÆÈ¢ò„ÄÅÊèê‰æõ‰ª£Á†ÅÂÆûÁé∞„ÄÅÁªôÂá∫ÂàõÊÑèÊèêÊ°à„ÄÅËøõË°åÁøªËØë„ÄÅÊÄªÁªìÊñáÊú¨„ÄÅÂàÜÊûêÊÉÖÁª™„ÄÅÊèê‰æõÂª∫ËÆÆÁ≠âÁ≠â„ÄÇÊúâ‰ªÄ‰πàÊàëËÉΩÂ∏ÆÂä©‰Ω†Ëß£Á≠îÁöÑÈóÆÈ¢òÂêóÔºü\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "# export OLLAMA_FLASH_ATTENTION=1\n",
    "llm = Ollama(\n",
    "    model='qwen2:7b-instruct',\n",
    "    base_url=\"http://localhost:11434\",\n",
    ")\n",
    "print(llm.invoke('‰Ω†ÊòØË∞ÅÔºü'))\n",
    "# 30s cpu  3.2s gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "jl36b7QsQA0t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl36b7QsQA0t",
    "outputId": "77c06e77-e633-40e2-e587-c41cee2a3d79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lack of domain knowledge in remote sensing tasks makes it hard for models like MiniGPT-4-v2 to align their behavior with user queries."
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "llm = Ollama(\n",
    "    model='qwen2:7b-instruct',\n",
    "    base_url=\"http://localhost:11434\",\n",
    ")\n",
    "\n",
    "# prompt_tmpl = \"\"\"\n",
    "# ‰Ω†ÊòØ‰∏Ä‰∏™ÈáëËûçÂàÜÊûêÂ∏àÔºåÊìÖÈïøÊ†πÊçÆÊâÄËé∑ÂèñÁöÑ‰ø°ÊÅØÁâáÊÆµÔºåÂØπÈóÆÈ¢òËøõË°åÂàÜÊûêÂíåÊé®ÁêÜ„ÄÇ\n",
    "# ‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†πÊçÆÊâÄËé∑ÂèñÁöÑ‰ø°ÊÅØÁâáÊÆµÔºà<<<<context>>><<<</context>>>‰πãÈó¥ÁöÑÂÜÖÂÆπÔºâÂõûÁ≠îÈóÆÈ¢ò„ÄÇ\n",
    "# ÂõûÁ≠î‰øùÊåÅÁÆÄÊ¥ÅÔºå‰∏çÂøÖÈáçÂ§çÈóÆÈ¢òÔºå‰∏çË¶ÅÊ∑ªÂä†ÊèèËø∞ÊÄßËß£ÈáäÂíå‰∏éÁ≠îÊ°àÊó†ÂÖ≥ÁöÑ‰ªª‰ΩïÂÜÖÂÆπ„ÄÇ\n",
    "# Â∑≤Áü•‰ø°ÊÅØÔºö\n",
    "# <<<<context>>>\n",
    "# {context}\n",
    "# <<<</context>>>\n",
    "\n",
    "# ÈóÆÈ¢òÔºö{question}\n",
    "# ËØ∑ÂõûÁ≠îÔºö\n",
    "# \"\"\"\n",
    "prompt_tmpl = \"\"\"\n",
    "You are a remote sensing expert who is good at analyzing and reasoning about problems based on the information fragments obtained.\n",
    "Your task is to answer questions based on the information fragments obtained (the content between <<<<context>>><<<</context>>>).\n",
    "Keep your answers concise, do not repeat the question, do not add descriptive explanations and anything irrelevant to the answer.\n",
    "Known information:\n",
    "<<<<context>>>\n",
    "{context}\n",
    "<<<</context>>>\n",
    "\n",
    "Question: {question}\n",
    "Please answer:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(prompt_tmpl)\n",
    "retriever = vector_db.as_retriever(search_kwargs={'k': 4})\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "for chunk in rag_chain.stream(\"What makes it hard for the models to align their behavior with user queries?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HbUjsGaF_B4g",
   "metadata": {
    "id": "HbUjsGaF_B4g"
   },
   "source": [
    "### ÈùûÊµÅÂºèËæìÂá∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "FhM128X6-v0V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhM128X6-v0V",
    "outputId": "856bfab3-a015-41b9-c859-c145b8c630bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lack of domain knowledge specific to remote sensing tasks.\n"
     ]
    }
   ],
   "source": [
    "# print(rag_chain.invoke('2023Âπ¥10ÊúàÁæéÂõΩISMÂà∂ÈÄ†‰∏öPMIÊåáÊï∞ËæÉ‰∏äÊúàÊúâ‰ΩïÂèòÂåñÔºü'))\n",
    "print(rag_chain.invoke('What makes it hard for the models to align their behavior with user queries?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6iEKt5prCrW8",
   "metadata": {
    "id": "6iEKt5prCrW8"
   },
   "source": [
    "## ÊµÅÁ®ãÊãÜËß£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26f8a2678a7d45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "    model='qwen2:7b-instruct',\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a025198-556b-4b3e-a4a5-1aa7e4381641",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T14:04:33.356053Z",
     "iopub.status.busy": "2024-07-21T14:04:33.355268Z",
     "iopub.status.idle": "2024-07-21T14:04:35.392083Z",
     "shell.execute_reply": "2024-07-21T14:04:35.391745Z",
     "shell.execute_reply.started": "2024-07-21T14:04:33.355982Z"
    },
    "id": "5a025198-556b-4b3e-a4a5-1aa7e4381641",
    "outputId": "697d71d2-62a1-48d1-e6c7-02b9dda7467c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ÊàëÊòØÈòøÈáå‰∫ëÂºÄÂèëÁöÑ‰∏ÄÊ¨æË∂ÖÂ§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÂè´ÈÄö‰πâÂçÉÈóÆ„ÄÇ‰Ωú‰∏∫‰∏Ä‰∏™È¢ÑËÆ≠ÁªÉËØ≠Ë®ÄÊ®°ÂûãÔºåÊàëÁöÑÁõÆÊ†áÊòØÁîüÊàê‰∏éÁªôÂÆöpromptÁõ∏ÂÖ≥ÁöÑ„ÄÅ‰∫∫Á±ªÈ´òË¥®ÈáèÁöÑÂõûÂ∫îÔºå‰ªéËÄåÂ∏ÆÂä©Áî®Êà∑Ëé∑ÂæóÊúâÁî®ÁöÑ‰ø°ÊÅØÊàñÂ®±‰πê‰ΩìÈ™å„ÄÇÂ¶ÇÊûúÊÇ®Êúâ‰ªª‰ΩïÈóÆÈ¢òÊàñÈúÄË¶ÅÂ∏ÆÂä©ÔºåËØ∑ÈöèÊó∂ÂëäËØâÊàëÔºÅ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('‰Ω†ÊòØË∞Å')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7cf52672-6e47-458d-b050-51cb45178184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:29:36.744056Z",
     "iopub.status.busy": "2024-07-21T14:29:36.743280Z",
     "iopub.status.idle": "2024-07-21T14:29:36.749038Z",
     "shell.execute_reply": "2024-07-21T14:29:36.748620Z",
     "shell.execute_reply.started": "2024-07-21T14:29:36.743985Z"
    },
    "id": "7cf52672-6e47-458d-b050-51cb45178184"
   },
   "outputs": [],
   "source": [
    "def rag(query, n_chunks=5):\n",
    "    prompt_tmpl = \"\"\"\n",
    "You are a remote sensing expert who is good at analyzing and reasoning about problems based on the information fragments obtained.\n",
    "Your task is to answer questions based on the information fragments obtained (the content between <<<<context>>><<<</context>>>).\n",
    "Keep your answers concise, do not repeat the question, do not add descriptive explanations and anything irrelevant to the answer.\n",
    "Known information:\n",
    "<<<<context>>>\n",
    "{context}\n",
    "<<<</context>>>\n",
    "\n",
    "Question: {question}\n",
    "Please answer:\n",
    "\"\"\".strip()\n",
    "\n",
    "    chunks = retrieve(vector_db, query, k=n_chunks)\n",
    "    prompt = prompt_tmpl.replace('{context}', '\\n\\n'.join([doc.page_content for doc in chunks])).replace('{question}', query)\n",
    "\n",
    "    return llm(prompt), [doc.page_content for doc in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "62bba41a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What abilities of VLMs are particularly noteworthy according to the text?'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "258a51b8-e303-444d-9304-700abf3deef8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-21T14:29:39.544828Z",
     "iopub.status.busy": "2024-07-21T14:29:39.544062Z",
     "iopub.status.idle": "2024-07-21T14:29:39.557948Z",
     "shell.execute_reply": "2024-07-21T14:29:39.555645Z",
     "shell.execute_reply.started": "2024-07-21T14:29:39.544758Z"
    },
    "id": "258a51b8-e303-444d-9304-700abf3deef8"
   },
   "outputs": [],
   "source": [
    "prediction_df = qa_df[qa_df['dataset'] == 'test'][['uuid', 'question', 'qa_type', 'answer']]\n",
    "\n",
    "answer_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "63415e64-1028-4954-99fe-c19c5583849a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "7a06b66a5d314b0cba2873ac7e4ef63f",
      "cb14f003a5f54307a2bcd268238f090b",
      "a16309c75be54ede82350da8480dc63c",
      "ea911d7a87d14b75bc25f9e2fd1127ae",
      "c9c618159ae94f2ba9766f418212758f",
      "642c1f7b73504ab8af704a9cb901f895",
      "2d14e7f153a341c4b1848d5666a014cd",
      "dcb4167367b64c8293b3d963645feb31",
      "1b0eeba3def649918fbc096799a7b279",
      "6a26f0ee05ff4b1d859d3a31a729c24b",
      "be462996e01346b4a1a92fbc17994320"
     ]
    },
    "execution": {
     "iopub.execute_input": "2024-07-21T14:29:55.595307Z",
     "iopub.status.busy": "2024-07-21T14:29:55.594446Z",
     "iopub.status.idle": "2024-07-21T14:35:08.104553Z",
     "shell.execute_reply": "2024-07-21T14:35:08.102161Z",
     "shell.execute_reply.started": "2024-07-21T14:29:55.595235Z"
    },
    "id": "63415e64-1028-4954-99fe-c19c5583849a",
    "outputId": "916ac0e0-fd69-4330-fe5e-bd4d9847cdcf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [06:51<00:00,  4.11s/it]\n"
     ]
    }
   ],
   "source": [
    "for idx, row in tqdm(prediction_df.iterrows(), total=len(prediction_df)):\n",
    "    uuid = row['uuid']\n",
    "    question = row['question']\n",
    "    answer, context = rag(question, n_chunks=4)\n",
    "    answer_dict[question] = {\n",
    "        'uuid': uuid,\n",
    "        'ref_answer': row['answer'],\n",
    "        'gen_answer': answer,\n",
    "        'context': context\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dbf72479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'What is proposed to address the limitations mentioned?': {'uuid': '37163d64-7378-4168-9998-b89240a20c84',\n",
       "  'ref_answer': 'GeoChat, the first versatile remote sensing VLM (Vision Language Model) that offers multitask conversational capabilities with high-resolution RS images',\n",
       "  'gen_answer': 'Bi-modal transformer-based approach',\n",
       "  'context': ['model cannot perform object grounding or accept regionData Size Response formatting prompts\\nDetailed Description 30k Describe the image in detail.\\nMulti-Round Conversation 65k -\\nComplex Questions 10k -\\nRSVQA-LRBEN[20] 56k Answer the question using a single word or phrase.\\nNWPU-RESISC-45[5] 31.5k\\nFloodnet[25] 4k\\nGrounding Description 25k [grounding] Describe the image in detail.\\nRegion Captioning 40k [identify] {bxleft, bytop, bxright, bybottom|Œ∏}',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE',\n",
       "   'ing images. A major gap exists in the remote sensing do-\\nmain towards developing general-purpose models to solve\\nall tasks together, while also maintaining conversation abil-\\nities. While RSGPT [12] is an initial effort that has shown\\ngood conversation ability along with solving multiple tasks,\\nit requires finetuning the model for each task separately,\\nwhich makes it cumbersome and not generalizable. Fur-\\n2\\n27832',\n",
       "   'Large Language Model\\nüî•MLP Adaptor \\n‚ùÑCLIP-ViT\\n‚ùÑ\\n<im_end> [task specific prompts] <im_start>[USER]: [identify] What is present in this area?[USER]:[refer] Football ground on the right side, surrounded by trees.\\nReferring Expression\\nImage Description\\nRegion  captioning\\nVisual QA\\nSure!']},\n",
       " 'What is one of the types of data used in the natural image domain?': {'uuid': '8d02ff70-fd24-4093-89d1-307e84228ab3',\n",
       "  'ref_answer': 'Aligned image-text data sourced from web imagery.',\n",
       "  'gen_answer': 'Referring expression datasets',\n",
       "  'context': ['velopment of versatile multimodal conversational assistants\\nwith broad applications in real-world scenarios [12].\\nHowever, general-domain VLMs designed for natural\\nimages, exhibit poor performance when presented with re-\\n1\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n27831',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   'visual question answering and scene classification datasets with\\nvarying image sizes and types of classes to ensure diversity.\\nConstituent Datasets: In the compilation of our in-\\nstruction set, we incorporate three distinct types of datasets,\\nencompassing the ones designed for object detection,\\nscene classification, and visual question answering (VQA).\\nSpecifically, we integrate three object detection (DOTA\\n[35], DIOR [6], and FAIR1M [27] which together form',\n",
       "   'tributes {a2, a3}can be arranged in any sequence.\\nSimilarly, the sentence template incorporates the rela-\\ntional attributes a5 to establish connections between two\\nobjects through this structure:\\n‚ÄùThe/A ‚ü®ai3‚ü© ‚ü®ai2‚ü©ai1ai5aj1‚ü®in/on the aj4‚ü©.‚Äù\\nHere, the indicies iandjrepresent the ithandjthobject.\\nVisual Grounding: Although referring expression\\ndatasets are available in the natural image domain [37, 38],\\nthey lack for the remote sensing domain. To this end, we']},\n",
       " 'What kind of data is mentioned as being abundant in the natural image domain?': {'uuid': '8d02ff70-fd24-4093-89d1-307e84228ab3',\n",
       "  'ref_answer': 'Aligned image-text data.',\n",
       "  'gen_answer': 'High-resolution remote sensing imagery.',\n",
       "  'context': ['VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   'velopment of versatile multimodal conversational assistants\\nwith broad applications in real-world scenarios [12].\\nHowever, general-domain VLMs designed for natural\\nimages, exhibit poor performance when presented with re-\\n1\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n27831',\n",
       "   '[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping\\nWang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao\\nXu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun\\nFu. Fair1m: A benchmark dataset for fine-grained object\\nrecognition in high-resolution remote sensing imagery. IS-\\nPRS Journal of Photogrammetry and Remote Sensing , 184:\\n116‚Äì130, 2022. 5\\n[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui.\\nLearning to rank question answer pairs with holographic dual',\n",
       "   'mains, allowing users to hold a dialogue about given vi-\\nsual content. However, such general-domain VLMs perform\\npoorly for Remote Sensing (RS) scenarios, leading to inac-\\ncurate or fabricated information when presented with RS\\ndomain-specific queries. Such a behavior emerges due to\\nthe unique challenges introduced by RS imagery. For exam-\\nple, to handle high-resolution RS imagery with diverse scale\\nchanges across categories and many small objects, region-']},\n",
       " 'What does GeoChat require to generate visually grounded responses?': {'uuid': '33890465-ae6c-42b4-85cf-b5d2e06849fc',\n",
       "  'ref_answer': 'GeoChat requires suitable task tokens and user queries to generate visually grounded responses.',\n",
       "  'gen_answer': 'GeoChat requires suitable task tokens and user queries to generate visually grounded responses.',\n",
       "  'context': ['ther, RSGPT cannot work for region-level reasoning or vi-\\nsual grounding, which our work aims to address.\\n3. GeoChat: Grounded Remote Sensing VLM\\nVisually grounded conversations for remote sensing aim to\\ngenerate textual responses interleaved with corresponding\\nobject locations. Further, a user can also provide visual\\nprompts (e.g., a bounding box) besides natural language\\nquestions, and the model should be able to answer questions\\nabout the specified Region of Interest (RoI). Such seamless',\n",
       "   'Figure 5. Qualitative results of GeoChat. ( left-right ) Results are shown on grounding, referring object detection, and disaster/damage\\ndetection. The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior. The\\nmodel can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left).',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'object locations in its responses to visually ground objects.\\nThis enables a diverse set of tasks possible with GeoChat\\nincluding referring expression detection, image/region cap-\\ntioning, scene classification, natural language conversations\\nand VQA, besides visually grounded conversations.\\nIn summary, this work has the following contributions:\\n‚Ä¢RS multimodal instruction following dataset . We present\\na novel data generation pipeline, to leverage existing ob-']},\n",
       " 'What types of responses can GeoChat generate?': {'uuid': '33890465-ae6c-42b4-85cf-b5d2e06849fc',\n",
       "  'ref_answer': 'GeoChat can generate visually grounded responses, visual question answering on images and regions, and scene classification.',\n",
       "  'gen_answer': 'GeoChat can generate textual responses, only visual grounding, and both text and object groundings interleaved together.',\n",
       "  'context': ['Figure 5. Qualitative results of GeoChat. ( left-right ) Results are shown on grounding, referring object detection, and disaster/damage\\ndetection. The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior. The\\nmodel can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left).',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.',\n",
       "   'object locations in its responses to visually ground objects.\\nThis enables a diverse set of tasks possible with GeoChat\\nincluding referring expression detection, image/region cap-\\ntioning, scene classification, natural language conversations\\nand VQA, besides visually grounded conversations.\\nIn summary, this work has the following contributions:\\n‚Ä¢RS multimodal instruction following dataset . We present\\na novel data generation pipeline, to leverage existing ob-']},\n",
       " 'Where are the visual question answering results displayed?': {'uuid': '33890465-ae6c-42b4-85cf-b5d2e06849fc',\n",
       "  'ref_answer': 'The visual question answering results are displayed on the top left and bottom right of the image.',\n",
       "  'gen_answer': 'IEEE Transactions on Geoscience and Remote Sensing',\n",
       "  'context': ['put the answer in a single word or phrase, as shown in Ta-\\nble 1. Our approach does not employ any task identifica-\\ntion tokens for vision-irrelevant commands. This unified\\napproach is supported by a modular design that efficiently\\nintegrates spatial data, giving the model flexibility in its rea-\\nsoning about visual content.\\nSpatial Location Representation. Our model must pre-\\ncisely identify the spatial position of the referenced items',\n",
       "   '5.3. Visual Question Answering\\nDatasets for evaluation. RSVQA-HRBEN [20] comprises\\n10,569 high-resolution photos and 1,066,316 question-\\nanswer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-\\nvided into training, validation, test 1, and test 2 sets, re-\\nspectively. This dataset has three question types: presence,\\n7\\n27837',\n",
       "   'cally provide system instructions as prompts that ask Vi-\\ncuna [7] to generate multi-round question and answer pairs\\nin a manner as if it could visualize the image (although it\\nonly has access to the text). This is achieved by providing\\nfew-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied. Specifically, from our short descriptions',\n",
       "   'Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video']},\n",
       " 'How is the visually grounded response presented?': {'uuid': '33890465-ae6c-42b4-85cf-b5d2e06849fc',\n",
       "  'ref_answer': 'The visually grounded response is presented as text with corresponding object locations shown on top.',\n",
       "  'gen_answer': 'Visually grounded responses are presented as textual answers that may be interleaved with corresponding object locations. Users can also provide visual prompts like bounding boxes, and the model should respond about the specified Region of Interest (RoI).',\n",
       "  'context': ['ther, RSGPT cannot work for region-level reasoning or vi-\\nsual grounding, which our work aims to address.\\n3. GeoChat: Grounded Remote Sensing VLM\\nVisually grounded conversations for remote sensing aim to\\ngenerate textual responses interleaved with corresponding\\nobject locations. Further, a user can also provide visual\\nprompts (e.g., a bounding box) besides natural language\\nquestions, and the model should be able to answer questions\\nabout the specified Region of Interest (RoI). Such seamless',\n",
       "   '15.9% on LLaV A-v1.5 [17], as shown in Table 8.\\n5.4. Visual Grounding\\nDatasets for evaluation. For the evaluation of grounding\\ntasks, we propose a new benchmark that contains different\\nreferring and grounding tasks. We use the validation set\\nfrom [31] and used the same dataset creation pipeline as in\\nSec. 4 to construct the test benchmark. There are a total\\nof 7593 [refer], 560 [grounding], and 495 grounding de-\\nscription questions, as well as 2793 for region captioning.',\n",
       "   'ity to easily switch between different types of remote sens-\\ning visual interpretation tasks. To eliminate uncertainty\\namong tasks, our approach assigns a unique task identifi-\\ncation to each one. We suggest three distinct task identities,\\nt‚àà{grounding, identify, refer }, each for grounded con-\\nversations, region captioning and referring expression com-\\nprehension. As for the case of visual question answering\\nand scene classification, we directly ask the model to out-',\n",
       "   'Figure 5. Qualitative results of GeoChat. ( left-right ) Results are shown on grounding, referring object detection, and disaster/damage\\ndetection. The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior. The\\nmodel can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left).']},\n",
       " 'Which models are mentioned as examples of effective self-supervised vision-language modeling?': {'uuid': '30143afb-0d70-4766-87d8-f491ba2e7077',\n",
       "  'ref_answer': 'Multimodal GPT-4 and LLaVA are mentioned as examples.',\n",
       "  'gen_answer': 'Multimodal GPT-4, LLaVA, InstructBLIP, Otter, MiniGPT-4',\n",
       "  'context': ['tom). This makes it the first RS VLM with grounding capability.\\ntations facilitate effective self-supervised vision-language\\nmodeling, as demonstrated by multimodal GPT-4 [23] and\\nopen-source initiatives like LLaV A [19]. These vision-\\nlanguage models (VLMs), developed through generative\\npretraining and instruction-tuning, exhibit robust zero-shot\\ntask completion across various user-oriented multimodal\\ntasks. The resulting capabilities open the door to the de-',\n",
       "   'with language models. The results achieved with VLMs\\nshow great promise; for example, LLaV A [18], Instruct-\\nBLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable\\ngains in language instruction following and visual reasoning\\nability for natural scenes. More recent studies have shown\\nthat these models can be adapted to other domains such as\\nvideos [22], biomedical [14, 30] and remote sensing [12].\\nRemote Sensing VLMs. The application of generalized',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   'Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\\nFung, and Steven Hoi. Instructblip: Towards general-\\npurpose vision-language models with instruction tuning,\\n2023. 2\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. ICLR , 2021. 2']},\n",
       " 'What issue can arise when general-domain VLMs are presented with spatial images from RS sensors?': {'uuid': 'd71dac09-bdc9-4f37-8e0c-75306aff5dbf',\n",
       "  'ref_answer': 'General-domain VLMs can provide inaccurate information or hallucinate when presented with spatial images from RS sensors.',\n",
       "  'gen_answer': 'General-domain VLMs can provide inaccurate information or hallucinate when presented with spatial images from remote sensing (RS) sensors.',\n",
       "  'context': ['motely sensed visual imagery. The performance disparity\\narises primarily from the distinct nature of content found\\nin remote sensing image-text pairings compared to the pub-\\nlicly available web data. As a result, general-domain VLMs\\ncan provide inaccurate information or hallucinate when pre-\\nsented with spatial images from RS sensors. Although there\\nhas been significant progress in the field of remote sensing\\nvisual question answering (VQA) [39, 41], earlier meth-',\n",
       "   'mains, allowing users to hold a dialogue about given vi-\\nsual content. However, such general-domain VLMs perform\\npoorly for Remote Sensing (RS) scenarios, leading to inac-\\ncurate or fabricated information when presented with RS\\ndomain-specific queries. Such a behavior emerges due to\\nthe unique challenges introduced by RS imagery. For exam-\\nple, to handle high-resolution RS imagery with diverse scale\\nchanges across categories and many small objects, region-',\n",
       "   'velopment of versatile multimodal conversational assistants\\nwith broad applications in real-world scenarios [12].\\nHowever, general-domain VLMs designed for natural\\nimages, exhibit poor performance when presented with re-\\n1\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\nExcept for this watermark, it is identical to the accepted version;\\nthe final published version of the proceedings is available on IEEE Xplore.\\n27831',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-']},\n",
       " 'What does the classification problem limit?': {'uuid': '227dfebb-21ab-41c2-9236-45b06554f901',\n",
       "  'ref_answer': 'The classification problem limits the applicability to open-ended answer generation and instruction-following.',\n",
       "  'gen_answer': 'The classification problem focuses on categorizing remote sensing images into different scene classes without limiting specific image sizes or class types. It aims for diversity in datasets to ensure comprehensive coverage and accurate scene identification across various contexts.',\n",
       "  'context': ['visual question answering and scene classification datasets with\\nvarying image sizes and types of classes to ensure diversity.\\nConstituent Datasets: In the compilation of our in-\\nstruction set, we incorporate three distinct types of datasets,\\nencompassing the ones designed for object detection,\\nscene classification, and visual question answering (VQA).\\nSpecifically, we integrate three object detection (DOTA\\n[35], DIOR [6], and FAIR1M [27] which together form',\n",
       "   'rankrset to 64 in our implementation. The model under-\\ngoes training consistently at an image resolution of 504 √ó\\n504 throughout the whole process. Each training step incor-\\nporates specifically crafted multi-modal instructional tem-\\nplates designed for a variety of vision-language tasks during\\nthe training process. We use AdamW [21] optimizer with a\\ncosine learning rate scheduler to train our model. We keep\\nthe global batch size as 144. We train our model in two',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE',\n",
       "   'Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 7, 8\\n[5] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-\\ning image scene classification: Benchmark and state of the\\nart.Proceedings of the IEEE , 105(10):1865‚Äì1883, 2017. 2,\\n3, 5\\n[6] Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo']},\n",
       " 'What does GeoChat aim to do with respect to local regions?': {'uuid': 'fb598345-793b-48e1-9800-48353185955b',\n",
       "  'ref_answer': 'GeoChat aims to seamlessly reason about local regions.',\n",
       "  'gen_answer': \"GeoChat aims to perform tasks that involve providing spatial box locations in the input, guiding the model's attention to specific regions within the image. This allows for region-level reasoning and visual grounding, addressing limitations of previous models like RSGPT.\",\n",
       "  'context': ['The goal is to perform conversation-based tasks at a holistic\\nlevel with image-wide context, such as visual question an-\\nswering (VQA), scene classification and image captioning.\\nb) Region-Level Conversation Tasks. This task in-\\nvolves providing spatial box locations bin the input to\\nGeoChat besides xandq. Region locations bguide the\\nmodel‚Äôs attention to specific regions within the image, so\\nthat the model can perform tasks such as region-level cap-',\n",
       "   'ther, RSGPT cannot work for region-level reasoning or vi-\\nsual grounding, which our work aims to address.\\n3. GeoChat: Grounded Remote Sensing VLM\\nVisually grounded conversations for remote sensing aim to\\ngenerate textual responses interleaved with corresponding\\nobject locations. Further, a user can also provide visual\\nprompts (e.g., a bounding box) besides natural language\\nquestions, and the model should be able to answer questions\\nabout the specified Region of Interest (RoI). Such seamless',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.']},\n",
       " 'How does GeoChat assist users in their requirements?': {'uuid': 'fb598345-793b-48e1-9800-48353185955b',\n",
       "  'ref_answer': \"GeoChat assists users by directing the model's responses according to user requirements through distinct task tokens.\",\n",
       "  'gen_answer': \"GeoChat assists users by generating visually grounded responses, performing visual question answering on images and regions, classifying scenes, conducting normal natural language conversations, providing object locations at different granularities for tasks like grounded image captioning/conversation, object grounding, and referring expression detection. Users can guide GeoChat's behavior using task-specific tokens to shape its responses according to the desired level of detail or context.\",\n",
       "  'context': ['Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'tioning, region-specific VQA or multi-turn conversation.\\nc) Grounded Conversation Tasks. With the use of spe-\\ncial tokens, termed as task-specification tokens t, GeoChat\\ncan be guided to provide object locations at different gran-\\nularities, while maintaining conversation abilities. It helps\\nin tasks including grounded image captioning/conversation,\\nobject grounding and referring expression detection.\\n3.1. GeoChat Architecture\\nGeoChat follows the architecture as of LLaV A-v1.5 [17],',\n",
       "   'Figure 5. Qualitative results of GeoChat. ( left-right ) Results are shown on grounding, referring object detection, and disaster/damage\\ndetection. The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior. The\\nmodel can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left).',\n",
       "   'soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.']},\n",
       " 'What additional abilities were added using their corresponding datasets?': {'uuid': 'b067adf7-31fb-496f-924b-e2806dd788cb',\n",
       "  'ref_answer': 'Visual question-answering and scene classification abilities',\n",
       "  'gen_answer': 'Visual question answering and scene classification abilities were added using their respective datasets.',\n",
       "  'context': ['visual question answering and scene classification datasets with\\nvarying image sizes and types of classes to ensure diversity.\\nConstituent Datasets: In the compilation of our in-\\nstruction set, we incorporate three distinct types of datasets,\\nencompassing the ones designed for object detection,\\nscene classification, and visual question answering (VQA).\\nSpecifically, we integrate three object detection (DOTA\\n[35], DIOR [6], and FAIR1M [27] which together form',\n",
       "   'In total, the AID [34] dataset has 10,000 images within 30\\nclasses. The images have been taken from different coun-\\ntries as well as different weather conditions. For evaluation,\\nwe use a 20% split of the AID [34] dataset. UCMerced [36]\\nis a Land Use scene classification dataset, with 2,100 im-\\nages and 21 classes. Each image is of size 256 √ó256. We\\nuse the whole UCMerced [36] dataset as a zero-shot test set.\\nResults. We prompt the models with all of the',\n",
       "   'ject detection dataset [31] to create short descriptions of\\nthe images, followed by using Vicuna-v1.5 [7] to cre-\\nate conversations using the generated text alone. Further,\\nwe add visual question-answering and scene classification\\nabilities using their corresponding datasets [5, 20]. This\\nresults in a total of 318kinstruction pairs for RS domain.\\n‚Ä¢GeoChat . Leveraging our dataset, we finetune LLaV A-\\n1.5 [14] to create the remote sensing-domain vision-',\n",
       "   '15.9% on LLaV A-v1.5 [17], as shown in Table 8.\\n5.4. Visual Grounding\\nDatasets for evaluation. For the evaluation of grounding\\ntasks, we propose a new benchmark that contains different\\nreferring and grounding tasks. We use the validation set\\nfrom [31] and used the same dataset creation pipeline as in\\nSec. 4 to construct the test benchmark. There are a total\\nof 7593 [refer], 560 [grounding], and 495 grounding de-\\nscription questions, as well as 2793 for region captioning.']},\n",
       " 'What is the name of the model that was fine-tuned to create the remote sensing-domain vision-language model?': {'uuid': 'b067adf7-31fb-496f-924b-e2806dd788cb',\n",
       "  'ref_answer': 'LLaVA-1.5',\n",
       "  'gen_answer': 'RSGPT',\n",
       "  'context': ['VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   '[4] (Table 9). As for region-level captioning, we evaluate\\nboth models based on the text accuracy with ground truth\\nregion-level captions (Table 10). Our model significantly\\noutperforms MiniGPT-4-v2 in terms of ROUGE and ME-\\nTEOR score.\\n6. Conclusion\\nAlthough recent advancements in large Vision-Language\\nModels (VLMs) have shown promise in nature image do-\\nmains, their performance in Remote Sensing (RS) scenar-\\nios is still limited due to the unique domain-specific chal-',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-']},\n",
       " 'Which version of LLaVA was fine-tuned for the GeoChat project?': {'uuid': 'b067adf7-31fb-496f-924b-e2806dd788cb',\n",
       "  'ref_answer': '1.5',\n",
       "  'gen_answer': 'LLaVa-1.5',\n",
       "  'context': ['language model - GeoChat. Our LoRA [11] fine-tuning is\\nefficient and avoids forgetting the necessary context em-\\nbedded in fully-tuned LLaV A model, whose MLP pro-\\njection is trained to align images into the word embed-\\nding space of the LLM (Vicuna-v1.5 [7]). This allowsGeoChat to retain the conversation and instruction follow-\\ning abilities of LLaV A and extend its domain-knowledge\\nto remote sensing tasks.\\n‚Ä¢ We also address the lack of evaluation benchmarks to as-',\n",
       "   'Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol-\\nlow the instructions provided for this specific task and re-\\nturns unrelated classes that are not a part of the dataset. It‚Äôs\\naccuracy is close to 5% if we pass the answers from Vicuna-\\nv1.5 [7] and ask it to check if the output sentence refers to\\nthe ground truth class or not. In comparison, Qwen-VL and\\nLLaVa-1.5 perform well in instruction following, but fall\\nshort to GeoChat, due to lack of domain knowledge.',\n",
       "   'Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.',\n",
       "   'instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-']},\n",
       " \"What aspect of remote sensing tasks is mentioned regarding GeoChat's capabilities?\": {'uuid': '9ffea684-e225-4c78-bfa6-80d144a39bb7',\n",
       "  'ref_answer': \"GeoChat's domain-knowledge is extended to remote sensing tasks.\",\n",
       "  'gen_answer': \"GeoChat's capabilities in multitask conversational functionalities with high-resolution remote-sensing images are mentioned.\",\n",
       "  'context': ['Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.',\n",
       "   'lenges. Addressing this gap, we present GeoChat, the\\nfirst unified remote sensing VLM that excels in multitask\\nconversational capabilities with high-resolution RS images.\\nGeoChat not only answers image-level queries but also en-\\ngages in region-specific dialogue, grounding responses with\\nprecise spatial coordinates. We create a novel RS mul-\\ntimodal instruction-following dataset comprising of 318k\\nimage-instruction pairs with a diverse multitask format.',\n",
       "   'sess the capability of existing VLMs on remote-sensing\\nconversations. To this end, we setup evaluation proto-\\ncols for conversation grounding in RS, as well as a setup\\na suite of tasks to allow comparisons with future efforts\\nin this direction. We show various supervised as well as\\nzero-shot evaluations for different remote sensing tasks,\\nincluding image captioning, visual question answering\\nand scene classification to demonstrate the generalisabil-\\nity of GeoChat conversational VLM.']},\n",
       " 'What does the text suggest about the future of remote sensing research?': {'uuid': 'd9dc2c4d-0a58-42ca-a2bd-874acc22baea',\n",
       "  'ref_answer': 'The text suggests that the setup of evaluation protocols and a suite of tasks will allow comparisons with future efforts in remote sensing research.',\n",
       "  'gen_answer': 'The text suggests that there is a need to enhance semantic understanding and deep visual comprehension in remote sensing, moving beyond mere object identification. Vision-language models have the capability to generate natural language descriptions and infer connections between objects, making them suitable for tasks like text-based image processing. This indicates a future trend of integrating advanced language models into remote sensing research for improved analysis and interpretation.',\n",
       "  'context': ['Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840',\n",
       "   'Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   'IEEE Transactions on Geoscience and Remote Sensing , 60:\\n1‚Äì11, 2022. 2, 7\\n[40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring\\ndata and models for visual grounding on remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 61:\\n1‚Äì13, 2023. 6\\n[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua\\nChen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-\\ntial hierarchical reasoning network for remote sensing visual\\nquestion answering. IEEE Transactions on Geoscience and']},\n",
       " 'What abilities of VLMs are particularly noteworthy according to the text?': {'uuid': 'f4a6cb93-c6de-4973-8c54-53ab68d4b951',\n",
       "  'ref_answer': 'Language instruction following and visual reasoning ability for natural scenes',\n",
       "  'gen_answer': 'Noteworthy abilities of VLMs include remarkable gains in language instruction following and visual reasoning ability for natural scenes, robust zero-shot task completion across various user-oriented multimodal tasks, and grounding capability.',\n",
       "  'context': ['with language models. The results achieved with VLMs\\nshow great promise; for example, LLaV A [18], Instruct-\\nBLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable\\ngains in language instruction following and visual reasoning\\nability for natural scenes. More recent studies have shown\\nthat these models can be adapted to other domains such as\\nvideos [22], biomedical [14, 30] and remote sensing [12].\\nRemote Sensing VLMs. The application of generalized',\n",
       "   'tom). This makes it the first RS VLM with grounding capability.\\ntations facilitate effective self-supervised vision-language\\nmodeling, as demonstrated by multimodal GPT-4 [23] and\\nopen-source initiatives like LLaV A [19]. These vision-\\nlanguage models (VLMs), developed through generative\\npretraining and instruction-tuning, exhibit robust zero-shot\\ntask completion across various user-oriented multimodal\\ntasks. The resulting capabilities open the door to the de-',\n",
       "   'ity of GeoChat conversational VLM.\\n2. Related Work\\nLarge Vision-Language Models. The typical architecture\\nof instruction-following Vision Language Models (VLMs)\\nconsists of utilising a pre-trained visual backbone[9] to en-\\ncode visual data, a large language model [7] for interpreting\\nuser instructions and generating responses, and a vision-\\nlanguage cross-modal connector, e.g., a linear projection\\nlayer [18, 42] or an MLP [17], for fusing visual information',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-']},\n",
       " 'What does VLM stand for?': {'uuid': '6f43958e-0330-4304-8473-93794cd37bc6',\n",
       "  'ref_answer': 'Vision-Language Models',\n",
       "  'gen_answer': 'VLM stands for Vision Language Model.',\n",
       "  'context': ['ity of GeoChat conversational VLM.\\n2. Related Work\\nLarge Vision-Language Models. The typical architecture\\nof instruction-following Vision Language Models (VLMs)\\nconsists of utilising a pre-trained visual backbone[9] to en-\\ncode visual data, a large language model [7] for interpreting\\nuser instructions and generating responses, and a vision-\\nlanguage cross-modal connector, e.g., a linear projection\\nlayer [18, 42] or an MLP [17], for fusing visual information',\n",
       "   'with language models. The results achieved with VLMs\\nshow great promise; for example, LLaV A [18], Instruct-\\nBLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable\\ngains in language instruction following and visual reasoning\\nability for natural scenes. More recent studies have shown\\nthat these models can be adapted to other domains such as\\nvideos [22], biomedical [14, 30] and remote sensing [12].\\nRemote Sensing VLMs. The application of generalized',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE',\n",
       "   'tom). This makes it the first RS VLM with grounding capability.\\ntations facilitate effective self-supervised vision-language\\nmodeling, as demonstrated by multimodal GPT-4 [23] and\\nopen-source initiatives like LLaV A [19]. These vision-\\nlanguage models (VLMs), developed through generative\\npretraining and instruction-tuning, exhibit robust zero-shot\\ntask completion across various user-oriented multimodal\\ntasks. The resulting capabilities open the door to the de-']},\n",
       " 'What kind of knowledge is required for answering visual questions in remote sensing?': {'uuid': '094e8a64-c881-4f68-a1b2-45f426f4d9b9',\n",
       "  'ref_answer': 'Both visual and linguistic knowledge is required for answering visual questions in remote sensing.',\n",
       "  'gen_answer': 'Remote sensing requires both visual and linguistic knowledge to answer visual questions effectively. Visual comprehension of the image content, understanding relationships between objects, generating natural language descriptions of images, and inferring connections between objects are essential for tasks like text-based image processing.',\n",
       "  'context': ['age retrieval, captioning images, and answering visual ques-\\ntions that call for both visual and linguistic knowledge. Al-\\nthough there has been progress in vision language models\\nfor remote sensing tasks, such as image captioning [43],\\nzero-shot classification [16] and visual question answering\\n[3, 39], these models can only perform a specific task they\\nare trained for, lack conversational capability and do not\\npossess generic semantic knowledge about the remote sens-',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   'Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,',\n",
       "   'Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video']},\n",
       " 'What is the main gap in the remote sensing domain?': {'uuid': 'd22efc72-1bb0-4ea2-932f-f35dbc2eaa31',\n",
       "  'ref_answer': 'The main gap in the remote sensing domain is the lack of general-purpose models that can solve all tasks together while maintaining conversation abilities.',\n",
       "  'gen_answer': 'The main gap in the remote sensing domain is the lack of general-purpose models that can solve multiple tasks together while maintaining conversation abilities.',\n",
       "  'context': ['mains, allowing users to hold a dialogue about given vi-\\nsual content. However, such general-domain VLMs perform\\npoorly for Remote Sensing (RS) scenarios, leading to inac-\\ncurate or fabricated information when presented with RS\\ndomain-specific queries. Such a behavior emerges due to\\nthe unique challenges introduced by RS imagery. For exam-\\nple, to handle high-resolution RS imagery with diverse scale\\nchanges across categories and many small objects, region-',\n",
       "   'ing images. A major gap exists in the remote sensing do-\\nmain towards developing general-purpose models to solve\\nall tasks together, while also maintaining conversation abil-\\nities. While RSGPT [12] is an initial effort that has shown\\ngood conversation ability along with solving multiple tasks,\\nit requires finetuning the model for each task separately,\\nwhich makes it cumbersome and not generalizable. Fur-\\n2\\n27832',\n",
       "   'motely sensed visual imagery. The performance disparity\\narises primarily from the distinct nature of content found\\nin remote sensing image-text pairings compared to the pub-\\nlicly available web data. As a result, general-domain VLMs\\ncan provide inaccurate information or hallucinate when pre-\\nsented with spatial images from RS sensors. Although there\\nhas been significant progress in the field of remote sensing\\nvisual question answering (VQA) [39, 41], earlier meth-',\n",
       "   'Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,']},\n",
       " 'What is the main focus of GeoChat?': {'uuid': 'e85ef725-1e7f-4182-8e79-d0ac232609a2',\n",
       "  'ref_answer': 'GeoChat focuses on visually grounded conversations for remote sensing, generating textual responses with corresponding object locations.',\n",
       "  'gen_answer': 'The main focus of GeoChat is to perform conversation-based tasks on remote-sensing (RS) images comprehensively. It aims to handle various tasks spanning different levels of granularity in visual imagery understanding, such as image/region captioning, referring object detection, and scene classification, both at the image level and region level. GeoChat utilizes task-specification tokens to provide spatial guidance for object locations during conversations while maintaining its ability to conduct normal natural language interactions about remotely sensed images.',\n",
       "  'context': ['Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.',\n",
       "   'tioning, region-specific VQA or multi-turn conversation.\\nc) Grounded Conversation Tasks. With the use of spe-\\ncial tokens, termed as task-specification tokens t, GeoChat\\ncan be guided to provide object locations at different gran-\\nularities, while maintaining conversation abilities. It helps\\nin tasks including grounded image captioning/conversation,\\nobject grounding and referring expression detection.\\n3.1. GeoChat Architecture\\nGeoChat follows the architecture as of LLaV A-v1.5 [17],',\n",
       "   'The goal is to perform conversation-based tasks at a holistic\\nlevel with image-wide context, such as visual question an-\\nswering (VQA), scene classification and image captioning.\\nb) Region-Level Conversation Tasks. This task in-\\nvolves providing spatial box locations bin the input to\\nGeoChat besides xandq. Region locations bguide the\\nmodel‚Äôs attention to specific regions within the image, so\\nthat the model can perform tasks such as region-level cap-']},\n",
       " 'What kind of inputs does GeoChat handle for Image-Level Conversation Tasks?': {'uuid': '637f112b-2a46-4628-99b8-3a74920123ee',\n",
       "  'ref_answer': 'For Image-Level Conversation Tasks, GeoChat handles an image and a user text query as inputs.',\n",
       "  'gen_answer': 'For Image-Level Conversation Tasks, GeoChat processes an image \\\\(x\\\\) and a user text query \\\\(q\\\\), without any specific spatial coordinates in its inputs or outputs. The goal is to perform tasks at a holistic level with image-wide context, such as visual question answering (VQA), scene classification, and image captioning.',\n",
       "  'context': ['soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.',\n",
       "   'The goal is to perform conversation-based tasks at a holistic\\nlevel with image-wide context, such as visual question an-\\nswering (VQA), scene classification and image captioning.\\nb) Region-Level Conversation Tasks. This task in-\\nvolves providing spatial box locations bin the input to\\nGeoChat besides xandq. Region locations bguide the\\nmodel‚Äôs attention to specific regions within the image, so\\nthat the model can perform tasks such as region-level cap-',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'and compare with a number of baseline methods. GeoChat\\ndemonstrates robust zero-shot performance on various RS\\ntasks, e.g., image and region captioning, visual question\\nanswering, scene classification, visually grounded conver-\\nsations and referring detection. Our code is available here.\\n1. Introduction\\nIn the natural image domain, the abundance of aligned\\nimage-text data sourced from web imagery or manual anno-\\n*Equally contributing first authors.']},\n",
       " 'What is the primary goal mentioned for conversation-based tasks?': {'uuid': '01501c61-faf9-4056-aa0a-24f910e2c104',\n",
       "  'ref_answer': 'The primary goal is to perform conversation-based tasks at a holistic level with image-wide context, such as visual question answering (VQA), scene classification, and image captioning.',\n",
       "  'gen_answer': 'The primary goal is to perform conversation-based tasks at a holistic level with image-wide context, such as visual question answering (VQA), scene classification, and image captioning.',\n",
       "  'context': ['The goal is to perform conversation-based tasks at a holistic\\nlevel with image-wide context, such as visual question an-\\nswering (VQA), scene classification and image captioning.\\nb) Region-Level Conversation Tasks. This task in-\\nvolves providing spatial box locations bin the input to\\nGeoChat besides xandq. Region locations bguide the\\nmodel‚Äôs attention to specific regions within the image, so\\nthat the model can perform tasks such as region-level cap-',\n",
       "   'tioning, region-specific VQA or multi-turn conversation.\\nc) Grounded Conversation Tasks. With the use of spe-\\ncial tokens, termed as task-specification tokens t, GeoChat\\ncan be guided to provide object locations at different gran-\\nularities, while maintaining conversation abilities. It helps\\nin tasks including grounded image captioning/conversation,\\nobject grounding and referring expression detection.\\n3.1. GeoChat Architecture\\nGeoChat follows the architecture as of LLaV A-v1.5 [17],',\n",
       "   'can not only answer image-level queries but also accepts\\nregion inputs to hold region-specific dialogue. Further-\\nmore, it can visually ground objects in its responses by re-\\nferring to their spatial coordinates. To address the lack\\nof domain-specific datasets, we generate a novel RS mul-\\ntimodal instruction-following dataset by extending image-\\ntext pairs from existing diverse RS datasets. We establish\\na comprehensive benchmark for RS multitask conversations',\n",
       "   'ods have framed the task as a classification problem. Here,\\nthe model chooses answers from predetermined responses\\nfound in the training data. It limits their applicability to\\nopen-ended answer generation and instruction-following.\\nIn this paper, we introduce GeoChat, an attempt to\\nextend multimodal instruction-tuning to the remote sens-\\ning domain for training a multitask conversational assis-\\ntant. However, remote-sensing domain lacks a multimodal']},\n",
       " 'How does the model use the provided region locations?': {'uuid': '01501c61-faf9-4056-aa0a-24f910e2c104',\n",
       "  'ref_answer': \"Region locations guide the model's attention to specific regions within the image.\",\n",
       "  'gen_answer': 'The model uses spatial location representations as inputs to reason about local regions and guide its attention to specific areas within the image for tasks such as region-level classification or visual grounding.',\n",
       "  'context': ['cation [5] and SAMRS for object detection [31].\\nA crucial capability of GeoChat is the unification of mul-\\ntiple image and region-level reasoning tasks for RS imagery\\nwithin a single pipeline (see Fig. 1). We achieve this via\\ndistinct task tokens that help suitably direct the model‚Äôs re-\\nsponses according to user requirements. In addition, the\\nmodel uses spatial location representations in its inputs to\\nseamlessly reason about local regions and can also generate',\n",
       "   'The goal is to perform conversation-based tasks at a holistic\\nlevel with image-wide context, such as visual question an-\\nswering (VQA), scene classification and image captioning.\\nb) Region-Level Conversation Tasks. This task in-\\nvolves providing spatial box locations bin the input to\\nGeoChat besides xandq. Region locations bguide the\\nmodel‚Äôs attention to specific regions within the image, so\\nthat the model can perform tasks such as region-level cap-',\n",
       "   'ther, RSGPT cannot work for region-level reasoning or vi-\\nsual grounding, which our work aims to address.\\n3. GeoChat: Grounded Remote Sensing VLM\\nVisually grounded conversations for remote sensing aim to\\ngenerate textual responses interleaved with corresponding\\nobject locations. Further, a user can also provide visual\\nprompts (e.g., a bounding box) besides natural language\\nquestions, and the model should be able to answer questions\\nabout the specified Region of Interest (RoI). Such seamless',\n",
       "   'normalised within the interval [0, 100] are used to represent\\nthe x and y coordinates. Region locations in this format are\\nused to interact with the model via its inputs and outputs.\\nVisual Backbone. GeoChat adapts the pretrained vision\\nbackbone of CLIP-ViT(L-14) [28], which has an input res-\\nolution of 336 √ó336. This results in effectively 576 patches\\nper image. Since this resolution is not sufficient to un-\\nderstand details presented in remote sensing imagery (e.g.,']},\n",
       " 'What is the name of the model being discussed for region-level tasks?': {'uuid': '01501c61-faf9-4056-aa0a-24f910e2c104',\n",
       "  'ref_answer': 'The model being discussed for region-level tasks is GeoChat.',\n",
       "  'gen_answer': 'GeoChat',\n",
       "  'context': ['[4] (Table 9). As for region-level captioning, we evaluate\\nboth models based on the text accuracy with ground truth\\nregion-level captions (Table 10). Our model significantly\\noutperforms MiniGPT-4-v2 in terms of ROUGE and ME-\\nTEOR score.\\n6. Conclusion\\nAlthough recent advancements in large Vision-Language\\nModels (VLMs) have shown promise in nature image do-\\nmains, their performance in Remote Sensing (RS) scenar-\\nios is still limited due to the unique domain-specific chal-',\n",
       "   'The goal is to perform conversation-based tasks at a holistic\\nlevel with image-wide context, such as visual question an-\\nswering (VQA), scene classification and image captioning.\\nb) Region-Level Conversation Tasks. This task in-\\nvolves providing spatial box locations bin the input to\\nGeoChat besides xandq. Region locations bguide the\\nmodel‚Äôs attention to specific regions within the image, so\\nthat the model can perform tasks such as region-level cap-',\n",
       "   'cation [5] and SAMRS for object detection [31].\\nA crucial capability of GeoChat is the unification of mul-\\ntiple image and region-level reasoning tasks for RS imagery\\nwithin a single pipeline (see Fig. 1). We achieve this via\\ndistinct task tokens that help suitably direct the model‚Äôs re-\\nsponses according to user requirements. In addition, the\\nmodel uses spatial location representations in its inputs to\\nseamlessly reason about local regions and can also generate',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE']},\n",
       " 'What differentiates the described model from LLaVA?': {'uuid': 'd329ad38-5f67-425c-9d12-82eadad757e9',\n",
       "  'ref_answer': 'The described model adds specific task prompts for grounding, image-level, or region-level conversations, which is different from LLaVA.',\n",
       "  'gen_answer': 'The described model adds specific task prompts indicating the desired type of task, such as grounding, image-level, or region-level conversations. It also allows spatial positions within inputs and outputs, enabling visual prompts as inputs and grounded objects in GeoChat outputs. In contrast, LLaVA lacks these features.',\n",
       "  'context': ['with language models. The results achieved with VLMs\\nshow great promise; for example, LLaV A [18], Instruct-\\nBLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable\\ngains in language instruction following and visual reasoning\\nability for natural scenes. More recent studies have shown\\nthat these models can be adapted to other domains such as\\nvideos [22], biomedical [14, 30] and remote sensing [12].\\nRemote Sensing VLMs. The application of generalized',\n",
       "   'use our short descriptions as referring expressions to cre-\\nate three different kinds of question answering pairs, i.e.\\ngrounding image description, referring expression, and re-\\ngion level captioning, as described in Table 1.\\n5. Experiments\\n5.1. Implementation Details\\nWe initialize the weights of our model with the pretrained\\nCLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA\\n[11] finetuning. Utilizing LoRA, we refine the parameters\\nWqandWvthrough low-rank adaptation, with a designated',\n",
       "   'which consists of three core components, i) Global Im-\\nage encoder, ii) an MLP adaptor (two linear layers) and\\niii) LLM. Different to LLaV A, we add specific task prompt\\nthat indicates the type of task desired from the model i.e.,\\ngrounding, image-level or region-level conversations. Ad-\\nditionally, we allow spatial positions within both inputs and\\noutputs, enabling visual prompts as inputs and grounded\\nobjects in GeoChat outputs. Notably, the original LLaV A',\n",
       "   'captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-']},\n",
       " 'What component of the model includes two linear layers?': {'uuid': 'd329ad38-5f67-425c-9d12-82eadad757e9',\n",
       "  'ref_answer': 'The MLP adaptor includes two linear layers.',\n",
       "  'gen_answer': 'The vision-language cross-modal connector includes two linear layers.',\n",
       "  'context': ['References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   'ity of GeoChat conversational VLM.\\n2. Related Work\\nLarge Vision-Language Models. The typical architecture\\nof instruction-following Vision Language Models (VLMs)\\nconsists of utilising a pre-trained visual backbone[9] to en-\\ncode visual data, a large language model [7] for interpreting\\nuser instructions and generating responses, and a vision-\\nlanguage cross-modal connector, e.g., a linear projection\\nlayer [18, 42] or an MLP [17], for fusing visual information',\n",
       "   '[4] (Table 9). As for region-level captioning, we evaluate\\nboth models based on the text accuracy with ground truth\\nregion-level captions (Table 10). Our model significantly\\noutperforms MiniGPT-4-v2 in terms of ROUGE and ME-\\nTEOR score.\\n6. Conclusion\\nAlthough recent advancements in large Vision-Language\\nModels (VLMs) have shown promise in nature image do-\\nmains, their performance in Remote Sensing (RS) scenar-\\nios is still limited due to the unique domain-specific chal-']},\n",
       " 'Which dataset requires answering the question using a single word or phrase?': {'uuid': 'dbeac12f-0a8e-40fe-84a1-d93ab3d8464b',\n",
       "  'ref_answer': 'RSVQA-LRBEN[20]',\n",
       "  'gen_answer': 'RSVQA-HRBEN',\n",
       "  'context': ['put the answer in a single word or phrase, as shown in Ta-\\nble 1. Our approach does not employ any task identifica-\\ntion tokens for vision-irrelevant commands. This unified\\napproach is supported by a modular design that efficiently\\nintegrates spatial data, giving the model flexibility in its rea-\\nsoning about visual content.\\nSpatial Location Representation. Our model must pre-\\ncisely identify the spatial position of the referenced items',\n",
       "   'Results. We prompt the models with all of the\\nclasses and prompt to classify the image using just one\\nword/phrase. For example, we input a prompt like ‚ÄùClas-\\nsify the image within one of the given classes: dense\\nresidential area, . . . , school. Answer with one\\nword or short phrase.‚Äù . We calculate zero-shot accu-\\nracy on both AID and UCMerced. GeoChat significantly\\noutperforms other VLM‚Äôs with an accuracy of 84.43% on\\nUCMerced [36] and 72.03% on AID [34], as presented in',\n",
       "   '5.3. Visual Question Answering\\nDatasets for evaluation. RSVQA-HRBEN [20] comprises\\n10,569 high-resolution photos and 1,066,316 question-\\nanswer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-\\nvided into training, validation, test 1, and test 2 sets, re-\\nspectively. This dataset has three question types: presence,\\n7\\n27837',\n",
       "   'visual question answering and scene classification datasets with\\nvarying image sizes and types of classes to ensure diversity.\\nConstituent Datasets: In the compilation of our in-\\nstruction set, we incorporate three distinct types of datasets,\\nencompassing the ones designed for object detection,\\nscene classification, and visual question answering (VQA).\\nSpecifically, we integrate three object detection (DOTA\\n[35], DIOR [6], and FAIR1M [27] which together form']},\n",
       " 'What table describes the instruction following data used to train GeoChat?': {'uuid': 'a82c1eab-01fd-422b-abcc-fc5b1215153d',\n",
       "  'ref_answer': 'Table 1 describes the instruction following data used to train GeoChat.',\n",
       "  'gen_answer': 'Table 1 describes the instruction following data used to train GeoChat.',\n",
       "  'context': ['Referring Expression 45k [refer] < p > Object < /p >\\nTable 1. Instruction following data used to train GeoChat. Instruc-\\ntion types and format are shown. We use a 308k set for training\\nand a separate 10k instruction-set for testing.\\ninputs. Further, the original LLaV A can not reason about\\nremote sensing images which is enabled via our domain-\\nspecific dataset. We describe each component in the archi-\\ntecture as follows:\\nTask Token: The unique quality of GeoChat is its abil-',\n",
       "   'instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-',\n",
       "   'object locations in its responses to visually ground objects.\\nThis enables a diverse set of tasks possible with GeoChat\\nincluding referring expression detection, image/region cap-\\ntioning, scene classification, natural language conversations\\nand VQA, besides visually grounded conversations.\\nIn summary, this work has the following contributions:\\n‚Ä¢RS multimodal instruction following dataset . We present\\na novel data generation pipeline, to leverage existing ob-',\n",
       "   'ods have framed the task as a classification problem. Here,\\nthe model chooses answers from predetermined responses\\nfound in the training data. It limits their applicability to\\nopen-ended answer generation and instruction-following.\\nIn this paper, we introduce GeoChat, an attempt to\\nextend multimodal instruction-tuning to the remote sens-\\ning domain for training a multitask conversational assis-\\ntant. However, remote-sensing domain lacks a multimodal']},\n",
       " 'What do bxleft and bytop denote in the box representation?': {'uuid': 'cb067fb7-ef34-4ea3-93b8-81f9947b1dd7',\n",
       "  'ref_answer': 'The top left corner point of the box',\n",
       "  'gen_answer': 'bxleft and bytop denote the top left corner point of the box.',\n",
       "  'context': ['for tasks such as grounded conversations, referring expres-\\nsion generation, and comprehension. To this end, we repre-\\nsent the box locations in a textual format to express the ge-\\nographical position: b={bxleft, bytop, bxright, bybottom|Œ∏}.\\nHere, bxleft, bytopdenote the top left corner point of box\\nwhile the bxright, bybottom represent the bottom right corner\\ncoordinates. The angle Œ∏represents the angle of rotation for\\nthe bounding box, from the lower edge. Numerical values',\n",
       "   'a 3√ó3 grid, defining regions such as Top Right, Top, Top\\nLeft, Left, Center, Right, Bottom Right, Bottom Left, and\\nBottom. Based on the object‚Äôs center pixel coordinates, we\\nassign its relative position accordingly.\\nTo define the relation between objects in a given image,\\nwe group different objects based on their distance between\\nthe bounding boxes, and for each sub-graph, we assign dif-\\nferent relationships between objects based on their class la-',\n",
       "   'work. To accomplish different vision-language tasks, we\\ndirectly depend on the Vicuna-v1.5(7B) [7] language to-\\nkens. We explicitly interact with the language model to\\nconstruct textual representations of bounding boxes to ex-\\npress their spatial coordinates for the visual grounding tasks\\nthat require the production of spatial locations. Similarly,\\nthe safe, aligned and effective behavior of LLM is ensured\\nvia system prompts appended together with given inputs. A',\n",
       "   '\\nObject Attributes and Relationships1.Ship{white,small,top-left},{grey,small,bottom-right}2.Harbor{small,right},{large,bottom-left},{small,topright},{small,left}3.Trees{top-right}.Referring Expression1.SmallGreyshipanchoredatlargeharboronbottom-right2.SomeTreesatthetop-right.3.Whiteshipatthetopofasmallharborontheright.Region Captioning1.Objectlocatedat{}.2.Sizeofobjectat.{}issmall.Figure 4. Types of annotations available in the GeoChat instruction-set. Top-row: For a given RS image, we obtain object attribute and']},\n",
       " 'What interval are the x and y coordinates normalised within?': {'uuid': 'faa82d57-2e4c-4ade-8839-63cd3c9bb14f',\n",
       "  'ref_answer': 'The x and y coordinates are normalised within the interval [0, 100].',\n",
       "  'gen_answer': 'The x and y coordinates are normalised within the interval [0, 100].',\n",
       "  'context': ['normalised within the interval [0, 100] are used to represent\\nthe x and y coordinates. Region locations in this format are\\nused to interact with the model via its inputs and outputs.\\nVisual Backbone. GeoChat adapts the pretrained vision\\nbackbone of CLIP-ViT(L-14) [28], which has an input res-\\nolution of 336 √ó336. This results in effectively 576 patches\\nper image. Since this resolution is not sufficient to un-\\nderstand details presented in remote sensing imagery (e.g.,',\n",
       "   'for tasks such as grounded conversations, referring expres-\\nsion generation, and comprehension. To this end, we repre-\\nsent the box locations in a textual format to express the ge-\\nographical position: b={bxleft, bytop, bxright, bybottom|Œ∏}.\\nHere, bxleft, bytopdenote the top left corner point of box\\nwhile the bxright, bybottom represent the bottom right corner\\ncoordinates. The angle Œ∏represents the angle of rotation for\\nthe bounding box, from the lower edge. Numerical values',\n",
       "   'a 3√ó3 grid, defining regions such as Top Right, Top, Top\\nLeft, Left, Center, Right, Bottom Right, Bottom Left, and\\nBottom. Based on the object‚Äôs center pixel coordinates, we\\nassign its relative position accordingly.\\nTo define the relation between objects in a given image,\\nwe group different objects based on their distance between\\nthe bounding boxes, and for each sub-graph, we assign dif-\\nferent relationships between objects based on their class la-',\n",
       "   'vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.']},\n",
       " 'What is the implication of the insufficiency of the resolution for remote sensing imagery?': {'uuid': 'faa82d57-2e4c-4ade-8839-63cd3c9bb14f',\n",
       "  'ref_answer': 'The implication is that finer details in remote sensing imagery may not be adequately captured or understood.',\n",
       "  'gen_answer': 'The insufficiency of resolution in remote sensing imagery leads to challenges such as handling diverse scale changes across categories and many small objects, which are critical for accurate analysis and interpretation. High-resolution imagery requires advanced processing capabilities to effectively manage these complexities.',\n",
       "  'context': ['mains, allowing users to hold a dialogue about given vi-\\nsual content. However, such general-domain VLMs perform\\npoorly for Remote Sensing (RS) scenarios, leading to inac-\\ncurate or fabricated information when presented with RS\\ndomain-specific queries. Such a behavior emerges due to\\nthe unique challenges introduced by RS imagery. For exam-\\nple, to handle high-resolution RS imagery with diverse scale\\nchanges across categories and many small objects, region-',\n",
       "   'Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   'motely sensed visual imagery. The performance disparity\\narises primarily from the distinct nature of content found\\nin remote sensing image-text pairings compared to the pub-\\nlicly available web data. As a result, general-domain VLMs\\ncan provide inaccurate information or hallucinate when pre-\\nsented with spatial images from RS sensors. Although there\\nhas been significant progress in the field of remote sensing\\nvisual question answering (VQA) [39, 41], earlier meth-']},\n",
       " 'What is being interpolated in the transformer-based CLIP model?': {'uuid': '7a2d75df-ef26-44aa-929a-249cd555b9d1',\n",
       "  'ref_answer': 'The positional encoding is being interpolated in the transformer-based CLIP model.',\n",
       "  'gen_answer': 'Positional encoding.',\n",
       "  'context': ['small objects and object details), we interpolate the posi-\\ntional encoding in the transformer-based CLIP [28] model\\nto scale with input image sizes of 504 √ó504. Although this\\n3\\n27833',\n",
       "   'Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\\nFung, and Steven Hoi. Instructblip: Towards general-\\npurpose vision-language models with instruction tuning,\\n2023. 2\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. ICLR , 2021. 2',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE',\n",
       "   'relationship information, referring expressions and region captions along with their corresponding region annotations (shown over the\\nimage). Bottom-row: This structured information is used to create the rich instruction-set with a total of 318k image-instruction pairs.\\ning related tasks. We use a pre-trained CLIP-ViT(L-14)\\nencoder[28],trained on large amounts of textual and visual\\ndata, a pretrained MLP adaptor[17], pretrained on a 558K\\nsubset of the LAION-CC-SBU [26] dataset with BLIP [15]']},\n",
       " 'What does CLIP stand for in the context of the transformer-based model mentioned?': {'uuid': '7a2d75df-ef26-44aa-929a-249cd555b9d1',\n",
       "  'ref_answer': 'CLIP stands for Contrastive Language-Image Pre-training.',\n",
       "  'gen_answer': 'CLIP stands for Convolutional Layer with Implicit Parameters. In this context, it refers to a model that combines textual and visual data for tasks like question answering and image description generation.',\n",
       "  'context': ['small objects and object details), we interpolate the posi-\\ntional encoding in the transformer-based CLIP [28] model\\nto scale with input image sizes of 504 √ó504. Although this\\n3\\n27833',\n",
       "   'relationship information, referring expressions and region captions along with their corresponding region annotations (shown over the\\nimage). Bottom-row: This structured information is used to create the rich instruction-set with a total of 318k image-instruction pairs.\\ning related tasks. We use a pre-trained CLIP-ViT(L-14)\\nencoder[28],trained on large amounts of textual and visual\\ndata, a pretrained MLP adaptor[17], pretrained on a 558K\\nsubset of the LAION-CC-SBU [26] dataset with BLIP [15]',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE',\n",
       "   'Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\\nFung, and Steven Hoi. Instructblip: Towards general-\\npurpose vision-language models with instruction tuning,\\n2023. 2\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. ICLR , 2021. 2']},\n",
       " 'What is processed along with the image input in GeoChat?': {'uuid': '94b6816d-3791-43e5-a3e3-6a3068a63c8c',\n",
       "  'ref_answer': 'a user query',\n",
       "  'gen_answer': 'In GeoChat, a user text query is processed along with the image input.',\n",
       "  'context': ['soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'and compare with a number of baseline methods. GeoChat\\ndemonstrates robust zero-shot performance on various RS\\ntasks, e.g., image and region captioning, visual question\\nanswering, scene classification, visually grounded conver-\\nsations and referring detection. Our code is available here.\\n1. Introduction\\nIn the natural image domain, the abundance of aligned\\nimage-text data sourced from web imagery or manual anno-\\n*Equally contributing first authors.',\n",
       "   \"<System Message>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to thehuman's questions.. Figure 2. An overview of GeoChat - the first grounded large vision-language model for remote sensing. Given an image input together\\nwith a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\"]},\n",
       " 'What component projects the output tokens from the frozen CLIP-ViT?': {'uuid': 'ef7dfbfb-f407-4baa-a301-34a23b37e63b',\n",
       "  'ref_answer': 'The MLP Cross-modal Adaptor projects the output tokens from the frozen CLIP-ViT.',\n",
       "  'gen_answer': 'MLP Adaptor',\n",
       "  'context': ['Large Language Model\\nüî•MLP Adaptor \\n‚ùÑCLIP-ViT\\n‚ùÑ\\n<im_end> [task specific prompts] <im_start>[USER]: [identify] What is present in this area?[USER]:[refer] Football ground on the right side, surrounded by trees.\\nReferring Expression\\nImage Description\\nRegion  captioning\\nVisual QA\\nSure!',\n",
       "   'small objects and object details), we interpolate the posi-\\ntional encoding in the transformer-based CLIP [28] model\\nto scale with input image sizes of 504 √ó504. Although this\\n3\\n27833',\n",
       "   'and-vision assistant for biomedicine in one day. arXiv\\npreprint arXiv:2306.00890 , 2023. 2\\n[15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\\nBlip: Bootstrapping language-image pre-training for uni-\\nfied vision-language understanding and generation. In In-\\nternational Conference on Machine Learning , pages 12888‚Äì\\n12900. PMLR, 2022. 5\\n[16] Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou. Rs-clip:\\nZero shot remote sensing scene classification via contrastive',\n",
       "   'which consists of three core components, i) Global Im-\\nage encoder, ii) an MLP adaptor (two linear layers) and\\niii) LLM. Different to LLaV A, we add specific task prompt\\nthat indicates the type of task desired from the model i.e.,\\ngrounding, image-level or region-level conversations. Ad-\\nditionally, we allow spatial positions within both inputs and\\noutputs, enabling visual prompts as inputs and grounded\\nobjects in GeoChat outputs. Notably, the original LLaV A']},\n",
       " 'Is the language model used in GeoChat open source?': {'uuid': 'ffb10c2c-e465-4e41-9e07-279e85b62f9b',\n",
       "  'ref_answer': 'Yes, the language model used in GeoChat is open source.',\n",
       "  'gen_answer': 'No information about the openness of the language model used in GeoChat is provided.',\n",
       "  'context': ['language model - GeoChat. Our LoRA [11] fine-tuning is\\nefficient and avoids forgetting the necessary context em-\\nbedded in fully-tuned LLaV A model, whose MLP pro-\\njection is trained to align images into the word embed-\\nding space of the LLM (Vicuna-v1.5 [7]). This allowsGeoChat to retain the conversation and instruction follow-\\ning abilities of LLaV A and extend its domain-knowledge\\nto remote sensing tasks.\\n‚Ä¢ We also address the lack of evaluation benchmarks to as-',\n",
       "   \"<System Message>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to thehuman's questions.. Figure 2. An overview of GeoChat - the first grounded large vision-language model for remote sensing. Given an image input together\\nwith a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\",\n",
       "   'instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-',\n",
       "   'sess the capability of existing VLMs on remote-sensing\\nconversations. To this end, we setup evaluation proto-\\ncols for conversation grounding in RS, as well as a setup\\na suite of tasks to allow comparisons with future efforts\\nin this direction. We show various supervised as well as\\nzero-shot evaluations for different remote sensing tasks,\\nincluding image captioning, visual question answering\\nand scene classification to demonstrate the generalisabil-\\nity of GeoChat conversational VLM.']},\n",
       " 'Which adaptation strategy is mentioned in the text?': {'uuid': '7d8f01da-3650-484f-a4b5-400f5cf48664',\n",
       "  'ref_answer': 'Low-Rank Adaptation (LoRA)',\n",
       "  'gen_answer': 'Low-Rank Adaptation (LoRA)',\n",
       "  'context': ['use our short descriptions as referring expressions to cre-\\nate three different kinds of question answering pairs, i.e.\\ngrounding image description, referring expression, and re-\\ngion level captioning, as described in Table 1.\\n5. Experiments\\n5.1. Implementation Details\\nWe initialize the weights of our model with the pretrained\\nCLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA\\n[11] finetuning. Utilizing LoRA, we refine the parameters\\nWqandWvthrough low-rank adaptation, with a designated',\n",
       "   '[USER] <im_start> Image Features <im_end> [Task Identifier] [ASSISTANT]Figure 3. Multi-task instruction template for GeoChat.\\nLow-Rank Adaptation (LoRA) [11] based strategy is used\\nfor fine-tuning the LLM. While training, instead of finetun-\\ning all of the weights that comprise the weight matrix of the\\npre-trained Vicuna-v1.5[7], we finetune two smaller matri-\\nces in LoRA [11] that approximate the original larger ma-\\ntrix. After that, the fine-tuned adaptor is fed into the pre-',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and',\n",
       "   'captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-']},\n",
       " 'Where is the other harbor located besides the one where the grey ship is anchored?': {'uuid': 'a5a5f5ef-a0a5-40f1-80d3-efcf925cfed4',\n",
       "  'ref_answer': 'The other harbor is located at the top right, adjacent to some trees.',\n",
       "  'gen_answer': 'The other harbor is located at the topright.',\n",
       "  'context': ['MultiTurnConversationUSER:Whereisthewhiteshippresent?ASSISTANT:Whiteshipispresentatthetopleft.USER:Whatisthecoloroftheshipanchoredattheharbor?ASSISTANT:Theshipanchoredattheharborisgreyincolor.Complex ReasoningUSER:Whatdoesthepresenceof4harborsand2shipsindicate?ASSISTANT:Thepresenceof4harborsand2shipsconstituteabustlingmaritimeenvironment.Thepresenceofsuchharborsmayindicateawaterareawiththepossibilityofexport/importsofgoodsaswellastourism.Detailed DescriptionUSER:PleaseDescribetheimageindetail.ASSISTANT:Thissatelliteimageshowsdepictsariverscenewithmultipleshipsandharborspresent.Thereareatotaloftwoshipsandfourharbors.Thegreyshipatthebottomrightisanchoredtotheharbor‚Ä¶‚Ä¶‚Ä¶..Grounded DescriptionUSER:[grounding]Describetheimageindetail.ASSISTANT:Thissatelliteimagedepictsagreyshipanchoredatalargeharbor.Thereisanotherharboratthetopright,whichisadjacenttosometrees.Thereisanotherwhiteshipclosetoaharborattheright.LLM',\n",
       "   '\\nObject Attributes and Relationships1.Ship{white,small,top-left},{grey,small,bottom-right}2.Harbor{small,right},{large,bottom-left},{small,topright},{small,left}3.Trees{top-right}.Referring Expression1.SmallGreyshipanchoredatlargeharboronbottom-right2.SomeTreesatthetop-right.3.Whiteshipatthetopofasmallharborontheright.Region Captioning1.Objectlocatedat{}.2.Sizeofobjectat.{}issmall.Figure 4. Types of annotations available in the GeoChat instruction-set. Top-row: For a given RS image, we obtain object attribute and',\n",
       "   'Ships and Harbors (e.g. ‚Äúanchored at, parked at‚Äù)\\nTrack Field and Soccer Field (e.g. ‚ÄúSurrounded by, Inside‚Äù)\\nVehicles, Bridge, Road, Roundabout (e.g. ‚Äúpassing through, passing through‚Äù)\\nVehicles and Building (e.g. ‚Äúparked‚Äù)\\nAirport and Plane (e.g. ‚Äúparked‚Äù)\\nShip and Helipad (e.g. ‚Äúon, contains‚Äù)\\nTable 4. Example of relationships between different objects used in\\nthe proposed instruction dataset.\\nand trees are missing. To address this, we propose to uti-',\n",
       "   'Give bounding box for the Boeing 747 at the bottom left.\\nSure!Inthesatelliteimage,therearefourbaseballfieldslocatednearsomebuildingsatthecenterofthescene.Thebaseballfieldsarearrangedinacloseproximitytoeachother.\\n[grounding] Please describe the image in detail.\\nYes, the road at the center is completely flooded.\\nIs the whole road flooded?\\nHow many buildings are flooded?Three buildings are flooded and are present at the top left.']},\n",
       " 'What type of information is shown over the image?': {'uuid': '3ec8d1fa-5d9e-46d4-9c4f-4eaa5b4bfc15',\n",
       "  'ref_answer': 'Relationship information, referring expressions, and region captions along with their corresponding region annotations.',\n",
       "  'gen_answer': 'Object attributes and relationships are shown over the image.',\n",
       "  'context': ['Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840',\n",
       "   'tions, it is important to derive a variety of attributes in RS\\nimages. To this end, we have selected five distinct types of\\nattributes, as outlined in Table 3. Object category informa-\\ntion can be directly obtained from the SAMRS dataset. For\\ncolor extraction, we use the K-Means clustering algorithm.\\nSpecifically, we extract the object‚Äôs pixels from the image\\nusing ground-truth box and cluster them into Kgroups. The\\ncenter of the largest cluster is then selected as the object‚Äôs',\n",
       "   'VLMs in remote sensing is comparatively sparse. The ma-\\njority of research so far has neglected the semantic under-\\nstanding of the items and their relationships towards a deep\\nvisual comprehension. Beyond merely identifying the ob-\\njects in an image, vision-language models are also capa-\\nble of generating natural language descriptions of the im-\\nage and inferring the connections between the objects. This\\nmakes them more appropriate for tasks like text-based im-',\n",
       "   '\\nObject Attributes and Relationships1.Ship{white,small,top-left},{grey,small,bottom-right}2.Harbor{small,right},{large,bottom-left},{small,topright},{small,left}3.Trees{top-right}.Referring Expression1.SmallGreyshipanchoredatlargeharboronbottom-right2.SomeTreesatthetop-right.3.Whiteshipatthetopofasmallharborontheright.Region Captioning1.Objectlocatedat{}.2.Sizeofobjectat.{}issmall.Figure 4. Types of annotations available in the GeoChat instruction-set. Top-row: For a given RS image, we obtain object attribute and']},\n",
       " 'Which additional tool was used alongside the dataset subset?': {'uuid': '3ec8d1fa-5d9e-46d4-9c4f-4eaa5b4bfc15',\n",
       "  'ref_answer': 'BLIP was used alongside the dataset subset.',\n",
       "  'gen_answer': 'The ViTAE-RVSA model was used alongside the LoveDA dataset.',\n",
       "  'context': ['visual question answering and scene classification datasets with\\nvarying image sizes and types of classes to ensure diversity.\\nConstituent Datasets: In the compilation of our in-\\nstruction set, we incorporate three distinct types of datasets,\\nencompassing the ones designed for object detection,\\nscene classification, and visual question answering (VQA).\\nSpecifically, we integrate three object detection (DOTA\\n[35], DIOR [6], and FAIR1M [27] which together form',\n",
       "   'lize ViTAE-RVSA [32] model, pre-trained on the LoveDA\\ndataset [33], which encompasses the required important\\nclasses. The model [32] is used to infer these classes on\\nthe SAMRS [31] dataset, yielding pseudo labels. To miti-\\ngate potential noise in these predictions, we remove the pre-\\ndictions of ViTAE-RVSA [32] for which we already have\\nground truth from the SAMRS [31] dataset to refine the re-\\nsults.\\nAttribute extraction: For referring expression annota-',\n",
       "   'instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-',\n",
       "   'the SAMRS [31] dataset), one scene classification (NWPU-\\nRESISC-45 [5]), one VQA (LRBEN[20]), and one flood\\ndetection [25] VQA dataset (see Table 2). The object detec-\\ntion datasets allow region-level reasoning capability as they\\noffer segmentation masks along with bounding boxes.\\nAddition of Missing Classes: Although a wide vari-\\nety of object classes are included in the object detection\\ndatabases, several essential categories like buildings, roads,\\n5\\n27835']},\n",
       " 'What is the purpose of the dataset created using LLM Vicuna?': {'uuid': 'ea517f2f-e86b-47c5-916a-437af825d149',\n",
       "  'ref_answer': 'To align the model to follow a range of instructions',\n",
       "  'gen_answer': 'The purpose of the dataset created using LLM Vicuna is to align the model with a range of instructions by presenting and curating varied instruction-following data with multi-round conversations regarding remote sensing imagery.',\n",
       "  'context': ['captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-',\n",
       "   'ject detection dataset [31] to create short descriptions of\\nthe images, followed by using Vicuna-v1.5 [7] to cre-\\nate conversations using the generated text alone. Further,\\nwe add visual question-answering and scene classification\\nabilities using their corresponding datasets [5, 20]. This\\nresults in a total of 318kinstruction pairs for RS domain.\\n‚Ä¢GeoChat . Leveraging our dataset, we finetune LLaV A-\\n1.5 [14] to create the remote sensing-domain vision-',\n",
       "   'work. To accomplish different vision-language tasks, we\\ndirectly depend on the Vicuna-v1.5(7B) [7] language to-\\nkens. We explicitly interact with the language model to\\nconstruct textual representations of bounding boxes to ex-\\npress their spatial coordinates for the visual grounding tasks\\nthat require the production of spatial locations. Similarly,\\nthe safe, aligned and effective behavior of LLM is ensured\\nvia system prompts appended together with given inputs. A',\n",
       "   'instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-']},\n",
       " 'What type of data does the RS Multimodal Instruction Dataset contain?': {'uuid': 'ea517f2f-e86b-47c5-916a-437af825d149',\n",
       "  'ref_answer': 'Varied instruction-following data with multi-round conversations regarding remote sensing imagery',\n",
       "  'gen_answer': 'The RS Multimodal Instruction Dataset contains image-text pairs, relationship information, referring expressions, region captions with corresponding region annotations, and structured information to create rich instruction-sets.',\n",
       "  'context': ['can not only answer image-level queries but also accepts\\nregion inputs to hold region-specific dialogue. Further-\\nmore, it can visually ground objects in its responses by re-\\nferring to their spatial coordinates. To address the lack\\nof domain-specific datasets, we generate a novel RS mul-\\ntimodal instruction-following dataset by extending image-\\ntext pairs from existing diverse RS datasets. We establish\\na comprehensive benchmark for RS multitask conversations',\n",
       "   'created using the below pipeline, we randomly sample 65k\\nimages to create multi-round conversations, 10k images to\\ngenerate complex question answers and 30k images to gen-\\nerate detailed descriptions for the given short descriptions.\\nIn combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs\\nfor training and 10k for testing. Next, we outline the\\ninstruction-set creation process.\\nDataset Category # Classes # Images Image Size',\n",
       "   'relationship information, referring expressions and region captions along with their corresponding region annotations (shown over the\\nimage). Bottom-row: This structured information is used to create the rich instruction-set with a total of 318k image-instruction pairs.\\ning related tasks. We use a pre-trained CLIP-ViT(L-14)\\nencoder[28],trained on large amounts of textual and visual\\ndata, a pretrained MLP adaptor[17], pretrained on a 558K\\nsubset of the LAION-CC-SBU [26] dataset with BLIP [15]',\n",
       "   'instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-']},\n",
       " 'What is the number of classes in the NWPU-RESISC-45 dataset?': {'uuid': 'b3ff2467-9a43-49e1-ab85-bda122797ba6',\n",
       "  'ref_answer': '45',\n",
       "  'gen_answer': '45',\n",
       "  'context': ['In total, the AID [34] dataset has 10,000 images within 30\\nclasses. The images have been taken from different coun-\\ntries as well as different weather conditions. For evaluation,\\nwe use a 20% split of the AID [34] dataset. UCMerced [36]\\nis a Land Use scene classification dataset, with 2,100 im-\\nages and 21 classes. Each image is of size 256 √ó256. We\\nuse the whole UCMerced [36] dataset as a zero-shot test set.\\nResults. We prompt the models with all of the',\n",
       "   'outperforms other zero-shot models and performs competitively to\\nSoTA-supervised models like RSGPT which are specifically fine-\\ntuned on target dataset (while ours is a generic model not specifi-\\ncally finetuned on target dataset).\\n[34] is a large-scale aerial image collection compiled from\\nGoogle Earth imagery, with 30 classes, such as a river,\\ndense residential area, etc. The images are labeled by spe-\\ncialists in the field of remote sensing image interpretation.',\n",
       "   'Dataset Category # Classes # Images Image Size\\nDOTA Object Detection 18 17,480 1024 √ó1024\\nDIOR Object Detection 20 23,463 800 √ó800\\nFAIR1M Object Detection 37 64,147 600 √ó600\\nLRBEN(rsvqa) Visual Question Answering - 600 256 √ó256\\nFloodnet Visual Question Answering - 4056 3000 √ó4000\\nNWPU-RESISC-45 Scene Classification 45 31,500 256 √ó256\\nTable 2. List of datasets used to creat our remote-sensing instruc-\\ntion set for GeoChat VLM training. We include object detection,',\n",
       "   'the SAMRS [31] dataset), one scene classification (NWPU-\\nRESISC-45 [5]), one VQA (LRBEN[20]), and one flood\\ndetection [25] VQA dataset (see Table 2). The object detec-\\ntion datasets allow region-level reasoning capability as they\\noffer segmentation masks along with bounding boxes.\\nAddition of Missing Classes: Although a wide vari-\\nety of object classes are included in the object detection\\ndatabases, several essential categories like buildings, roads,\\n5\\n27835']},\n",
       " 'What capability do the object detection datasets provide?': {'uuid': 'c4ac6f83-bcba-4a7f-b19e-7766a3de786a',\n",
       "  'ref_answer': 'The object detection datasets provide region-level reasoning capability by offering segmentation masks along with bounding boxes.',\n",
       "  'gen_answer': 'The object detection datasets allow region-level reasoning capability as they offer segmentation masks along with bounding boxes.',\n",
       "  'context': ['the SAMRS [31] dataset), one scene classification (NWPU-\\nRESISC-45 [5]), one VQA (LRBEN[20]), and one flood\\ndetection [25] VQA dataset (see Table 2). The object detec-\\ntion datasets allow region-level reasoning capability as they\\noffer segmentation masks along with bounding boxes.\\nAddition of Missing Classes: Although a wide vari-\\nety of object classes are included in the object detection\\ndatabases, several essential categories like buildings, roads,\\n5\\n27835',\n",
       "   'visual question answering and scene classification datasets with\\nvarying image sizes and types of classes to ensure diversity.\\nConstituent Datasets: In the compilation of our in-\\nstruction set, we incorporate three distinct types of datasets,\\nencompassing the ones designed for object detection,\\nscene classification, and visual question answering (VQA).\\nSpecifically, we integrate three object detection (DOTA\\n[35], DIOR [6], and FAIR1M [27] which together form',\n",
       "   'Dataset Category # Classes # Images Image Size\\nDOTA Object Detection 18 17,480 1024 √ó1024\\nDIOR Object Detection 20 23,463 800 √ó800\\nFAIR1M Object Detection 37 64,147 600 √ó600\\nLRBEN(rsvqa) Visual Question Answering - 600 256 √ó256\\nFloodnet Visual Question Answering - 4056 3000 √ó4000\\nNWPU-RESISC-45 Scene Classification 45 31,500 256 √ó256\\nTable 2. List of datasets used to creat our remote-sensing instruc-\\ntion set for GeoChat VLM training. We include object detection,',\n",
       "   'cation [5] and SAMRS for object detection [31].\\nA crucial capability of GeoChat is the unification of mul-\\ntiple image and region-level reasoning tasks for RS imagery\\nwithin a single pipeline (see Fig. 1). We achieve this via\\ndistinct task tokens that help suitably direct the model‚Äôs re-\\nsponses according to user requirements. In addition, the\\nmodel uses spatial location representations in its inputs to\\nseamlessly reason about local regions and can also generate']},\n",
       " 'Is the entire road flooded?': {'uuid': 'ccc1f456-e73f-42f3-a160-ffb0acecc545',\n",
       "  'ref_answer': 'Yes, the whole road is flooded.',\n",
       "  'gen_answer': 'No',\n",
       "  'context': ['Give bounding box for the Boeing 747 at the bottom left.\\nSure!Inthesatelliteimage,therearefourbaseballfieldslocatednearsomebuildingsatthecenterofthescene.Thebaseballfieldsarearrangedinacloseproximitytoeachother.\\n[grounding] Please describe the image in detail.\\nYes, the road at the center is completely flooded.\\nIs the whole road flooded?\\nHow many buildings are flooded?Three buildings are flooded and are present at the top left.',\n",
       "   '8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.',\n",
       "   'the SAMRS [31] dataset), one scene classification (NWPU-\\nRESISC-45 [5]), one VQA (LRBEN[20]), and one flood\\ndetection [25] VQA dataset (see Table 2). The object detec-\\ntion datasets allow region-level reasoning capability as they\\noffer segmentation masks along with bounding boxes.\\nAddition of Missing Classes: Although a wide vari-\\nety of object classes are included in the object detection\\ndatabases, several essential categories like buildings, roads,\\n5\\n27835',\n",
       "   'Ships and Harbors (e.g. ‚Äúanchored at, parked at‚Äù)\\nTrack Field and Soccer Field (e.g. ‚ÄúSurrounded by, Inside‚Äù)\\nVehicles, Bridge, Road, Roundabout (e.g. ‚Äúpassing through, passing through‚Äù)\\nVehicles and Building (e.g. ‚Äúparked‚Äù)\\nAirport and Plane (e.g. ‚Äúparked‚Äù)\\nShip and Helipad (e.g. ‚Äúon, contains‚Äù)\\nTable 4. Example of relationships between different objects used in\\nthe proposed instruction dataset.\\nand trees are missing. To address this, we propose to uti-']},\n",
       " 'Which infrastructure and vehicles are listed together, and what are their relationship examples?': {'uuid': '1c477cd0-505e-4fc2-9341-925366306d0a',\n",
       "  'ref_answer': \"Vehicles, Bridge, Road, and Roundabout are listed together, with relationships such as 'passing through'.\",\n",
       "  'gen_answer': 'Infrastructure and vehicles listed together include ships and harbors (\"anchored at, parked at\"), track fields and soccer fields (\"surrounded by, inside\"), vehicles and bridges/roads/roundabouts (\"passing through, passing through\"), and vehicles and buildings (\"parked\"). Their relationship examples are specified in Table 4.',\n",
       "  'context': ['Ships and Harbors (e.g. ‚Äúanchored at, parked at‚Äù)\\nTrack Field and Soccer Field (e.g. ‚ÄúSurrounded by, Inside‚Äù)\\nVehicles, Bridge, Road, Roundabout (e.g. ‚Äúpassing through, passing through‚Äù)\\nVehicles and Building (e.g. ‚Äúparked‚Äù)\\nAirport and Plane (e.g. ‚Äúparked‚Äù)\\nShip and Helipad (e.g. ‚Äúon, contains‚Äù)\\nTable 4. Example of relationships between different objects used in\\nthe proposed instruction dataset.\\nand trees are missing. To address this, we propose to uti-',\n",
       "   'The model can also specify object types, object counts, object attributes and object relationships.\\nAttribute Example\\na1 category (e.g. ‚Äúplane, ship‚Äù)\\na2 color (e.g. ‚Äúgray, white‚Äù)\\na3 relative size (e.g. ‚Äúsmall, large‚Äù)\\na4 relative location (e.g. ‚Äútop right, bottom‚Äù)\\na5 relation (e.g. ‚Äúparked at, driving through‚Äù)\\nTable 3. List of attributes collected for objects. Attributes are used\\nto obtain referring expressions e.g., small-sized plane to the left.Categories Example',\n",
       "   'bels. Table 4 presents various examples of object relation-\\nships. To establish relationships like ‚Äúsurrounded by,‚Äù we\\ncross-reference pixel-level coordinates to verify if one ob-\\nject is entirely contained within another object.\\nExpression Generation: To emulate natural language\\nexpressions, we employ predefined textual templates based\\non [40]. The phrase template encompasses the attributes\\n{a1, . . . , a5 }from Table 3. The expression for a group of\\n6\\n27836',\n",
       "   'tributes {a2, a3}can be arranged in any sequence.\\nSimilarly, the sentence template incorporates the rela-\\ntional attributes a5 to establish connections between two\\nobjects through this structure:\\n‚ÄùThe/A ‚ü®ai3‚ü© ‚ü®ai2‚ü©ai1ai5aj1‚ü®in/on the aj4‚ü©.‚Äù\\nHere, the indicies iandjrepresent the ithandjthobject.\\nVisual Grounding: Although referring expression\\ndatasets are available in the natural image domain [37, 38],\\nthey lack for the remote sensing domain. To this end, we']},\n",
       " 'What relationship is provided between a ship and a helipad?': {'uuid': '1c477cd0-505e-4fc2-9341-925366306d0a',\n",
       "  'ref_answer': \"The relationships provided are 'on' and 'contains'.\",\n",
       "  'gen_answer': '\"on\" or \"contains\"',\n",
       "  'context': ['Ships and Harbors (e.g. ‚Äúanchored at, parked at‚Äù)\\nTrack Field and Soccer Field (e.g. ‚ÄúSurrounded by, Inside‚Äù)\\nVehicles, Bridge, Road, Roundabout (e.g. ‚Äúpassing through, passing through‚Äù)\\nVehicles and Building (e.g. ‚Äúparked‚Äù)\\nAirport and Plane (e.g. ‚Äúparked‚Äù)\\nShip and Helipad (e.g. ‚Äúon, contains‚Äù)\\nTable 4. Example of relationships between different objects used in\\nthe proposed instruction dataset.\\nand trees are missing. To address this, we propose to uti-',\n",
       "   '\\nObject Attributes and Relationships1.Ship{white,small,top-left},{grey,small,bottom-right}2.Harbor{small,right},{large,bottom-left},{small,topright},{small,left}3.Trees{top-right}.Referring Expression1.SmallGreyshipanchoredatlargeharboronbottom-right2.SomeTreesatthetop-right.3.Whiteshipatthetopofasmallharborontheright.Region Captioning1.Objectlocatedat{}.2.Sizeofobjectat.{}issmall.Figure 4. Types of annotations available in the GeoChat instruction-set. Top-row: For a given RS image, we obtain object attribute and',\n",
       "   'Give bounding box for the Boeing 747 at the bottom left.\\nSure!Inthesatelliteimage,therearefourbaseballfieldslocatednearsomebuildingsatthecenterofthescene.Thebaseballfieldsarearrangedinacloseproximitytoeachother.\\n[grounding] Please describe the image in detail.\\nYes, the road at the center is completely flooded.\\nIs the whole road flooded?\\nHow many buildings are flooded?Three buildings are flooded and are present at the top left.',\n",
       "   'bels. Table 4 presents various examples of object relation-\\nships. To establish relationships like ‚Äúsurrounded by,‚Äù we\\ncross-reference pixel-level coordinates to verify if one ob-\\nject is entirely contained within another object.\\nExpression Generation: To emulate natural language\\nexpressions, we employ predefined textual templates based\\non [40]. The phrase template encompasses the attributes\\n{a1, . . . , a5 }from Table 3. The expression for a group of\\n6\\n27836']},\n",
       " 'What does the categorization of object sizes depend on?': {'uuid': 'df69d168-4c98-4b1f-8eb0-5bf903d2049f',\n",
       "  'ref_answer': 'The area measurement of all instances of a class in the dataset',\n",
       "  'gen_answer': 'The categorization of object sizes depends on measuring the area of all instances of a class in the entire dataset and assigning the 80th percentile as the large label, with the 20th percentile designated as small size, and the remaining falling into the normal category.',\n",
       "  'context': ['color. To specify the relative size of the object, we catego-\\nrize objects into three sizes: small, normal, and large. This\\ncategorization is determined by measuring the area of all in-stances of a class in the entire dataset and assigning the 80th\\npercentile as the large label. Similarly, the 20thpercentile\\nis designated as small size, with the remaining falling into\\nthe normal category. To determine the object‚Äôs relative po-\\nsition within the images, we partition the entire image into',\n",
       "   'small objects and object details), we interpolate the posi-\\ntional encoding in the transformer-based CLIP [28] model\\nto scale with input image sizes of 504 √ó504. Although this\\n3\\n27833',\n",
       "   'the SAMRS [31] dataset), one scene classification (NWPU-\\nRESISC-45 [5]), one VQA (LRBEN[20]), and one flood\\ndetection [25] VQA dataset (see Table 2). The object detec-\\ntion datasets allow region-level reasoning capability as they\\noffer segmentation masks along with bounding boxes.\\nAddition of Missing Classes: Although a wide vari-\\nety of object classes are included in the object detection\\ndatabases, several essential categories like buildings, roads,\\n5\\n27835',\n",
       "   'The model can also specify object types, object counts, object attributes and object relationships.\\nAttribute Example\\na1 category (e.g. ‚Äúplane, ship‚Äù)\\na2 color (e.g. ‚Äúgray, white‚Äù)\\na3 relative size (e.g. ‚Äúsmall, large‚Äù)\\na4 relative location (e.g. ‚Äútop right, bottom‚Äù)\\na5 relation (e.g. ‚Äúparked at, driving through‚Äù)\\nTable 3. List of attributes collected for objects. Attributes are used\\nto obtain referring expressions e.g., small-sized plane to the left.Categories Example']},\n",
       " 'What are the components used in defining object relationships in an image?': {'uuid': 'ceb5217e-a212-40ce-acc9-8fb6645d7f58',\n",
       "  'ref_answer': 'The components used in defining object relationships in an image include the distance between bounding boxes and the class labels of the objects.',\n",
       "  'gen_answer': 'The components used in defining object relationships in an image include distance between bounding boxes and class labels of objects. Relationships like \"surrounded by\" are established by cross-referencing pixel-level coordinates to verify if one object is entirely contained within another.',\n",
       "  'context': ['a 3√ó3 grid, defining regions such as Top Right, Top, Top\\nLeft, Left, Center, Right, Bottom Right, Bottom Left, and\\nBottom. Based on the object‚Äôs center pixel coordinates, we\\nassign its relative position accordingly.\\nTo define the relation between objects in a given image,\\nwe group different objects based on their distance between\\nthe bounding boxes, and for each sub-graph, we assign dif-\\nferent relationships between objects based on their class la-',\n",
       "   'bels. Table 4 presents various examples of object relation-\\nships. To establish relationships like ‚Äúsurrounded by,‚Äù we\\ncross-reference pixel-level coordinates to verify if one ob-\\nject is entirely contained within another object.\\nExpression Generation: To emulate natural language\\nexpressions, we employ predefined textual templates based\\non [40]. The phrase template encompasses the attributes\\n{a1, . . . , a5 }from Table 3. The expression for a group of\\n6\\n27836',\n",
       "   'relationship information, referring expressions and region captions along with their corresponding region annotations (shown over the\\nimage). Bottom-row: This structured information is used to create the rich instruction-set with a total of 318k image-instruction pairs.\\ning related tasks. We use a pre-trained CLIP-ViT(L-14)\\nencoder[28],trained on large amounts of textual and visual\\ndata, a pretrained MLP adaptor[17], pretrained on a 558K\\nsubset of the LAION-CC-SBU [26] dataset with BLIP [15]',\n",
       "   'tributes {a2, a3}can be arranged in any sequence.\\nSimilarly, the sentence template incorporates the rela-\\ntional attributes a5 to establish connections between two\\nobjects through this structure:\\n‚ÄùThe/A ‚ü®ai3‚ü© ‚ü®ai2‚ü©ai1ai5aj1‚ü®in/on the aj4‚ü©.‚Äù\\nHere, the indicies iandjrepresent the ithandjthobject.\\nVisual Grounding: Although referring expression\\ndatasets are available in the natural image domain [37, 38],\\nthey lack for the remote sensing domain. To this end, we']},\n",
       " 'What are the attributes that may be absent in the sentence structure formula provided?': {'uuid': '6ee0362f-2585-43ad-89b9-5d78d19b9576',\n",
       "  'ref_answer': '{a2, a3}',\n",
       "  'gen_answer': 'The attributes {a1} from Table 3 may be absent in the sentence structure formula.',\n",
       "  'context': ['bels. Table 4 presents various examples of object relation-\\nships. To establish relationships like ‚Äúsurrounded by,‚Äù we\\ncross-reference pixel-level coordinates to verify if one ob-\\nject is entirely contained within another object.\\nExpression Generation: To emulate natural language\\nexpressions, we employ predefined textual templates based\\non [40]. The phrase template encompasses the attributes\\n{a1, . . . , a5 }from Table 3. The expression for a group of\\n6\\n27836',\n",
       "   'tributes {a2, a3}can be arranged in any sequence.\\nSimilarly, the sentence template incorporates the rela-\\ntional attributes a5 to establish connections between two\\nobjects through this structure:\\n‚ÄùThe/A ‚ü®ai3‚ü© ‚ü®ai2‚ü©ai1ai5aj1‚ü®in/on the aj4‚ü©.‚Äù\\nHere, the indicies iandjrepresent the ithandjthobject.\\nVisual Grounding: Although referring expression\\ndatasets are available in the natural image domain [37, 38],\\nthey lack for the remote sensing domain. To this end, we',\n",
       "   'lize ViTAE-RVSA [32] model, pre-trained on the LoveDA\\ndataset [33], which encompasses the required important\\nclasses. The model [32] is used to infer these classes on\\nthe SAMRS [31] dataset, yielding pseudo labels. To miti-\\ngate potential noise in these predictions, we remove the pre-\\ndictions of ViTAE-RVSA [32] for which we already have\\nground truth from the SAMRS [31] dataset to refine the re-\\nsults.\\nAttribute extraction: For referring expression annota-',\n",
       "   'use our short descriptions as referring expressions to cre-\\nate three different kinds of question answering pairs, i.e.\\ngrounding image description, referring expression, and re-\\ngion level captioning, as described in Table 1.\\n5. Experiments\\n5.1. Implementation Details\\nWe initialize the weights of our model with the pretrained\\nCLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA\\n[11] finetuning. Utilizing LoRA, we refine the parameters\\nWqandWvthrough low-rank adaptation, with a designated']},\n",
       " 'How can the attributes {a2, a3} be arranged in the sentence structure formula?': {'uuid': '6ee0362f-2585-43ad-89b9-5d78d19b9576',\n",
       "  'ref_answer': '{a2, a3} can be arranged in any sequence.',\n",
       "  'gen_answer': 'The attributes {a2, a3} can be arranged in any sequence within the sentence structure formula.',\n",
       "  'context': ['tributes {a2, a3}can be arranged in any sequence.\\nSimilarly, the sentence template incorporates the rela-\\ntional attributes a5 to establish connections between two\\nobjects through this structure:\\n‚ÄùThe/A ‚ü®ai3‚ü© ‚ü®ai2‚ü©ai1ai5aj1‚ü®in/on the aj4‚ü©.‚Äù\\nHere, the indicies iandjrepresent the ithandjthobject.\\nVisual Grounding: Although referring expression\\ndatasets are available in the natural image domain [37, 38],\\nthey lack for the remote sensing domain. To this end, we',\n",
       "   'bels. Table 4 presents various examples of object relation-\\nships. To establish relationships like ‚Äúsurrounded by,‚Äù we\\ncross-reference pixel-level coordinates to verify if one ob-\\nject is entirely contained within another object.\\nExpression Generation: To emulate natural language\\nexpressions, we employ predefined textual templates based\\non [40]. The phrase template encompasses the attributes\\n{a1, . . . , a5 }from Table 3. The expression for a group of\\n6\\n27836',\n",
       "   'The model can also specify object types, object counts, object attributes and object relationships.\\nAttribute Example\\na1 category (e.g. ‚Äúplane, ship‚Äù)\\na2 color (e.g. ‚Äúgray, white‚Äù)\\na3 relative size (e.g. ‚Äúsmall, large‚Äù)\\na4 relative location (e.g. ‚Äútop right, bottom‚Äù)\\na5 relation (e.g. ‚Äúparked at, driving through‚Äù)\\nTable 3. List of attributes collected for objects. Attributes are used\\nto obtain referring expressions e.g., small-sized plane to the left.Categories Example',\n",
       "   'lize ViTAE-RVSA [32] model, pre-trained on the LoveDA\\ndataset [33], which encompasses the required important\\nclasses. The model [32] is used to infer these classes on\\nthe SAMRS [31] dataset, yielding pseudo labels. To miti-\\ngate potential noise in these predictions, we remove the pre-\\ndictions of ViTAE-RVSA [32] for which we already have\\nground truth from the SAMRS [31] dataset to refine the re-\\nsults.\\nAttribute extraction: For referring expression annota-']},\n",
       " 'Which pretrained model is used to initialize the weights of their model?': {'uuid': '56288e4f-caba-4fca-9af7-c6518459ae87',\n",
       "  'ref_answer': 'The pretrained model used to initialize the weights is CLIP-ViT.',\n",
       "  'gen_answer': 'LLM Vicuna-v1.5[7]',\n",
       "  'context': ['tasks and optimize training efficiency, we employ a strat-\\negy that involves initializing the network with pre-trained\\nweights and fine-tuning specific segments for remote sens-\\n4\\n27834',\n",
       "   'tom). This makes it the first RS VLM with grounding capability.\\ntations facilitate effective self-supervised vision-language\\nmodeling, as demonstrated by multimodal GPT-4 [23] and\\nopen-source initiatives like LLaV A [19]. These vision-\\nlanguage models (VLMs), developed through generative\\npretraining and instruction-tuning, exhibit robust zero-shot\\ntask completion across various user-oriented multimodal\\ntasks. The resulting capabilities open the door to the de-',\n",
       "   'captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-',\n",
       "   'lize ViTAE-RVSA [32] model, pre-trained on the LoveDA\\ndataset [33], which encompasses the required important\\nclasses. The model [32] is used to infer these classes on\\nthe SAMRS [31] dataset, yielding pseudo labels. To miti-\\ngate potential noise in these predictions, we remove the pre-\\ndictions of ViTAE-RVSA [32] for which we already have\\nground truth from the SAMRS [31] dataset to refine the re-\\nsults.\\nAttribute extraction: For referring expression annota-']},\n",
       " 'Which version of LLM is mentioned in the text?': {'uuid': '56288e4f-caba-4fca-9af7-c6518459ae87',\n",
       "  'ref_answer': 'Vicuna-v1.5 is the version of LLM mentioned in the text.',\n",
       "  'gen_answer': 'LLM Vicuna-v1.5',\n",
       "  'context': ['scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and',\n",
       "   'captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   'Linguistics , 2:67‚Äì78, 2014. 7\\n[38] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\\nand Tamara L Berg. Modeling context in referring expres-\\nsions. In Computer Vision‚ÄìECCV 2016: 14th European\\nConference, Amsterdam, The Netherlands, October 11-14,\\n2016, Proceedings, Part II 14 , pages 69‚Äì85. Springer, 2016.\\n7[39] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang\\nZhu. From easy to hard: Learning language-guided curricu-\\nlum for visual question answering on remote sensing data.']},\n",
       " 'What kind of tasks were the instructional templates designed for?': {'uuid': '3e17f0b1-7136-4312-85db-68f4bed9d275',\n",
       "  'ref_answer': 'The instructional templates were designed for a variety of vision-language tasks.',\n",
       "  'gen_answer': 'The instructional templates were designed for multimodal instruction-tuning in the remote sensing domain to train a multitask conversational assistant.',\n",
       "  'context': ['cally provide system instructions as prompts that ask Vi-\\ncuna [7] to generate multi-round question and answer pairs\\nin a manner as if it could visualize the image (although it\\nonly has access to the text). This is achieved by providing\\nfew-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied. Specifically, from our short descriptions',\n",
       "   'ods have framed the task as a classification problem. Here,\\nthe model chooses answers from predetermined responses\\nfound in the training data. It limits their applicability to\\nopen-ended answer generation and instruction-following.\\nIn this paper, we introduce GeoChat, an attempt to\\nextend multimodal instruction-tuning to the remote sens-\\ning domain for training a multitask conversational assis-\\ntant. However, remote-sensing domain lacks a multimodal',\n",
       "   'can not only answer image-level queries but also accepts\\nregion inputs to hold region-specific dialogue. Further-\\nmore, it can visually ground objects in its responses by re-\\nferring to their spatial coordinates. To address the lack\\nof domain-specific datasets, we generate a novel RS mul-\\ntimodal instruction-following dataset by extending image-\\ntext pairs from existing diverse RS datasets. We establish\\na comprehensive benchmark for RS multitask conversations',\n",
       "   'bels. Table 4 presents various examples of object relation-\\nships. To establish relationships like ‚Äúsurrounded by,‚Äù we\\ncross-reference pixel-level coordinates to verify if one ob-\\nject is entirely contained within another object.\\nExpression Generation: To emulate natural language\\nexpressions, we employ predefined textual templates based\\non [40]. The phrase template encompasses the attributes\\n{a1, . . . , a5 }from Table 3. The expression for a group of\\n6\\n27836']},\n",
       " 'How many additional steps are taken in the second stage of training?': {'uuid': '81bd261d-09f2-4453-929a-9d2496989a29',\n",
       "  'ref_answer': '1600 more steps',\n",
       "  'gen_answer': '2144 steps - (number from the first stage) = 1600 more steps',\n",
       "  'context': ['stages, first, we train using all of our datasets for 1 epoch,\\ncorrespondingly 2144 steps, followed by stage 2, where we\\nonly train on the grounding dataset for 1600 more steps(at\\n128 batch size).\\n5.2. Scene Classification\\nDatasets for evaluation. For scene classification, we eval-\\nuate our model using AID [34] and UCMerced [36]. AIDMethod Presence Comparison Rural/Urban Avg. Accuracy\\nLLaV A-1.5[17] 55.46 68.20 59.00 62.77\\nQwen-vl-Chat [1] 38.57 67.59 61.00 55.35',\n",
       "   'rankrset to 64 in our implementation. The model under-\\ngoes training consistently at an image resolution of 504 √ó\\n504 throughout the whole process. Each training step incor-\\nporates specifically crafted multi-modal instructional tem-\\nplates designed for a variety of vision-language tasks during\\nthe training process. We use AdamW [21] optimizer with a\\ncosine learning rate scheduler to train our model. We keep\\nthe global batch size as 144. We train our model in two',\n",
       "   'Referring Expression 45k [refer] < p > Object < /p >\\nTable 1. Instruction following data used to train GeoChat. Instruc-\\ntion types and format are shown. We use a 308k set for training\\nand a separate 10k instruction-set for testing.\\ninputs. Further, the original LLaV A can not reason about\\nremote sensing images which is enabled via our domain-\\nspecific dataset. We describe each component in the archi-\\ntecture as follows:\\nTask Token: The unique quality of GeoChat is its abil-',\n",
       "   'created using the below pipeline, we randomly sample 65k\\nimages to create multi-round conversations, 10k images to\\ngenerate complex question answers and 30k images to gen-\\nerate detailed descriptions for the given short descriptions.\\nIn combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs\\nfor training and 10k for testing. Next, we outline the\\ninstruction-set creation process.\\nDataset Category # Classes # Images Image Size']},\n",
       " \"What is the highest score for the 'EasyToHard' model in the VQA task?\": {'uuid': '51580020-2258-4d2f-a38e-400da7ab9920',\n",
       "  'ref_answer': '91.67',\n",
       "  'gen_answer': '90.66',\n",
       "  'context': ['Qwen-vl-Chat [1] 38.57 67.59 61.00 55.35\\nMiniGPTv2 [4] 55.16 55.22 39.00 54.96\\nRSVQA[20] 87.47 81.50 90.00 86.32\\nEasyToHard[39] 90.66 87.49 91.67 89.94\\nBi-Modal[2] 91.06 91.16 92.66 91.63\\nSHRNet [41] 91.03 90.48 94.00 91.84\\nRSGPT[12] 91.17 91.70 94.00 92.29\\nGeoChat 91.09 90.33 94.00 90.70\\nTable 6. Comparisons with general zero-shot (top) and RS-VQA\\nspecialized (middle) models on RSVQA-LRBEN [20] dataset for\\nVQA task. [1, 4, 17] are evaluated in zero-shot setting. GeoChat',\n",
       "   'Results. We prompt the models with all of the\\nclasses and prompt to classify the image using just one\\nword/phrase. For example, we input a prompt like ‚ÄùClas-\\nsify the image within one of the given classes: dense\\nresidential area, . . . , school. Answer with one\\nword or short phrase.‚Äù . We calculate zero-shot accu-\\nracy on both AID and UCMerced. GeoChat significantly\\noutperforms other VLM‚Äôs with an accuracy of 84.43% on\\nUCMerced [36] and 72.03% on AID [34], as presented in',\n",
       "   'HRBEN [20] dataset for visual qa. All models here have not been\\ntrained on the target dataset. GeoChat performs favorably well\\ncompared to generic VLMs.\\ncomparison, and count. For evaluation, we use the test set-2\\nfor RSVQA-HRBEN [20] with 47k question answer pairs.\\nRSVQA-LR [20] is made up of 772 low-resolution im-\\nages and 77,232 question-answer pairs, with 77.8%, 11.1%,\\nand 11.1% used for training, validation, and testing, re-\\nspectively. There are four different categories of questions:',\n",
       "   '5.3. Visual Question Answering\\nDatasets for evaluation. RSVQA-HRBEN [20] comprises\\n10,569 high-resolution photos and 1,066,316 question-\\nanswer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-\\nvided into training, validation, test 1, and test 2 sets, re-\\nspectively. This dataset has three question types: presence,\\n7\\n27837']},\n",
       " \"What is the lowest score for 'RSVQA' in the comparison table?\": {'uuid': '51580020-2258-4d2f-a38e-400da7ab9920',\n",
       "  'ref_answer': '81.50',\n",
       "  'gen_answer': '87.47',\n",
       "  'context': ['presence, comparison, rural/urban, and count. We omitted\\narea and count questions during evaluation because the re-\\nsponses are numerical and quantifiable into numerous cat-\\negories. In the RSVQA-LRBEN [20] dataset, for exam-\\nple, counting questions are quantified into five categories:\\n0, between 1 and 10, between 11 and 100, between 101 and\\n1000, and greater than 1000. For evaluation, we use the test\\nset of RSVQA-LRBEN [20] with 7k question-answer pairs.',\n",
       "   'Qwen-vl-Chat [1] 38.57 67.59 61.00 55.35\\nMiniGPTv2 [4] 55.16 55.22 39.00 54.96\\nRSVQA[20] 87.47 81.50 90.00 86.32\\nEasyToHard[39] 90.66 87.49 91.67 89.94\\nBi-Modal[2] 91.06 91.16 92.66 91.63\\nSHRNet [41] 91.03 90.48 94.00 91.84\\nRSGPT[12] 91.17 91.70 94.00 92.29\\nGeoChat 91.09 90.33 94.00 90.70\\nTable 6. Comparisons with general zero-shot (top) and RS-VQA\\nspecialized (middle) models on RSVQA-LRBEN [20] dataset for\\nVQA task. [1, 4, 17] are evaluated in zero-shot setting. GeoChat',\n",
       "   'Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.',\n",
       "   'HRBEN [20] dataset for visual qa. All models here have not been\\ntrained on the target dataset. GeoChat performs favorably well\\ncompared to generic VLMs.\\ncomparison, and count. For evaluation, we use the test set-2\\nfor RSVQA-HRBEN [20] with 47k question answer pairs.\\nRSVQA-LR [20] is made up of 772 low-resolution im-\\nages and 77,232 question-answer pairs, with 77.8%, 11.1%,\\nand 11.1% used for training, validation, and testing, re-\\nspectively. There are four different categories of questions:']},\n",
       " 'What is the nature of the model being discussed in terms of its tuning?': {'uuid': '8db34f6a-4819-4a24-9cbe-3d447ab9b24d',\n",
       "  'ref_answer': 'The model is a generic model and is not specifically fine-tuned on the target dataset.',\n",
       "  'gen_answer': 'The model being discussed requires finetuning for each specific task separately. However, to adapt it to remote sensing images, LoRA fine-tuning is applied while keeping the MLP adaptor and CLIP encoder frozen during training.',\n",
       "  'context': ['ing images. A major gap exists in the remote sensing do-\\nmain towards developing general-purpose models to solve\\nall tasks together, while also maintaining conversation abil-\\nities. While RSGPT [12] is an initial effort that has shown\\ngood conversation ability along with solving multiple tasks,\\nit requires finetuning the model for each task separately,\\nwhich makes it cumbersome and not generalizable. Fur-\\n2\\n27832',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and',\n",
       "   'captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-',\n",
       "   'vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.']},\n",
       " 'What is the source of the large-scale aerial image collection mentioned in the text?': {'uuid': '8db34f6a-4819-4a24-9cbe-3d447ab9b24d',\n",
       "  'ref_answer': 'The large-scale aerial image collection is compiled from Google Earth imagery.',\n",
       "  'gen_answer': 'The large-scale aerial image collection mentioned in the text is compiled from Google Earth imagery.',\n",
       "  'context': ['outperforms other zero-shot models and performs competitively to\\nSoTA-supervised models like RSGPT which are specifically fine-\\ntuned on target dataset (while ours is a generic model not specifi-\\ncally finetuned on target dataset).\\n[34] is a large-scale aerial image collection compiled from\\nGoogle Earth imagery, with 30 classes, such as a river,\\ndense residential area, etc. The images are labeled by spe-\\ncialists in the field of remote sensing image interpretation.',\n",
       "   '8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.',\n",
       "   'Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840',\n",
       "   'scene classification. IEEE Transactions on Geoscience and\\nRemote Sensing , 55(7):3965‚Äì3981, 2017. 7\\n[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-\\nlongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-\\npei Zhang. Dota: A large-scale dataset for object detection in\\naerial images. In The IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , 2018. 5\\n[36] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-']},\n",
       " 'Is the model described specifically fine-tuned on the target dataset?': {'uuid': '8db34f6a-4819-4a24-9cbe-3d447ab9b24d',\n",
       "  'ref_answer': 'No, the model is a generic model not specifically fine-tuned on the target dataset.',\n",
       "  'gen_answer': 'No.',\n",
       "  'context': ['captions, and Vicuna-v1.5[7] to initialize our model. To\\nadapt our model to remote sensing images, we subsequently\\nLoRA [11] fine-tune the LLM , while keeping the MLP\\nadaptor and the CLIP encoder [28] frozen during training.\\n4. RS Multimodal Instruction Dataset\\nBy using LLM Vicuna [7], we align the model to follow\\na range of instructions by presenting and curating varied\\ninstruction-following data with multi-round conversations\\nregarding remote sensing imagery (Table 1). We specifi-',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and',\n",
       "   'lize ViTAE-RVSA [32] model, pre-trained on the LoveDA\\ndataset [33], which encompasses the required important\\nclasses. The model [32] is used to infer these classes on\\nthe SAMRS [31] dataset, yielding pseudo labels. To miti-\\ngate potential noise in these predictions, we remove the pre-\\ndictions of ViTAE-RVSA [32] for which we already have\\nground truth from the SAMRS [31] dataset to refine the re-\\nsults.\\nAttribute extraction: For referring expression annota-',\n",
       "   'outperforms other zero-shot models and performs competitively to\\nSoTA-supervised models like RSGPT which are specifically fine-\\ntuned on target dataset (while ours is a generic model not specifi-\\ncally finetuned on target dataset).\\n[34] is a large-scale aerial image collection compiled from\\nGoogle Earth imagery, with 30 classes, such as a river,\\ndense residential area, etc. The images are labeled by spe-\\ncialists in the field of remote sensing image interpretation.']},\n",
       " 'What is the size of each image in the UCMerced dataset?': {'uuid': '743e5079-821b-4534-acaa-2689bb5ca548',\n",
       "  'ref_answer': 'Each image in the UCMerced dataset is of size 256 √ó 256.',\n",
       "  'gen_answer': '256 √ó256',\n",
       "  'context': ['In total, the AID [34] dataset has 10,000 images within 30\\nclasses. The images have been taken from different coun-\\ntries as well as different weather conditions. For evaluation,\\nwe use a 20% split of the AID [34] dataset. UCMerced [36]\\nis a Land Use scene classification dataset, with 2,100 im-\\nages and 21 classes. Each image is of size 256 √ó256. We\\nuse the whole UCMerced [36] dataset as a zero-shot test set.\\nResults. We prompt the models with all of the',\n",
       "   'Dataset Category # Classes # Images Image Size\\nDOTA Object Detection 18 17,480 1024 √ó1024\\nDIOR Object Detection 20 23,463 800 √ó800\\nFAIR1M Object Detection 37 64,147 600 √ó600\\nLRBEN(rsvqa) Visual Question Answering - 600 256 √ó256\\nFloodnet Visual Question Answering - 4056 3000 √ó4000\\nNWPU-RESISC-45 Scene Classification 45 31,500 256 √ó256\\nTable 2. List of datasets used to creat our remote-sensing instruc-\\ntion set for GeoChat VLM training. We include object detection,',\n",
       "   'small objects and object details), we interpolate the posi-\\ntional encoding in the transformer-based CLIP [28] model\\nto scale with input image sizes of 504 √ó504. Although this\\n3\\n27833',\n",
       "   'created using the below pipeline, we randomly sample 65k\\nimages to create multi-round conversations, 10k images to\\ngenerate complex question answers and 30k images to gen-\\nerate detailed descriptions for the given short descriptions.\\nIn combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs\\nfor training and 10k for testing. Next, we outline the\\ninstruction-set creation process.\\nDataset Category # Classes # Images Image Size']},\n",
       " 'How many question-answer pairs are in RSVQA-HRBEN?': {'uuid': '56c2211c-158d-4f9f-b943-9bf6e8daab58',\n",
       "  'ref_answer': '1,066,316 question-answer pairs',\n",
       "  'gen_answer': 'RSVQA-HRBEN comprises 1,066,316 question-answer pairs.',\n",
       "  'context': ['5.3. Visual Question Answering\\nDatasets for evaluation. RSVQA-HRBEN [20] comprises\\n10,569 high-resolution photos and 1,066,316 question-\\nanswer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-\\nvided into training, validation, test 1, and test 2 sets, re-\\nspectively. This dataset has three question types: presence,\\n7\\n27837',\n",
       "   'HRBEN [20] dataset for visual qa. All models here have not been\\ntrained on the target dataset. GeoChat performs favorably well\\ncompared to generic VLMs.\\ncomparison, and count. For evaluation, we use the test set-2\\nfor RSVQA-HRBEN [20] with 47k question answer pairs.\\nRSVQA-LR [20] is made up of 772 low-resolution im-\\nages and 77,232 question-answer pairs, with 77.8%, 11.1%,\\nand 11.1% used for training, validation, and testing, re-\\nspectively. There are four different categories of questions:',\n",
       "   'presence, comparison, rural/urban, and count. We omitted\\narea and count questions during evaluation because the re-\\nsponses are numerical and quantifiable into numerous cat-\\negories. In the RSVQA-LRBEN [20] dataset, for exam-\\nple, counting questions are quantified into five categories:\\n0, between 1 and 10, between 11 and 100, between 101 and\\n1000, and greater than 1000. For evaluation, we use the test\\nset of RSVQA-LRBEN [20] with 7k question-answer pairs.',\n",
       "   'Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.']},\n",
       " 'How does GeoChat perform compared to the baseline?': {'uuid': 'a01e4504-ca5e-414a-b72b-43fcadd41b25',\n",
       "  'ref_answer': 'GeoChat outperforms the baseline, but there is still significant room for further improvement.',\n",
       "  'gen_answer': 'GeoChat performs better than or on par with a number of baseline methods in various remote sensing tasks, including zero-shot performance.',\n",
       "  'context': ['Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.',\n",
       "   'GeoChat achieves robust zero-shot performance across var-\\nious RS tasks including scene classification, VQA, multi-\\nturn dialogue, visual grounding and referring object detec-\\ntion, thus establishing a comprehensive benchmark.\\n8\\n27838',\n",
       "   'and compare with a number of baseline methods. GeoChat\\ndemonstrates robust zero-shot performance on various RS\\ntasks, e.g., image and region captioning, visual question\\nanswering, scene classification, visually grounded conver-\\nsations and referring detection. Our code is available here.\\n1. Introduction\\nIn the natural image domain, the abundance of aligned\\nimage-text data sourced from web imagery or manual anno-\\n*Equally contributing first authors.',\n",
       "   'Figure 5. Qualitative results of GeoChat. ( left-right ) Results are shown on grounding, referring object detection, and disaster/damage\\ndetection. The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior. The\\nmodel can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left).']},\n",
       " 'How many images does RSVQA-LR [20] consist of?': {'uuid': '37c378f9-8660-4b13-8b66-0d9b68163c46',\n",
       "  'ref_answer': '772 low-resolution images',\n",
       "  'gen_answer': 'RSVQA-LR [20] consists of 772 low-resolution images.',\n",
       "  'context': ['5.3. Visual Question Answering\\nDatasets for evaluation. RSVQA-HRBEN [20] comprises\\n10,569 high-resolution photos and 1,066,316 question-\\nanswer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-\\nvided into training, validation, test 1, and test 2 sets, re-\\nspectively. This dataset has three question types: presence,\\n7\\n27837',\n",
       "   'HRBEN [20] dataset for visual qa. All models here have not been\\ntrained on the target dataset. GeoChat performs favorably well\\ncompared to generic VLMs.\\ncomparison, and count. For evaluation, we use the test set-2\\nfor RSVQA-HRBEN [20] with 47k question answer pairs.\\nRSVQA-LR [20] is made up of 772 low-resolution im-\\nages and 77,232 question-answer pairs, with 77.8%, 11.1%,\\nand 11.1% used for training, validation, and testing, re-\\nspectively. There are four different categories of questions:',\n",
       "   'presence, comparison, rural/urban, and count. We omitted\\narea and count questions during evaluation because the re-\\nsponses are numerical and quantifiable into numerous cat-\\negories. In the RSVQA-LRBEN [20] dataset, for exam-\\nple, counting questions are quantified into five categories:\\n0, between 1 and 10, between 11 and 100, between 101 and\\n1000, and greater than 1000. For evaluation, we use the test\\nset of RSVQA-LRBEN [20] with 7k question-answer pairs.',\n",
       "   'ject detection dataset [31] to create short descriptions of\\nthe images, followed by using Vicuna-v1.5 [7] to cre-\\nate conversations using the generated text alone. Further,\\nwe add visual question-answering and scene classification\\nabilities using their corresponding datasets [5, 20]. This\\nresults in a total of 318kinstruction pairs for RS domain.\\n‚Ä¢GeoChat . Leveraging our dataset, we finetune LLaV A-\\n1.5 [14] to create the remote sensing-domain vision-']},\n",
       " 'What is the distribution of RSVQA-LR [20] for training, validation, and testing?': {'uuid': '37c378f9-8660-4b13-8b66-0d9b68163c46',\n",
       "  'ref_answer': '77.8% for training, 11.1% for validation, and 11.1% for testing',\n",
       "  'gen_answer': 'RSVQA-LR [20] uses 77.8% of its data for training, 11.1% for validation, and 11.1% for testing.',\n",
       "  'context': ['HRBEN [20] dataset for visual qa. All models here have not been\\ntrained on the target dataset. GeoChat performs favorably well\\ncompared to generic VLMs.\\ncomparison, and count. For evaluation, we use the test set-2\\nfor RSVQA-HRBEN [20] with 47k question answer pairs.\\nRSVQA-LR [20] is made up of 772 low-resolution im-\\nages and 77,232 question-answer pairs, with 77.8%, 11.1%,\\nand 11.1% used for training, validation, and testing, re-\\nspectively. There are four different categories of questions:',\n",
       "   'presence, comparison, rural/urban, and count. We omitted\\narea and count questions during evaluation because the re-\\nsponses are numerical and quantifiable into numerous cat-\\negories. In the RSVQA-LRBEN [20] dataset, for exam-\\nple, counting questions are quantified into five categories:\\n0, between 1 and 10, between 11 and 100, between 101 and\\n1000, and greater than 1000. For evaluation, we use the test\\nset of RSVQA-LRBEN [20] with 7k question-answer pairs.',\n",
       "   '5.3. Visual Question Answering\\nDatasets for evaluation. RSVQA-HRBEN [20] comprises\\n10,569 high-resolution photos and 1,066,316 question-\\nanswer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-\\nvided into training, validation, test 1, and test 2 sets, re-\\nspectively. This dataset has three question types: presence,\\n7\\n27837',\n",
       "   'Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.']},\n",
       " 'What is the name of the test set where GeoChat performed close to SOTA specialist models?': {'uuid': 'f7d0f6a9-deb0-4c17-b77a-7b589d2d8915',\n",
       "  'ref_answer': 'The test set where GeoChat performed close to SOTA specialist models is the RSVQA-LRBEN test set.',\n",
       "  'gen_answer': 'RSVQA-LRBEN',\n",
       "  'context': ['Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'GeoChat achieves robust zero-shot performance across var-\\nious RS tasks including scene classification, VQA, multi-\\nturn dialogue, visual grounding and referring object detec-\\ntion, thus establishing a comprehensive benchmark.\\n8\\n27838',\n",
       "   'language model - GeoChat. Our LoRA [11] fine-tuning is\\nefficient and avoids forgetting the necessary context em-\\nbedded in fully-tuned LLaV A model, whose MLP pro-\\njection is trained to align images into the word embed-\\nding space of the LLM (Vicuna-v1.5 [7]). This allowsGeoChat to retain the conversation and instruction follow-\\ning abilities of LLaV A and extend its domain-knowledge\\nto remote sensing tasks.\\n‚Ä¢ We also address the lack of evaluation benchmarks to as-']},\n",
       " 'Which table shows the performance of the method?': {'uuid': '1ce24cb0-e65f-4e94-bb50-2741f4dfa0bc',\n",
       "  'ref_answer': 'Table 7',\n",
       "  'gen_answer': 'Table 7',\n",
       "  'context': ['referenced using one attribute from a2, a3 or a4 in Table 3. [grounding] : objects referenced using a combination of attributes from a1-a5\\nin Table 3. Overall, GeoChat outperforms the baseline, but there is still significant room for further improvement on this complex task.\\nModel Presence Comparison Average Accuracy\\nQwen-VL[1] 66.44 60.41 63.06\\nLLaV A-1.5[17] 69.83 67.29 68.40\\nMiniGPTv2[4] 40.79 50.91 46.46\\nGeoChat 58.45 83.19 72.30\\nTable 8. Comparison with other general ZS model‚Äôs on RSVQA-',\n",
       "   'We use accuracy@0.5 as the evaluation metric. Accuracy is\\ncalculated if the predicted box has an overlap of more thanModel acc@0.5 acc@.25 METEOR\\nMiniGPTv2[4] 10.8 30.9 16.4\\nGeoChat 11.7 33.9 48.9\\nTable 9. Results on grounding description task.\\nModel ROUGE-1 ROUGE-L METEOR\\nMiniGPTv2[4] 32.1 31.2 10.0\\nGeoChat 87.3 87.2 83.9\\nTable 10. Region level captioning performance.\\n0.5 IoU with the ground-truth box.\\nResults. Table 7 shows the performance of our method',\n",
       "   '15.9% on LLaV A-v1.5 [17], as shown in Table 8.\\n5.4. Visual Grounding\\nDatasets for evaluation. For the evaluation of grounding\\ntasks, we propose a new benchmark that contains different\\nreferring and grounding tasks. We use the validation set\\nfrom [31] and used the same dataset creation pipeline as in\\nSec. 4 to construct the test benchmark. There are a total\\nof 7593 [refer], 560 [grounding], and 495 grounding de-\\nscription questions, as well as 2793 for region captioning.',\n",
       "   'Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol-\\nlow the instructions provided for this specific task and re-\\nturns unrelated classes that are not a part of the dataset. It‚Äôs\\naccuracy is close to 5% if we pass the answers from Vicuna-\\nv1.5 [7] and ask it to check if the output sentence refers to\\nthe ground truth class or not. In comparison, Qwen-VL and\\nLLaVa-1.5 perform well in instruction following, but fall\\nshort to GeoChat, due to lack of domain knowledge.']},\n",
       " 'Overall, the model performance is low on small objects or when it has to predict multiple boxes.': {'uuid': '58eff65a-541e-4bf4-8074-b921545dd641',\n",
       "  'ref_answer': 'Model performance is generally low on small objects or when predicting multiple boxes.',\n",
       "  'gen_answer': 'Our model works better on medium size images compared to MiniGPT-4-v2. It provides a better description with slightly better box accuracy. Model performance evaluation metric is accuracy@0.5, where our model achieves higher values for tasks like grounding and captioning. Specifically, in the task of grounding, our model outperforms MiniGPT-2v on ROUGE metrics (ROUGE-1, ROUGE-L) and METEOR. For region-level captioning, our method also shows superior performance.',\n",
       "  'context': ['and MiniGPT-4-v2 [4] on the proposed benchmark. Over-\\nall, the model performance is low on small objects or when\\nit has to predict multiple boxes. Compared to MiniGPT-4-\\nv2[4], our model works better on medium size images. On\\nthe grounding description task, we calculate both, the IoU\\nfor the multiple bounding boxes generated as well as the\\ntext answer generated. Our model provides a better descrip-\\ntion with slightly better box accuracy than MiniGPT-4-v2',\n",
       "   'We use accuracy@0.5 as the evaluation metric. Accuracy is\\ncalculated if the predicted box has an overlap of more thanModel acc@0.5 acc@.25 METEOR\\nMiniGPTv2[4] 10.8 30.9 16.4\\nGeoChat 11.7 33.9 48.9\\nTable 9. Results on grounding description task.\\nModel ROUGE-1 ROUGE-L METEOR\\nMiniGPTv2[4] 32.1 31.2 10.0\\nGeoChat 87.3 87.2 83.9\\nTable 10. Region level captioning performance.\\n0.5 IoU with the ground-truth box.\\nResults. Table 7 shows the performance of our method',\n",
       "   'Model Small Medium Large Single-object grounding Multi-object grounding [refer] [grounding] Overall\\nMiniGPTv2 [4] 1.7 9.9 21.9 9.1 3.6 8.2 2.6 7.6\\nGeoChat 2.9 13.6 21.7 16.0 4.3 10.5 11.8 10.6\\nTable 7. Performance (acc@0.5%) comparison of GeoChat on our benchmark. Small, medium and large refer to the size of the objects\\nbased on the bounding box area. Single/multi-object refer to how many objects the question asks the model to predict. [refer] : object',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and']},\n",
       " 'What metric is used to evaluate the grounding description task?': {'uuid': '58eff65a-541e-4bf4-8074-b921545dd641',\n",
       "  'ref_answer': 'IoU for multiple bounding boxes and the text answer are used to evaluate the grounding description task.',\n",
       "  'gen_answer': 'Accuracy@0.5',\n",
       "  'context': ['15.9% on LLaV A-v1.5 [17], as shown in Table 8.\\n5.4. Visual Grounding\\nDatasets for evaluation. For the evaluation of grounding\\ntasks, we propose a new benchmark that contains different\\nreferring and grounding tasks. We use the validation set\\nfrom [31] and used the same dataset creation pipeline as in\\nSec. 4 to construct the test benchmark. There are a total\\nof 7593 [refer], 560 [grounding], and 495 grounding de-\\nscription questions, as well as 2793 for region captioning.',\n",
       "   'and MiniGPT-4-v2 [4] on the proposed benchmark. Over-\\nall, the model performance is low on small objects or when\\nit has to predict multiple boxes. Compared to MiniGPT-4-\\nv2[4], our model works better on medium size images. On\\nthe grounding description task, we calculate both, the IoU\\nfor the multiple bounding boxes generated as well as the\\ntext answer generated. Our model provides a better descrip-\\ntion with slightly better box accuracy than MiniGPT-4-v2',\n",
       "   'We use accuracy@0.5 as the evaluation metric. Accuracy is\\ncalculated if the predicted box has an overlap of more thanModel acc@0.5 acc@.25 METEOR\\nMiniGPTv2[4] 10.8 30.9 16.4\\nGeoChat 11.7 33.9 48.9\\nTable 9. Results on grounding description task.\\nModel ROUGE-1 ROUGE-L METEOR\\nMiniGPTv2[4] 32.1 31.2 10.0\\nGeoChat 87.3 87.2 83.9\\nTable 10. Region level captioning performance.\\n0.5 IoU with the ground-truth box.\\nResults. Table 7 shows the performance of our method',\n",
       "   'use our short descriptions as referring expressions to cre-\\nate three different kinds of question answering pairs, i.e.\\ngrounding image description, referring expression, and re-\\ngion level captioning, as described in Table 1.\\n5. Experiments\\n5.1. Implementation Details\\nWe initialize the weights of our model with the pretrained\\nCLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA\\n[11] finetuning. Utilizing LoRA, we refine the parameters\\nWqandWvthrough low-rank adaptation, with a designated']},\n",
       " \"What aspects of the model's performance are mentioned in the comparison with MiniGPT-4-v2?\": {'uuid': '58eff65a-541e-4bf4-8074-b921545dd641',\n",
       "  'ref_answer': 'The aspects mentioned include performance on small objects, prediction of multiple boxes, performance on medium-sized images, and grounding description task metrics such as IoU and text answers.',\n",
       "  'gen_answer': \"The model's performance is compared on small objects prediction, multiple box predictions, medium size images, grounding description task (evaluated by IoU for bounding boxes and text accuracy), region-level captioning (evaluated by text accuracy with ground truth region-level captions), and instruction following. The comparison shows better results for the mentioned tasks when using this model over MiniGPT-4-v2.\",\n",
       "  'context': ['and MiniGPT-4-v2 [4] on the proposed benchmark. Over-\\nall, the model performance is low on small objects or when\\nit has to predict multiple boxes. Compared to MiniGPT-4-\\nv2[4], our model works better on medium size images. On\\nthe grounding description task, we calculate both, the IoU\\nfor the multiple bounding boxes generated as well as the\\ntext answer generated. Our model provides a better descrip-\\ntion with slightly better box accuracy than MiniGPT-4-v2',\n",
       "   '[4] (Table 9). As for region-level captioning, we evaluate\\nboth models based on the text accuracy with ground truth\\nregion-level captions (Table 10). Our model significantly\\noutperforms MiniGPT-4-v2 in terms of ROUGE and ME-\\nTEOR score.\\n6. Conclusion\\nAlthough recent advancements in large Vision-Language\\nModels (VLMs) have shown promise in nature image do-\\nmains, their performance in Remote Sensing (RS) scenar-\\nios is still limited due to the unique domain-specific chal-',\n",
       "   'Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol-\\nlow the instructions provided for this specific task and re-\\nturns unrelated classes that are not a part of the dataset. It‚Äôs\\naccuracy is close to 5% if we pass the answers from Vicuna-\\nv1.5 [7] and ask it to check if the output sentence refers to\\nthe ground truth class or not. In comparison, Qwen-VL and\\nLLaVa-1.5 perform well in instruction following, but fall\\nshort to GeoChat, due to lack of domain knowledge.',\n",
       "   'Qwen-vl-Chat [1] 38.57 67.59 61.00 55.35\\nMiniGPTv2 [4] 55.16 55.22 39.00 54.96\\nRSVQA[20] 87.47 81.50 90.00 86.32\\nEasyToHard[39] 90.66 87.49 91.67 89.94\\nBi-Modal[2] 91.06 91.16 92.66 91.63\\nSHRNet [41] 91.03 90.48 94.00 91.84\\nRSGPT[12] 91.17 91.70 94.00 92.29\\nGeoChat 91.09 90.33 94.00 90.70\\nTable 6. Comparisons with general zero-shot (top) and RS-VQA\\nspecialized (middle) models on RSVQA-LRBEN [20] dataset for\\nVQA task. [1, 4, 17] are evaluated in zero-shot setting. GeoChat']},\n",
       " 'What type of queries does GeoChat handle?': {'uuid': 'eaaff66d-e614-4a89-ace1-eeeee146d1cf',\n",
       "  'ref_answer': 'GeoChat handles image-level queries and region-specific dialogue, grounding responses with precise spatial coordinates.',\n",
       "  'gen_answer': 'GeoChat handles user text queries related to image/region captioning, referring object detection, and image/region-level conversations about remotely sensed images.',\n",
       "  'context': ['soning tasks that span varying levels of granularity in vi-\\nsual imagery understanding e.g., image/region captioning,\\nreferring object detection and image/region-level conversa-\\ntions about remotely sensed images. We formally outline\\nthe tasks possible with GeoChat below.\\na) Image-Level Conversation Tasks. In this task,\\nGeoChat processes an image xand a user text query qwith-\\nout any specific spatial coordinates in its inputs or outputs.',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.',\n",
       "   'Figure 5. Qualitative results of GeoChat. ( left-right ) Results are shown on grounding, referring object detection, and disaster/damage\\ndetection. The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior. The\\nmodel can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left).']},\n",
       " 'What kind of remote sensing model is GeoChat?': {'uuid': 'eaaff66d-e614-4a89-ace1-eeeee146d1cf',\n",
       "  'ref_answer': 'GeoChat is a unified remote sensing VLM.',\n",
       "  'gen_answer': 'GeoChat is the first unified remote sensing Vision-Language Model (VLM) designed for multitask conversational capabilities with high-resolution RS images.',\n",
       "  'context': ['Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'lenges. Addressing this gap, we present GeoChat, the\\nfirst unified remote sensing VLM that excels in multitask\\nconversational capabilities with high-resolution RS images.\\nGeoChat not only answers image-level queries but also en-\\ngages in region-specific dialogue, grounding responses with\\nprecise spatial coordinates. We create a novel RS mul-\\ntimodal instruction-following dataset comprising of 318k\\nimage-instruction pairs with a diverse multitask format.',\n",
       "   'level reasoning is necessary alongside holistic scene inter-\\npretation. Furthermore, the lack of domain-specific mul-\\ntimodal instruction following data as well as strong back-\\nbone models for RS make it hard for the models to align\\ntheir behavior with user queries. To address these lim-\\nitations, we propose GeoChat - the first versatile remote\\nsensing VLM that offers multitask conversational capabil-\\nities with high-resolution RS images. Specifically, GeoChat',\n",
       "   \"<System Message>A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to thehuman's questions.. Figure 2. An overview of GeoChat - the first grounded large vision-language model for remote sensing. Given an image input together\\nwith a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\"]},\n",
       " 'What does GeoChat achieve in terms of performance?': {'uuid': '62bea293-9d52-4aea-a37d-eb283b5b2bd0',\n",
       "  'ref_answer': 'GeoChat achieves robust zero-shot performance across various RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection.',\n",
       "  'gen_answer': 'GeoChat achieves robust zero-shot performance across various RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding, and referring object detection.',\n",
       "  'context': ['GeoChat achieves robust zero-shot performance across var-\\nious RS tasks including scene classification, VQA, multi-\\nturn dialogue, visual grounding and referring object detec-\\ntion, thus establishing a comprehensive benchmark.\\n8\\n27838',\n",
       "   'Results. To constrain the answers to a simple yes/no and\\nfor rural/urban question types, we add a suitable prompt at\\nthe end of each question. GeoChat performs close to the\\nSOTA specialist models on RSVQA-LRBEN test set, which\\nis RSGPT [12], finetuned on the target dataset for 5 itera-\\ntions in comparison. For RSVQA-HRBEN, GeoChat out-\\nperforms other VLM‚Äôs in zero-shot setting on average ac-\\ncuracy by 3.9%, while beating the Comparison subset by\\n15.9% on LLaV A-v1.5 [17], as shown in Table 8.',\n",
       "   'and compare with a number of baseline methods. GeoChat\\ndemonstrates robust zero-shot performance on various RS\\ntasks, e.g., image and region captioning, visual question\\nanswering, scene classification, visually grounded conver-\\nsations and referring detection. Our code is available here.\\n1. Introduction\\nIn the natural image domain, the abundance of aligned\\nimage-text data sourced from web imagery or manual anno-\\n*Equally contributing first authors.',\n",
       "   'Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-']},\n",
       " 'How does GeoChat contribute to the field of remote sensing?': {'uuid': '62bea293-9d52-4aea-a37d-eb283b5b2bd0',\n",
       "  'ref_answer': 'GeoChat contributes by establishing a comprehensive benchmark through its robust zero-shot performance across various RS tasks.',\n",
       "  'gen_answer': 'GeoChat contributes to the field of remote sensing by serving as a unified framework for various tasks such as visual question answering, scene classification, and natural language conversations based on high-resolution RS images. It addresses gaps in handling multiple tasks simultaneously with a focus on spatial reasoning and region-specific dialogue. Additionally, GeoChat provides a novel dataset for multimodal instruction-following tasks, enhancing the capability of models to align their behavior with user queries in remote sensing contexts.',\n",
       "  'context': ['Figure 1. GeoChat can accomplish multiple tasks for remote-\\nsensing (RS) image comprehension in a unified framework. Given\\nsuitable task tokens and user queries, the model can generate visu-\\nally grounded responses (text with corresponding object locations\\n- shown on top), visual question answering on images and regions\\n(top left and bottom right, respectively) as well as scene classifi-\\ncation (top right) and normal natural language conversations (bot-',\n",
       "   'lenges. Addressing this gap, we present GeoChat, the\\nfirst unified remote sensing VLM that excels in multitask\\nconversational capabilities with high-resolution RS images.\\nGeoChat not only answers image-level queries but also en-\\ngages in region-specific dialogue, grounding responses with\\nprecise spatial coordinates. We create a novel RS mul-\\ntimodal instruction-following dataset comprising of 318k\\nimage-instruction pairs with a diverse multitask format.',\n",
       "   'level reasoning is necessary alongside holistic scene inter-\\npretation. Furthermore, the lack of domain-specific mul-\\ntimodal instruction following data as well as strong back-\\nbone models for RS make it hard for the models to align\\ntheir behavior with user queries. To address these lim-\\nitations, we propose GeoChat - the first versatile remote\\nsensing VLM that offers multitask conversational capabil-\\nities with high-resolution RS images. Specifically, GeoChat',\n",
       "   'sess the capability of existing VLMs on remote-sensing\\nconversations. To this end, we setup evaluation proto-\\ncols for conversation grounding in RS, as well as a setup\\na suite of tasks to allow comparisons with future efforts\\nin this direction. We show various supervised as well as\\nzero-shot evaluations for different remote sensing tasks,\\nincluding image captioning, visual question answering\\nand scene classification to demonstrate the generalisabil-\\nity of GeoChat conversational VLM.']},\n",
       " 'What is the title of the research by Yakoub Bazi et al.?': {'uuid': '9d89dea7-13f9-4c5a-b4a8-86bf591cdd66',\n",
       "  'ref_answer': \"The title of the research is 'Bi-modal transformer-based approach for visual question answering in remote sensing imagery'.\",\n",
       "  'gen_answer': 'The question does not provide information about a specific research titled \"Yakoub Bazi et al.\" based on the given context.',\n",
       "  'context': ['vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.',\n",
       "   'Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 7, 8\\n[5] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-\\ning image scene classification: Benchmark and state of the\\nart.Proceedings of the IEEE , 105(10):1865‚Äì1883, 2017. 2,\\n3, 5\\n[6] Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo',\n",
       "   'lstm architecture. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval , pages 695‚Äì704. ACM, 2017. 3, 4, 5\\n[29] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham\\nCholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin,\\nEric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards\\naccurate and lightweight fully transparent gpt. arXiv preprint\\narXiv:2402.16840 , 2024. 2\\n[30] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mul-',\n",
       "   '8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.']},\n",
       " 'What is the title of the paper authored by Christel Chappuis, Val√©rie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia?': {'uuid': '2fe8a064-ddde-45de-96ff-51dc00c9b662',\n",
       "  'ref_answer': \"The title of the paper is 'Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering'.\",\n",
       "  'gen_answer': 'Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering',\n",
       "  'context': ['Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,',\n",
       "   'vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.',\n",
       "   '8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-']},\n",
       " 'What is the volume and issue number of the journal Transactions on Geoscience and Remote Sensing that published the mentioned paper?': {'uuid': '2fe8a064-ddde-45de-96ff-51dc00c9b662',\n",
       "  'ref_answer': 'The volume and issue number is 60:1‚Äì11.',\n",
       "  'gen_answer': 'The volume is 61 and the issue number is unspecified.',\n",
       "  'context': ['IEEE Transactions on Geoscience and Remote Sensing , 60:\\n1‚Äì11, 2022. 2, 7\\n[40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring\\ndata and models for visual grounding on remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 61:\\n1‚Äì13, 2023. 6\\n[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua\\nChen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-\\ntial hierarchical reasoning network for remote sensing visual\\nquestion answering. IEEE Transactions on Geoscience and',\n",
       "   'Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,',\n",
       "   'Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840',\n",
       "   'GeoChat\\n : Grounded Large Vision-Language Model for Remote Sensing\\nKartik Kuckreja1, 2* Muhammad Sohail Danish1*Muzammal Naseer1\\nAbhijit Das2Salman Khan1, 3Fahad Shahbaz Khan1, 4\\n1Mohamed bin Zayed University of AI,2Birla Institute of Technology & Science, Hyderabad\\n3Australian National University,4Link ¬®oping University\\nkartik.kuckreja@mbzuai.ac.ae, muhammad.sohail@mbzuai.ac.ae\\nAbstract\\nRecent advancements in Large Vision-Language Mod-\\nels (VLMs) have shown great promise in natural image do-']},\n",
       " 'Who are the authors of Minigpt-v2?': {'uuid': 'eedade6d-d3fd-4bc0-8de0-6a2e55569d61',\n",
       "  'ref_answer': 'The authors of Minigpt-v2 are Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.',\n",
       "  'gen_answer': 'Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.',\n",
       "  'context': ['Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 7, 8\\n[5] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-\\ning image scene classification: Benchmark and state of the\\nart.Proceedings of the IEEE , 105(10):1865‚Äì1883, 2017. 2,\\n3, 5\\n[6] Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo',\n",
       "   'and MiniGPT-4-v2 [4] on the proposed benchmark. Over-\\nall, the model performance is low on small objects or when\\nit has to predict multiple boxes. Compared to MiniGPT-4-\\nv2[4], our model works better on medium size images. On\\nthe grounding description task, we calculate both, the IoU\\nfor the multiple bounding boxes generated as well as the\\ntext answer generated. Our model provides a better descrip-\\ntion with slightly better box accuracy than MiniGPT-4-v2',\n",
       "   'lstm architecture. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval , pages 695‚Äì704. ACM, 2017. 3, 4, 5\\n[29] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham\\nCholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin,\\nEric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards\\naccurate and lightweight fully transparent gpt. arXiv preprint\\narXiv:2402.16840 , 2024. 2\\n[30] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mul-',\n",
       "   'Lang, Yanqing Yao, and Junwei Han. Anchor-free oriented\\nproposal generator for object detection. IEEE Transactions\\non Geoscience and Remote Sensing , 60:1‚Äì11, 2022. 5\\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\\nXing. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality, 2023. 2, 4, 5, 7\\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat']},\n",
       " 'In which conference was the LoRA paper presented?': {'uuid': 'c6345f79-af83-444c-9873-66351f7252c9',\n",
       "  'ref_answer': 'International Conference on Learning Representations',\n",
       "  'gen_answer': 'The LoRA paper was presented at the International Conference on Learning Representations.',\n",
       "  'context': ['trained model and utilised for inference. The LoRA adap-\\ntation ensures faster training and avoids forgetting original\\nknowledge embedded in the LLM trained and fine-tuned on\\ngeneric natural language instructions. This is an important\\nfeature since it allows the model to bring in external con-\\ntext about generic object types, landmarks and affordances\\nin the remote-sensing reasoning framework of GeoChat.\\n3.2. Training Details\\nTo enhance the effectiveness of our model on general visual',\n",
       "   '[USER] <im_start> Image Features <im_end> [Task Identifier] [ASSISTANT]Figure 3. Multi-task instruction template for GeoChat.\\nLow-Rank Adaptation (LoRA) [11] based strategy is used\\nfor fine-tuning the LLM. While training, instead of finetun-\\ning all of the weights that comprise the weight matrix of the\\npre-trained Vicuna-v1.5[7], we finetune two smaller matri-\\nces in LoRA [11] that approximate the original larger ma-\\ntrix. After that, the fine-tuned adaptor is fed into the pre-',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and',\n",
       "   'language model - GeoChat. Our LoRA [11] fine-tuning is\\nefficient and avoids forgetting the necessary context em-\\nbedded in fully-tuned LLaV A model, whose MLP pro-\\njection is trained to align images into the word embed-\\nding space of the LLM (Vicuna-v1.5 [7]). This allowsGeoChat to retain the conversation and instruction follow-\\ning abilities of LLaV A and extend its domain-knowledge\\nto remote sensing tasks.\\n‚Ä¢ We also address the lack of evaluation benchmarks to as-']},\n",
       " 'Who are the authors of the LoRA paper?': {'uuid': 'c6345f79-af83-444c-9873-66351f7252c9',\n",
       "  'ref_answer': 'Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen',\n",
       "  'gen_answer': 'Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.',\n",
       "  'context': ['trained model and utilised for inference. The LoRA adap-\\ntation ensures faster training and avoids forgetting original\\nknowledge embedded in the LLM trained and fine-tuned on\\ngeneric natural language instructions. This is an important\\nfeature since it allows the model to bring in external con-\\ntext about generic object types, landmarks and affordances\\nin the remote-sensing reasoning framework of GeoChat.\\n3.2. Training Details\\nTo enhance the effectiveness of our model on general visual',\n",
       "   '[USER] <im_start> Image Features <im_end> [Task Identifier] [ASSISTANT]Figure 3. Multi-task instruction template for GeoChat.\\nLow-Rank Adaptation (LoRA) [11] based strategy is used\\nfor fine-tuning the LLM. While training, instead of finetun-\\ning all of the weights that comprise the weight matrix of the\\npre-trained Vicuna-v1.5[7], we finetune two smaller matri-\\nces in LoRA [11] that approximate the original larger ma-\\ntrix. After that, the fine-tuned adaptor is fed into the pre-',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and',\n",
       "   'lstm architecture. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval , pages 695‚Äì704. ACM, 2017. 3, 4, 5\\n[29] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham\\nCholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin,\\nEric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards\\naccurate and lightweight fully transparent gpt. arXiv preprint\\narXiv:2402.16840 , 2024. 2\\n[30] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mul-']},\n",
       " 'What is the reference number for the paper that discusses bootstrapping language-image pre-training?': {'uuid': 'e1592506-175f-48a4-b396-2c6ee34f6f6f',\n",
       "  'ref_answer': 'The reference number is 15',\n",
       "  'gen_answer': 'There seems to be no direct reference in the provided context about a paper discussing bootstrapping language-image pre-training. The documents listed primarily discuss various models and methodologies related to remote sensing, vision-language models, instruction tuning, and large language model applications; however, none specifically address bootstrapping language-image pre-training.',\n",
       "  'context': ['Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   'vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.',\n",
       "   'understanding via large vision and language models. arXiv\\npreprint arXiv:2306.05424 , 2023. 2\\n[23] OpenAI. Gpt-4 technical report, 2023. 1\\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748‚Äì8763. PMLR, 2021. 7',\n",
       "   'Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 7, 8\\n[5] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-\\ning image scene classification: Benchmark and state of the\\nart.Proceedings of the IEEE , 105(10):1865‚Äì1883, 2017. 2,\\n3, 5\\n[6] Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo']},\n",
       " \"Who are the authors of the paper titled 'Rs-clip: Zero shot remote sensing scene classification via contrastive'?\": {'uuid': 'e1592506-175f-48a4-b396-2c6ee34f6f6f',\n",
       "  'ref_answer': 'The authors are Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou',\n",
       "  'gen_answer': 'Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou.',\n",
       "  'context': ['and-vision assistant for biomedicine in one day. arXiv\\npreprint arXiv:2306.00890 , 2023. 2\\n[15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\\nBlip: Bootstrapping language-image pre-training for uni-\\nfied vision-language understanding and generation. In In-\\nternational Conference on Machine Learning , pages 12888‚Äì\\n12900. PMLR, 2022. 5\\n[16] Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou. Rs-clip:\\nZero shot remote sensing scene classification via contrastive',\n",
       "   'outperforms other zero-shot models and performs competitively to\\nSoTA-supervised models like RSGPT which are specifically fine-\\ntuned on target dataset (while ours is a generic model not specifi-\\ncally finetuned on target dataset).\\n[34] is a large-scale aerial image collection compiled from\\nGoogle Earth imagery, with 30 classes, such as a river,\\ndense residential area, etc. The images are labeled by spe-\\ncialists in the field of remote sensing image interpretation.',\n",
       "   'Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,',\n",
       "   'scene classification. IEEE Transactions on Geoscience and\\nRemote Sensing , 55(7):3965‚Äì3981, 2017. 7\\n[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-\\nlongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-\\npei Zhang. Dota: A large-scale dataset for object detection in\\naerial images. In The IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , 2018. 5\\n[36] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-']},\n",
       " \"What is the arXiv preprint number of the paper 'Improved baselines with visual instruction tuning'?\": {'uuid': '9588d12d-a7c6-49db-a06e-02092a4ae1a7',\n",
       "  'ref_answer': 'arXiv:2310.03744',\n",
       "  'gen_answer': 'arXiv:2310.03744',\n",
       "  'context': ['vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.',\n",
       "   'Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video',\n",
       "   'arXiv preprint arXiv:2111.02114 , 2021. 5\\n9\\n27839',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-']},\n",
       " \"Which year was the paper 'vision-language supervision' published?\": {'uuid': '9588d12d-a7c6-49db-a06e-02092a4ae1a7',\n",
       "  'ref_answer': '2023',\n",
       "  'gen_answer': '2023',\n",
       "  'context': ['vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.',\n",
       "   'Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\\nFung, and Steven Hoi. Instructblip: Towards general-\\npurpose vision-language models with instruction tuning,\\n2023. 2\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. ICLR , 2021. 2',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   'understanding via large vision and language models. arXiv\\npreprint arXiv:2306.05424 , 2023. 2\\n[23] OpenAI. Gpt-4 technical report, 2023. 1\\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748‚Äì8763. PMLR, 2021. 7']},\n",
       " \"What is the publication year of the paper 'Decoupled weight decay regularization'?\": {'uuid': 'b75373ce-9d75-4d18-aaaf-10589779bedf',\n",
       "  'ref_answer': '2017',\n",
       "  'gen_answer': '2017',\n",
       "  'context': ['Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video',\n",
       "   'vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and',\n",
       "   'GeoChat\\n : Grounded Large Vision-Language Model for Remote Sensing\\nKartik Kuckreja1, 2* Muhammad Sohail Danish1*Muzammal Naseer1\\nAbhijit Das2Salman Khan1, 3Fahad Shahbaz Khan1, 4\\n1Mohamed bin Zayed University of AI,2Birla Institute of Technology & Science, Hyderabad\\n3Australian National University,4Link ¬®oping University\\nkartik.kuckreja@mbzuai.ac.ae, muhammad.sohail@mbzuai.ac.ae\\nAbstract\\nRecent advancements in Large Vision-Language Mod-\\nels (VLMs) have shown great promise in natural image do-']},\n",
       " \"Which authors are associated with the work titled 'Video-chatgpt: Towards detailed video'?\": {'uuid': 'b75373ce-9d75-4d18-aaaf-10589779bedf',\n",
       "  'ref_answer': 'Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan',\n",
       "  'gen_answer': 'Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, Fahad Shahbaz Khan.',\n",
       "  'context': ['Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi,\\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\\nMinigpt-v2: large language model as a unified interface\\nfor vision-language multi-task learning. arXiv preprint\\narXiv:2310.09478 , 2023. 7, 8\\n[5] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens-\\ning image scene classification: Benchmark and state of the\\nart.Proceedings of the IEEE , 105(10):1865‚Äì1883, 2017. 2,\\n3, 5\\n[6] Gong Cheng, Jiabao Wang, Ke Li, Xingxing Xie, Chunbo',\n",
       "   'Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video',\n",
       "   'GeoChat\\n : Grounded Large Vision-Language Model for Remote Sensing\\nKartik Kuckreja1, 2* Muhammad Sohail Danish1*Muzammal Naseer1\\nAbhijit Das2Salman Khan1, 3Fahad Shahbaz Khan1, 4\\n1Mohamed bin Zayed University of AI,2Birla Institute of Technology & Science, Hyderabad\\n3Australian National University,4Link ¬®oping University\\nkartik.kuckreja@mbzuai.ac.ae, muhammad.sohail@mbzuai.ac.ae\\nAbstract\\nRecent advancements in Large Vision-Language Mod-\\nels (VLMs) have shown great promise in natural image do-',\n",
       "   'Lang, Yanqing Yao, and Junwei Han. Anchor-free oriented\\nproposal generator for object detection. IEEE Transactions\\non Geoscience and Remote Sensing , 60:1‚Äì11, 2022. 5\\n[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\\nXing. Vicuna: An open-source chatbot impressing gpt-4\\nwith 90%* chatgpt quality, 2023. 2, 4, 5, 7\\n[8] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat']},\n",
       " \"In what year was the 'Visual instruction tuning' work mentioned?\": {'uuid': 'b75373ce-9d75-4d18-aaaf-10589779bedf',\n",
       "  'ref_answer': '2023',\n",
       "  'gen_answer': '2023',\n",
       "  'context': ['vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.',\n",
       "   'Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video',\n",
       "   'instruction-tuning conversational dataset. Inspired by re-\\ncent work in instruction-tuning [14, 19, 29, 42], GeoChat\\nuses Vicuna-v1.5 [7] and an automated pipeline to generate\\ndiverse remote sensing multimodal instruction-following\\ndata comprising of nearly 318kinstructions. We create\\nthe image-text pairs from various existing remote sensing\\ndatasets developed for diverse tasks. These includes LR-\\nBEN for VQA [20], NWPU-RESISC-45 for scene classifi-',\n",
       "   'Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\\nFung, and Steven Hoi. Instructblip: Towards general-\\npurpose vision-language models with instruction tuning,\\n2023. 2\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\\nworth 16x16 words: Transformers for image recognition at\\nscale. ICLR , 2021. 2']},\n",
       " 'What year was the arXiv preprint published?': {'uuid': 'e253846a-71de-4252-b554-4da123b3a98e',\n",
       "  'ref_answer': '2023',\n",
       "  'gen_answer': '2023',\n",
       "  'context': ['arXiv preprint arXiv:2111.02114 , 2021. 5\\n9\\n27839',\n",
       "   'understanding via large vision and language models. arXiv\\npreprint arXiv:2306.05424 , 2023. 2\\n[23] OpenAI. Gpt-4 technical report, 2023. 1\\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748‚Äì8763. PMLR, 2021. 7',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   '8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.']},\n",
       " 'In which conference was the paper by Alec Radford and colleagues presented?': {'uuid': 'e253846a-71de-4252-b554-4da123b3a98e',\n",
       "  'ref_answer': 'International conference on machine learning',\n",
       "  'gen_answer': 'International conference on machine learning',\n",
       "  'context': ['understanding via large vision and language models. arXiv\\npreprint arXiv:2306.05424 , 2023. 2\\n[23] OpenAI. Gpt-4 technical report, 2023. 1\\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748‚Äì8763. PMLR, 2021. 7',\n",
       "   'arXiv preprint arXiv:2111.02114 , 2021. 5\\n9\\n27839',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   'lstm architecture. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval , pages 695‚Äì704. ACM, 2017. 3, 4, 5\\n[29] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham\\nCholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin,\\nEric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards\\naccurate and lightweight fully transparent gpt. arXiv preprint\\narXiv:2402.16840 , 2024. 2\\n[30] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mul-']},\n",
       " 'Who are the authors of the Laion-400m dataset?': {'uuid': '774e7322-1726-41ac-ad7f-c62f0d698765',\n",
       "  'ref_answer': 'Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki',\n",
       "  'gen_answer': 'Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki.',\n",
       "  'context': ['8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.',\n",
       "   '[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping\\nWang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao\\nXu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun\\nFu. Fair1m: A benchmark dataset for fine-grained object\\nrecognition in high-resolution remote sensing imagery. IS-\\nPRS Journal of Photogrammetry and Remote Sensing , 184:\\n116‚Äì130, 2022. 5\\n[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui.\\nLearning to rank question answer pairs with holographic dual',\n",
       "   'lappilly, Hisham Cholakkal, Rao Muhammad Anwer,\\nSalman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan.\\nXraygpt: Chest radiographs summarization using large med-\\nical vision-language models. arXiv: 2306.07971 , 2023. 2\\n[31] Di Wang, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei\\nZhang. Scaling-up remote sensing segmentation dataset with\\nsegment anything model. In arxiv , 2023. 2, 5, 6, 8\\n[32] Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du,\\nDacheng Tao, and Liangpei Zhang. Advancing plain vision',\n",
       "   'vision-language supervision. International Journal of Ap-\\nplied Earth Observation and Geoinformation , 124:103497,\\n2023. 2\\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\\nImproved baselines with visual instruction tuning. arXiv\\npreprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\\nVisual instruction tuning. arXiv preprint arXiv:2304.08485 ,\\n2023. 2\\n[19] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.']},\n",
       " 'What is the main feature of the Laion-400m dataset?': {'uuid': '774e7322-1726-41ac-ad7f-c62f0d698765',\n",
       "  'ref_answer': 'clip-filtered 400 million image-text pairs',\n",
       "  'gen_answer': 'Laion-400m is an open dataset containing 400 million image-text pairs that have been filtered using clips.',\n",
       "  'context': ['8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.',\n",
       "   '[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping\\nWang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao\\nXu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun\\nFu. Fair1m: A benchmark dataset for fine-grained object\\nrecognition in high-resolution remote sensing imagery. IS-\\nPRS Journal of Photogrammetry and Remote Sensing , 184:\\n116‚Äì130, 2022. 5\\n[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui.\\nLearning to rank question answer pairs with holographic dual',\n",
       "   'Dataset Category # Classes # Images Image Size\\nDOTA Object Detection 18 17,480 1024 √ó1024\\nDIOR Object Detection 20 23,463 800 √ó800\\nFAIR1M Object Detection 37 64,147 600 √ó600\\nLRBEN(rsvqa) Visual Question Answering - 600 256 √ó256\\nFloodnet Visual Question Answering - 4056 3000 √ó4000\\nNWPU-RESISC-45 Scene Classification 45 31,500 256 √ó256\\nTable 2. List of datasets used to creat our remote-sensing instruc-\\ntion set for GeoChat VLM training. We include object detection,',\n",
       "   'outperforms other zero-shot models and performs competitively to\\nSoTA-supervised models like RSGPT which are specifically fine-\\ntuned on target dataset (while ours is a generic model not specifi-\\ncally finetuned on target dataset).\\n[34] is a large-scale aerial image collection compiled from\\nGoogle Earth imagery, with 30 classes, such as a river,\\ndense residential area, etc. The images are labeled by spe-\\ncialists in the field of remote sensing image interpretation.']},\n",
       " \"What are the page numbers of the publication 'Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery'?\": {'uuid': 'bae3d30e-8386-49ae-80e4-61f9e1036b21',\n",
       "  'ref_answer': '116‚Äì130',\n",
       "  'gen_answer': '116‚Äì130',\n",
       "  'context': ['[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping\\nWang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao\\nXu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun\\nFu. Fair1m: A benchmark dataset for fine-grained object\\nrecognition in high-resolution remote sensing imagery. IS-\\nPRS Journal of Photogrammetry and Remote Sensing , 184:\\n116‚Äì130, 2022. 5\\n[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui.\\nLearning to rank question answer pairs with holographic dual',\n",
       "   '8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.',\n",
       "   'scene classification. IEEE Transactions on Geoscience and\\nRemote Sensing , 55(7):3965‚Äì3981, 2017. 7\\n[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-\\nlongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-\\npei Zhang. Dota: A large-scale dataset for object detection in\\naerial images. In The IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , 2018. 5\\n[36] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-']},\n",
       " 'What is the citation number for the Mobillama paper?': {'uuid': '24283ff2-74d3-43ed-89d6-d15a52483a26',\n",
       "  'ref_answer': '2',\n",
       "  'gen_answer': '2402.16840',\n",
       "  'context': ['lstm architecture. In Proceedings of the 40th International\\nACM SIGIR Conference on Research and Development in\\nInformation Retrieval , pages 695‚Äì704. ACM, 2017. 3, 4, 5\\n[29] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham\\nCholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin,\\nEric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards\\naccurate and lightweight fully transparent gpt. arXiv preprint\\narXiv:2402.16840 , 2024. 2\\n[30] Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mul-',\n",
       "   'arXiv preprint arXiv:2111.02114 , 2021. 5\\n9\\n27839',\n",
       "   '8748‚Äì8763. PMLR, 2021. 7\\n[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho\\nSarkar, Debvrat Varshney, Masoud Yari, and Robin Mur-\\nphy. Floodnet: A high resolution aerial imagery dataset\\nfor post flood scene understanding. arXiv preprint\\narXiv:2012.02951 , 2020. 3, 5\\n[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\\nOpen dataset of clip-filtered 400 million image-text pairs.',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-']},\n",
       " \"Which journal published the paper 'transformer toward remote sensing foundation model'?\": {'uuid': '2bd164ad-af46-4f8c-b077-f21b75d1bc59',\n",
       "  'ref_answer': 'IEEE Transactions on Geoscience and Remote Sensing',\n",
       "  'gen_answer': 'IEEE Transactions on Geoscience and Remote Sensing',\n",
       "  'context': ['transformer toward remote sensing foundation model. IEEE\\nTransactions on Geoscience and Remote Sensing , 61:1‚Äì15,\\n2023. 6\\n[33] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and\\nYanfei Zhong. LoveDA: A remote sensing land-cover dataset\\nfor domain adaptive semantic segmentation, 2021. 6\\n[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang\\nBai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid:\\nA benchmark data set for performance evaluation of aerial',\n",
       "   'IEEE Transactions on Geoscience and Remote Sensing , 60:\\n1‚Äì11, 2022. 2, 7\\n[40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring\\ndata and models for visual grounding on remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 61:\\n1‚Äì13, 2023. 6\\n[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua\\nChen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-\\ntial hierarchical reasoning network for remote sensing visual\\nquestion answering. IEEE Transactions on Geoscience and',\n",
       "   'Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840',\n",
       "   'References\\n[1] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\\nZhou. Qwen-vl: A frontier large vision-language model with\\nversatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7,\\n8\\n[2] Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mo-\\nhamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and\\nFarid Melgani. Bi-modal transformer-based approach for vi-\\nsual question answering in remote sensing imagery. IEEE']},\n",
       " 'What is the focus of the research described in the IEEE Transactions on Geoscience and Remote Sensing citation?': {'uuid': '3556a521-8749-4771-96b3-fe1f968c2d6b',\n",
       "  'ref_answer': 'The research focuses on scene classification.',\n",
       "  'gen_answer': 'The focus of the research is on developing methods for visual grounding, spatial hierarchical reasoning, transformer models, remote sensing land-cover datasets, domain adaptive semantic segmentation, aerial visual instruction tuning, and visual question answering for remote sensing data.',\n",
       "  'context': ['IEEE Transactions on Geoscience and Remote Sensing , 60:\\n1‚Äì11, 2022. 2, 7\\n[40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring\\ndata and models for visual grounding on remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 61:\\n1‚Äì13, 2023. 6\\n[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua\\nChen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-\\ntial hierarchical reasoning network for remote sensing visual\\nquestion answering. IEEE Transactions on Geoscience and',\n",
       "   'Transactions on Geoscience and Remote Sensing , 60:1‚Äì11,\\n2022. 7\\n[3] Christel Chappuis, Val ¬¥erie Zermatten, Sylvain Lobry,\\nBertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting\\nvisual context to a language model for remote sensing visual\\nquestion answering. In Proceedings of the IEEE/CVF Con-\\nference on Computer Vision and Pattern Recognition , pages\\n1372‚Äì1381, 2022. 2\\n[4] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi,',\n",
       "   'transformer toward remote sensing foundation model. IEEE\\nTransactions on Geoscience and Remote Sensing , 61:1‚Äì15,\\n2023. 6\\n[33] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and\\nYanfei Zhong. LoveDA: A remote sensing land-cover dataset\\nfor domain adaptive semantic segmentation, 2021. 6\\n[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang\\nBai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid:\\nA benchmark data set for performance evaluation of aerial',\n",
       "   'Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia.\\nRsvqa: Visual question answering for remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 58\\n(12):8555‚Äì8566, 2020. 2, 3, 5, 7, 8\\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\nregularization. arXiv preprint arXiv:1711.05101 , 2017. 7\\n[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fa-\\nhad Shahbaz Khan. Video-chatgpt: Towards detailed video']},\n",
       " 'Who are the authors of the work published in Transactions of the Association for Computational Linguistics in 2014?': {'uuid': '7c42c969-4b9b-4d90-9428-492913954435',\n",
       "  'ref_answer': 'Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier',\n",
       "  'gen_answer': 'Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg.',\n",
       "  'context': ['Linguistics , 2:67‚Äì78, 2014. 7\\n[38] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\\nand Tamara L Berg. Modeling context in referring expres-\\nsions. In Computer Vision‚ÄìECCV 2016: 14th European\\nConference, Amsterdam, The Netherlands, October 11-14,\\n2016, Proceedings, Part II 14 , pages 69‚Äì85. Springer, 2016.\\n7[39] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang\\nZhu. From easy to hard: Learning language-guided curricu-\\nlum for visual question answering on remote sensing data.',\n",
       "   'Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   'understanding via large vision and language models. arXiv\\npreprint arXiv:2306.05424 , 2023. 2\\n[23] OpenAI. Gpt-4 technical report, 2023. 1\\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\\ntransferable visual models from natural language supervi-\\nsion. In International conference on machine learning , pages\\n8748‚Äì8763. PMLR, 2021. 7',\n",
       "   'scale. ICLR , 2021. 2\\n[10] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\\nunits (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\\n[11] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\\nLoRA: Low-rank adaptation of large language models. In In-\\nternational Conference on Learning Representations , 2022.\\n2, 4, 5, 7\\n[12] Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and']},\n",
       " 'On what pages was the paper about land-use classification presented at the conference?': {'uuid': '7c42c969-4b9b-4d90-9428-492913954435',\n",
       "  'ref_answer': 'pages 270‚Äì279',\n",
       "  'gen_answer': 'Pages 270‚Äì279',\n",
       "  'context': ['tial extensions for land-use classification. In Proceedings of\\nthe 18th SIGSPATIAL international conference on advances\\nin geographic information systems , pages 270‚Äì279, 2010. 7\\n[37] Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken-\\nmaier. From image descriptions to visual denotations: New\\nsimilarity metrics for semantic inference over event descrip-\\ntions. Transactions of the Association for Computational\\nLinguistics , 2:67‚Äì78, 2014. 7',\n",
       "   'scene classification. IEEE Transactions on Geoscience and\\nRemote Sensing , 55(7):3965‚Äì3981, 2017. 7\\n[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-\\nlongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-\\npei Zhang. Dota: A large-scale dataset for object detection in\\naerial images. In The IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR) , 2018. 5\\n[36] Yi Yang and Shawn Newsam. Bag-of-visual-words and spa-',\n",
       "   'Remote Sensing , 61:1‚Äì15, 2023. 2, 7\\n[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\\nunderstanding with advanced large language models. arXiv\\npreprint arXiv:2304.10592 , 2023. 2\\n[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transform-\\ning remote sensing images to textual descriptions. Interna-\\ntional Journal of Applied Earth Observation and Geoinfor-\\nmation , 108:102741, 2022. 2\\n10\\n27840',\n",
       "   'transformer toward remote sensing foundation model. IEEE\\nTransactions on Geoscience and Remote Sensing , 61:1‚Äì15,\\n2023. 6\\n[33] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and\\nYanfei Zhong. LoveDA: A remote sensing land-cover dataset\\nfor domain adaptive semantic segmentation, 2021. 6\\n[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang\\nBai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid:\\nA benchmark data set for performance evaluation of aerial']},\n",
       " 'What topic does the paper by Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu address?': {'uuid': '2c17ae39-4bfb-4c8f-ace5-b4e475dd5de2',\n",
       "  'ref_answer': 'The paper addresses learning language-guided curriculum for visual question answering on remote sensing data.',\n",
       "  'gen_answer': 'The paper by Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu addresses learning language-guided curriculum for visual question answering on remote sensing data.',\n",
       "  'context': ['Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1,\\n2, 7, 8\\n[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal\\nmodel with in-context instruction tuning. arXiv preprint\\narXiv:2305.03726 , 2023. 2\\n[14] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\\nand Jianfeng Gao. Llava-med: Training a large language-',\n",
       "   'IEEE Transactions on Geoscience and Remote Sensing , 60:\\n1‚Äì11, 2022. 2, 7\\n[40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring\\ndata and models for visual grounding on remote sensing data.\\nIEEE Transactions on Geoscience and Remote Sensing , 61:\\n1‚Äì13, 2023. 6\\n[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua\\nChen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-\\ntial hierarchical reasoning network for remote sensing visual\\nquestion answering. IEEE Transactions on Geoscience and',\n",
       "   '[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping\\nWang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao\\nXu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun\\nFu. Fair1m: A benchmark dataset for fine-grained object\\nrecognition in high-resolution remote sensing imagery. IS-\\nPRS Journal of Photogrammetry and Remote Sensing , 184:\\n116‚Äì130, 2022. 5\\n[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui.\\nLearning to rank question answer pairs with holographic dual',\n",
       "   'Linguistics , 2:67‚Äì78, 2014. 7\\n[38] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg,\\nand Tamara L Berg. Modeling context in referring expres-\\nsions. In Computer Vision‚ÄìECCV 2016: 14th European\\nConference, Amsterdam, The Netherlands, October 11-14,\\n2016, Proceedings, Part II 14 , pages 69‚Äì85. Springer, 2016.\\n7[39] Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang\\nZhu. From easy to hard: Learning language-guided curricu-\\nlum for visual question answering on remote sensing data.']}}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1Tq4E-QmizCF",
   "metadata": {
    "id": "1Tq4E-QmizCF"
   },
   "outputs": [],
   "source": [
    "prediction_df.loc[:, 'gen_answer'] = prediction_df['question'].apply(lambda q: answer_dict[q]['gen_answer'])\n",
    "prediction_df.loc[:, 'context'] = prediction_df['question'].apply(lambda q: answer_dict[q]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "RJdE6AFRizr_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "RJdE6AFRizr_",
    "outputId": "dd08120b-ffcc-4c2f-b2b1-3313f466660b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uuid</th>\n",
       "      <th>question</th>\n",
       "      <th>qa_type</th>\n",
       "      <th>answer</th>\n",
       "      <th>gen_answer</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>58eff65a-541e-4bf4-8074-b921545dd641</td>\n",
       "      <td>Overall, the model performance is low on small...</td>\n",
       "      <td>detailed</td>\n",
       "      <td>Model performance is generally low on small ob...</td>\n",
       "      <td>Our model works better on medium size images c...</td>\n",
       "      <td>[and MiniGPT-4-v2 [4] on the proposed benchmar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>ffb10c2c-e465-4e41-9e07-279e85b62f9b</td>\n",
       "      <td>Is the language model used in GeoChat open sou...</td>\n",
       "      <td>detailed</td>\n",
       "      <td>Yes, the language model used in GeoChat is ope...</td>\n",
       "      <td>No information about the openness of the langu...</td>\n",
       "      <td>[language model - GeoChat. Our LoRA [11] fine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>faa82d57-2e4c-4ade-8839-63cd3c9bb14f</td>\n",
       "      <td>What interval are the x and y coordinates norm...</td>\n",
       "      <td>detailed</td>\n",
       "      <td>The x and y coordinates are normalised within ...</td>\n",
       "      <td>The x and y coordinates are normalised within ...</td>\n",
       "      <td>[normalised within the interval [0, 100] are u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>7a2d75df-ef26-44aa-929a-249cd555b9d1</td>\n",
       "      <td>What is being interpolated in the transformer-...</td>\n",
       "      <td>detailed</td>\n",
       "      <td>The positional encoding is being interpolated ...</td>\n",
       "      <td>Positional encoding.</td>\n",
       "      <td>[small objects and object details), we interpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>b75373ce-9d75-4d18-aaaf-10589779bedf</td>\n",
       "      <td>In what year was the 'Visual instruction tunin...</td>\n",
       "      <td>detailed</td>\n",
       "      <td>2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>[vision-language supervision. International Jo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     uuid  \\\n",
       "463  58eff65a-541e-4bf4-8074-b921545dd641   \n",
       "225  ffb10c2c-e465-4e41-9e07-279e85b62f9b   \n",
       "179  faa82d57-2e4c-4ade-8839-63cd3c9bb14f   \n",
       "185  7a2d75df-ef26-44aa-929a-249cd555b9d1   \n",
       "542  b75373ce-9d75-4d18-aaaf-10589779bedf   \n",
       "\n",
       "                                              question   qa_type  \\\n",
       "463  Overall, the model performance is low on small...  detailed   \n",
       "225  Is the language model used in GeoChat open sou...  detailed   \n",
       "179  What interval are the x and y coordinates norm...  detailed   \n",
       "185  What is being interpolated in the transformer-...  detailed   \n",
       "542  In what year was the 'Visual instruction tunin...  detailed   \n",
       "\n",
       "                                                answer  \\\n",
       "463  Model performance is generally low on small ob...   \n",
       "225  Yes, the language model used in GeoChat is ope...   \n",
       "179  The x and y coordinates are normalised within ...   \n",
       "185  The positional encoding is being interpolated ...   \n",
       "542                                               2023   \n",
       "\n",
       "                                            gen_answer  \\\n",
       "463  Our model works better on medium size images c...   \n",
       "225  No information about the openness of the langu...   \n",
       "179  The x and y coordinates are normalised within ...   \n",
       "185                               Positional encoding.   \n",
       "542                                               2023   \n",
       "\n",
       "                                               context  \n",
       "463  [and MiniGPT-4-v2 [4] on the proposed benchmar...  \n",
       "225  [language model - GeoChat. Our LoRA [11] fine-...  \n",
       "179  [normalised within the interval [0, 100] are u...  \n",
       "185  [small objects and object details), we interpo...  \n",
       "542  [vision-language supervision. International Jo...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.sample(5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gold-yolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0312233d069f47c689d98e7e0fa7da5b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_94d281a28c64465294758a200817ada1",
       "IPY_MODEL_6efb43f11ea045038eade7ea773cdd8e",
       "IPY_MODEL_3bc9f26eda0e4f818e6f23df31195c5e"
      ],
      "layout": "IPY_MODEL_b2bb7e0af2d442a3974e8471c1765d54"
     }
    },
    "0367de25b0804a079fef3a9fa4a1b2f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a391251a1c54dbba308f304d9d0b6dd",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_fe47f740059d4216b018d9439bc9aafa",
      "value": "‚Äá394/394‚Äá[00:00&lt;00:00,‚Äá17.7kB/s]"
     }
    },
    "03ad8540620b4a0b977503df9550e03e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38a531c0e0454085b6a8bad2b6fe162b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_5946f069abe14153a0929aafca9bbdfc",
      "value": "‚Äá191/191‚Äá[00:00&lt;00:00,‚Äá10.0kB/s]"
     }
    },
    "044f48908671438ab65b8040a4b09f40": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04b4df7da6054cebb1b4a7361d904517": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_caa7636b326743e0a0b9075d029bf908",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_c66e0318f9b444af84686d737555ace6",
      "value": "‚Äá124/124‚Äá[00:00&lt;00:00,‚Äá9.05kB/s]"
     }
    },
    "04d57f5f4e694641a38239de125e446d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ed00c82e5220472d822e8dee4d773608",
      "max": 349,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_db0cded2a2e14f7c821eb10ea54a6b80",
      "value": 349
     }
    },
    "05a457d2c35b403e9ad2d0cdee3b617d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "05ddafa8e53b4323a3b87773d8c52d7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cc3634bb596c4730a6ff72fbf6093e2b",
       "IPY_MODEL_44f1502b9d8c44bdb8e67823df3b61df",
       "IPY_MODEL_d7b73b320a214a1c8564cb87c9fc7c1f"
      ],
      "layout": "IPY_MODEL_ce2e09ea7a584e87828f247c01e710ef"
     }
    },
    "0bbf9d684d77494794eb50139323f82b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b94356d13b624b87afe94eae90c8bb73",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ade929269f854be2bed144b486671d6c",
      "value": "100%"
     }
    },
    "0c24bf2912564885944b15fe875b9e86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e8bdeab87ba4aa4863bc8cb1f5d05e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fab84da34e7b4acaac607f3ebf41fc5e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6d56498445964a739f55bd5914671611",
      "value": "‚Äá30.4k/30.4k‚Äá[00:00&lt;00:00,‚Äá1.20MB/s]"
     }
    },
    "0fa3d8cc67f245679850c5b9e9ba7b6b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1199feec159e44c3a5ebdb582e687950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6024d09af914986bec1834c773f1627",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fd7a02cc1f884940bb98c5bed786d5a1",
      "value": 125
     }
    },
    "12126a45430f407d8c903f408679df7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14963d0f85154fbf9ab49ef84840f921": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0bbf9d684d77494794eb50139323f82b",
       "IPY_MODEL_25d2069884884c5d9b145cbbc6d454f2",
       "IPY_MODEL_85b79c6c8ebb440eb8ff423dbc75c183"
      ],
      "layout": "IPY_MODEL_6da130e16c28435d983f5b817c406265"
     }
    },
    "156f58b920b143c4b2147878f84a2809": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "173b837173314d86b948673d376f5c3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55bf13ec08794f5c821fe62b1104c89b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_425ebec3c1ca4393ab7bb6f45b694d82",
      "value": "‚Äá125/125‚Äá[00:00&lt;00:00,‚Äá9.11kB/s]"
     }
    },
    "173f5ea18ee643eda658ed6ec7fb8888": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1afffc47ddfa4095bcdf42fef01b1771": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b0eeba3def649918fbc096799a7b279": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1bb07d56deff47b89d80c88460420a28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2312778eaac94a2696f435c8443b58f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12126a45430f407d8c903f408679df7d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_432e5082c3424903a2e0fcf857947d6f",
      "value": "vocab.txt:‚Äá100%"
     }
    },
    "2528a771e3174761ac4fdbf5bb737722": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25d2069884884c5d9b145cbbc6d454f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dba47c2b2dc41cda3afc4918321f605",
      "max": 93,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cf393ed116a248e9a924daa4ff24da64",
      "value": 93
     }
    },
    "269f2ddeec3e4ae6a9d13a46e2a831a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "274c88624c3449799ec984186146b688": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a1fa9e5abcc40a5b27565b86eba4f2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2b252be54a4e4758b9e039ce0d16ca3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_269f2ddeec3e4ae6a9d13a46e2a831a1",
      "max": 109540,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_05a457d2c35b403e9ad2d0cdee3b617d",
      "value": 109540
     }
    },
    "2bd0a1cdd3a0493db3a2b7c9ea999614": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5167b90dfe31453f98f7a97c1bf45e77",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_36f12265773f4c95b0665bfaf3a383db",
      "value": "‚Äá1.30G/1.30G‚Äá[00:08&lt;00:00,‚Äá274MB/s]"
     }
    },
    "2d14e7f153a341c4b1848d5666a014cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3228b0e6be2146928c6dc0fbd5d2636e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "354eacb8b3ff436aa6e9c98a0392a478": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87abda91a5274305b493028d82c72e22",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6ef4117d7a3045bbbab01f71b6d9c828",
      "value": "‚Äá110k/110k‚Äá[00:00&lt;00:00,‚Äá865kB/s]"
     }
    },
    "36f12265773f4c95b0665bfaf3a383db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "385a708cb16f4de9bf83796f6c321d34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38a531c0e0454085b6a8bad2b6fe162b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a24ba6d66704525a0f65044d36c3116": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3bc9f26eda0e4f818e6f23df31195c5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_683160f1e0ca40599b35d8ba784bc10a",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bdfb41f5379d4c12872ad647c51937e8",
      "value": "‚Äá439k/439k‚Äá[00:00&lt;00:00,‚Äá3.47MB/s]"
     }
    },
    "425ebec3c1ca4393ab7bb6f45b694d82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "432e5082c3424903a2e0fcf857947d6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44f1502b9d8c44bdb8e67823df3b61df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c24bf2912564885944b15fe875b9e86",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2a1fa9e5abcc40a5b27565b86eba4f2a",
      "value": 1000
     }
    },
    "474dde96127645e38017b38b11a78a01": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4bf9408046b24e70bcbc434ce81f1586": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b3e6e10a9c0945a2b29c3455a6db590e",
       "IPY_MODEL_d8bf0fbdcbc24b938ccc3f5525496c8b",
       "IPY_MODEL_68549cda864b414c9cc9c69c52c1c7bb"
      ],
      "layout": "IPY_MODEL_3228b0e6be2146928c6dc0fbd5d2636e"
     }
    },
    "4d7be10c64b94c98965305f696f9398a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4dd431b7458e4cfdb0b45ad6f4560574": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e2cca81afd544b5af33bad236a574cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2312778eaac94a2696f435c8443b58f7",
       "IPY_MODEL_2b252be54a4e4758b9e039ce0d16ca3c",
       "IPY_MODEL_354eacb8b3ff436aa6e9c98a0392a478"
      ],
      "layout": "IPY_MODEL_4dd431b7458e4cfdb0b45ad6f4560574"
     }
    },
    "5167b90dfe31453f98f7a97c1bf45e77": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51ae5239814d484386f1f776b24b1c6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "525fb3b924614e74937ae9cd26711527": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "549d665ac28c4d4bb3be5593c4653191": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_819646ccaab2464d8688a31e4f345d96",
       "IPY_MODEL_1199feec159e44c3a5ebdb582e687950",
       "IPY_MODEL_173b837173314d86b948673d376f5c3c"
      ],
      "layout": "IPY_MODEL_b0dec5996bcf42f3a9b798c07180513d"
     }
    },
    "55bf13ec08794f5c821fe62b1104c89b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5946f069abe14153a0929aafca9bbdfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "608807c98d464ba987fe6c08e6e8958c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "642c1f7b73504ab8af704a9cb901f895": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64c85d2d6d164b579de82467cd0fc5a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "683160f1e0ca40599b35d8ba784bc10a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68549cda864b414c9cc9c69c52c1c7bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_385a708cb16f4de9bf83796f6c321d34",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_93c6143ee1e34a589f102307fce2be46",
      "value": "‚Äá52.0/52.0‚Äá[00:00&lt;00:00,‚Äá3.97kB/s]"
     }
    },
    "6a26f0ee05ff4b1d859d3a31a729c24b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a6de690242042c788e7bf4e93cbc256": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d56498445964a739f55bd5914671611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6da130e16c28435d983f5b817c406265": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ef4117d7a3045bbbab01f71b6d9c828": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6efb43f11ea045038eade7ea773cdd8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b318510199794db1907adce0cdc007ce",
      "max": 439124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_956e4999e48e4d30bc971154d70c1ead",
      "value": 439124
     }
    },
    "70c9685019ed480fa6b1a30b224d3d0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "767ff3484a4d45db9c32932a892521a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "779a96631812433ab007fcc32bda72a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78362c0f00114573bdd9ef5883815140": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78b34247c0ec43a6acf54a0fc913dae3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a06b66a5d314b0cba2873ac7e4ef63f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cb14f003a5f54307a2bcd268238f090b",
       "IPY_MODEL_a16309c75be54ede82350da8480dc63c",
       "IPY_MODEL_ea911d7a87d14b75bc25f9e2fd1127ae"
      ],
      "layout": "IPY_MODEL_c9c618159ae94f2ba9766f418212758f"
     }
    },
    "819646ccaab2464d8688a31e4f345d96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b742e09a17024e9ea54cbe81bb328d8c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f3e5aeafad9b470bbb54880644519aa8",
      "value": "special_tokens_map.json:‚Äá100%"
     }
    },
    "852f2146db4743888460097cec7bf938": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85b79c6c8ebb440eb8ff423dbc75c183": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_608807c98d464ba987fe6c08e6e8958c",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_51ae5239814d484386f1f776b24b1c6a",
      "value": "‚Äá93/93‚Äá[00:03&lt;00:00,‚Äá36.24it/s]"
     }
    },
    "87abda91a5274305b493028d82c72e22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8a391251a1c54dbba308f304d9d0b6dd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ae8db517ad14268a8f7864a01b8191c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a76676eca3684c3eb4a2a1d896c3a966",
       "IPY_MODEL_04d57f5f4e694641a38239de125e446d",
       "IPY_MODEL_92e13d95804a45d4a636dfbdec0772c2"
      ],
      "layout": "IPY_MODEL_a761689717ac4ec89c218a99fce3eb27"
     }
    },
    "8dba47c2b2dc41cda3afc4918321f605": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8df9bde6169b46a498949108e00fbaf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92e13d95804a45d4a636dfbdec0772c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce09b1931e5146d6b4a592afc8146bd6",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ab1782bb839b490f890a7b78c40cb9f9",
      "value": "‚Äá349/349‚Äá[00:00&lt;00:00,‚Äá15.5kB/s]"
     }
    },
    "93c6143ee1e34a589f102307fce2be46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "94d281a28c64465294758a200817ada1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_474dde96127645e38017b38b11a78a01",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_78b34247c0ec43a6acf54a0fc913dae3",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "956e4999e48e4d30bc971154d70c1ead": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9c6d6ecd27ec456f8fe894c8ab31fab1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a14c32c2257e40c7ab724e3de4a0e954",
       "IPY_MODEL_c1e1ac7befd34eefb7463d85ded81796",
       "IPY_MODEL_03ad8540620b4a0b977503df9550e03e"
      ],
      "layout": "IPY_MODEL_4d7be10c64b94c98965305f696f9398a"
     }
    },
    "9e6053b8a79841c1a2c75c110ed2e2c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a14c32c2257e40c7ab724e3de4a0e954": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_852f2146db4743888460097cec7bf938",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_779a96631812433ab007fcc32bda72a3",
      "value": "1_Pooling/config.json:‚Äá100%"
     }
    },
    "a16309c75be54ede82350da8480dc63c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dcb4167367b64c8293b3d963645feb31",
      "max": 100,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1b0eeba3def649918fbc096799a7b279",
      "value": 100
     }
    },
    "a2eb0ac561e045ec9cb44586da815516": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6267c3585994150b0119e18f92b253c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a761689717ac4ec89c218a99fce3eb27": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a76676eca3684c3eb4a2a1d896c3a966": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_156f58b920b143c4b2147878f84a2809",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_d11d56deec8547c48d7bfec194fead9b",
      "value": "modules.json:‚Äá100%"
     }
    },
    "a9798a3378554f408c7bd016abcb1d26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8f8237bbaea49d6824ca103d3a66bba",
       "IPY_MODEL_fc9daa43549645708c23293195087cdb",
       "IPY_MODEL_0e8bdeab87ba4aa4863bc8cb1f5d05e2"
      ],
      "layout": "IPY_MODEL_0fa3d8cc67f245679850c5b9e9ba7b6b"
     }
    },
    "ab1782bb839b490f890a7b78c40cb9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ade929269f854be2bed144b486671d6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b0dec5996bcf42f3a9b798c07180513d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2bb7e0af2d442a3974e8471c1765d54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b318510199794db1907adce0cdc007ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3e6e10a9c0945a2b29c3455a6db590e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cab762281bc642b689349908f8c8d509",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a2eb0ac561e045ec9cb44586da815516",
      "value": "sentence_bert_config.json:‚Äá100%"
     }
    },
    "b51bb55a9e12441fa1cc8bf46fcee318": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_525fb3b924614e74937ae9cd26711527",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_78362c0f00114573bdd9ef5883815140",
      "value": "config_sentence_transformers.json:‚Äá100%"
     }
    },
    "b627f32931c343e3927a930a82c2e180": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b63739062c34496d9e18d64e9d572c04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b730a3024024445eb7b93ce25cf4a8bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b742e09a17024e9ea54cbe81bb328d8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b94356d13b624b87afe94eae90c8bb73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9d1345184cf4f35a7f107079bad50fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bdfb41f5379d4c12872ad647c51937e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be462996e01346b4a1a92fbc17994320": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1e1ac7befd34eefb7463d85ded81796": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a6de690242042c788e7bf4e93cbc256",
      "max": 191,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_767ff3484a4d45db9c32932a892521a5",
      "value": 191
     }
    },
    "c21838ae58a642cd92d5bed9e2d64410": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c66e0318f9b444af84686d737555ace6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6701afdd6f24032acfa724c5d1b5ca3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8f8237bbaea49d6824ca103d3a66bba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7f88ad7f22e4076bcc3a4f5a3348883",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_b627f32931c343e3927a930a82c2e180",
      "value": "README.md:‚Äá100%"
     }
    },
    "c9c618159ae94f2ba9766f418212758f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "caa7636b326743e0a0b9075d029bf908": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cab762281bc642b689349908f8c8d509": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb14f003a5f54307a2bcd268238f090b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_642c1f7b73504ab8af704a9cb901f895",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2d14e7f153a341c4b1848d5666a014cd",
      "value": "100%"
     }
    },
    "cb2afcae4aeb45c4a097532ff5fe0660": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2528a771e3174761ac4fdbf5bb737722",
      "max": 394,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_274c88624c3449799ec984186146b688",
      "value": 394
     }
    },
    "cc3634bb596c4730a6ff72fbf6093e2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b63739062c34496d9e18d64e9d572c04",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3a24ba6d66704525a0f65044d36c3116",
      "value": "config.json:‚Äá100%"
     }
    },
    "cd7dd87b8b924da890595c26a47456e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8df9bde6169b46a498949108e00fbaf8",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b9d1345184cf4f35a7f107079bad50fc",
      "value": 124
     }
    },
    "ce09b1931e5146d6b4a592afc8146bd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce2e09ea7a584e87828f247c01e710ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf393ed116a248e9a924daa4ff24da64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d11d56deec8547c48d7bfec194fead9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1c23ba161fe48deb887bef9a696e98c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee2051c6fa9c4c62abffb2c41ad6cbf9",
       "IPY_MODEL_e41b27af5bdb4917b5c176b14f1d38f1",
       "IPY_MODEL_2bd0a1cdd3a0493db3a2b7c9ea999614"
      ],
      "layout": "IPY_MODEL_1bb07d56deff47b89d80c88460420a28"
     }
    },
    "d4250feb91a547b8ba55a716ed72bc41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe7e1ce77a8e459e817b5c3a061dc3b9",
       "IPY_MODEL_cb2afcae4aeb45c4a097532ff5fe0660",
       "IPY_MODEL_0367de25b0804a079fef3a9fa4a1b2f2"
      ],
      "layout": "IPY_MODEL_044f48908671438ab65b8040a4b09f40"
     }
    },
    "d529d59457d14794862a5c98602eeb51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d7b73b320a214a1c8564cb87c9fc7c1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4705f680041401f9ded40e945e5e72e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_64c85d2d6d164b579de82467cd0fc5a9",
      "value": "‚Äá1.00k/1.00k‚Äá[00:00&lt;00:00,‚Äá74.0kB/s]"
     }
    },
    "d8bf0fbdcbc24b938ccc3f5525496c8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e6053b8a79841c1a2c75c110ed2e2c1",
      "max": 52,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_173f5ea18ee643eda658ed6ec7fb8888",
      "value": 52
     }
    },
    "db0cded2a2e14f7c821eb10ea54a6b80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dcb4167367b64c8293b3d963645feb31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e41b27af5bdb4917b5c176b14f1d38f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4f24a8906f7451abfe8cea83dbc6c2d",
      "max": 1302220525,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d529d59457d14794862a5c98602eeb51",
      "value": 1302220525
     }
    },
    "e6024d09af914986bec1834c773f1627": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8484544ce2c4b82b306676a3c73f9bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea911d7a87d14b75bc25f9e2fd1127ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a26f0ee05ff4b1d859d3a31a729c24b",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_be462996e01346b4a1a92fbc17994320",
      "value": "‚Äá100/100‚Äá[06:00&lt;00:00,‚Äá‚Äá5.72s/it]"
     }
    },
    "ed00c82e5220472d822e8dee4d773608": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee2051c6fa9c4c62abffb2c41ad6cbf9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e8484544ce2c4b82b306676a3c73f9bf",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_70c9685019ed480fa6b1a30b224d3d0a",
      "value": "pytorch_model.bin:‚Äá100%"
     }
    },
    "f3e5aeafad9b470bbb54880644519aa8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4705f680041401f9ded40e945e5e72e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4f24a8906f7451abfe8cea83dbc6c2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7f88ad7f22e4076bcc3a4f5a3348883": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fab84da34e7b4acaac607f3ebf41fc5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc64c338a9234e2b95f2eb7193a8e976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b51bb55a9e12441fa1cc8bf46fcee318",
       "IPY_MODEL_cd7dd87b8b924da890595c26a47456e9",
       "IPY_MODEL_04b4df7da6054cebb1b4a7361d904517"
      ],
      "layout": "IPY_MODEL_b730a3024024445eb7b93ce25cf4a8bf"
     }
    },
    "fc9daa43549645708c23293195087cdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1afffc47ddfa4095bcdf42fef01b1771",
      "max": 30376,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c21838ae58a642cd92d5bed9e2d64410",
      "value": 30376
     }
    },
    "fd7a02cc1f884940bb98c5bed786d5a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fe47f740059d4216b018d9439bc9aafa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fe7e1ce77a8e459e817b5c3a061dc3b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6701afdd6f24032acfa724c5d1b5ca3",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a6267c3585994150b0119e18f92b253c",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
