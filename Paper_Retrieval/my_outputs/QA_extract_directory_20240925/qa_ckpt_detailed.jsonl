{"uuid": "805948dc-9161-4357-b2b7-bb88784386f5", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the document?\", \"context\": \"GeoChat : Grounded Large Vision-Language Model for Remote Sensing\", \"answer\": \"GeoChat: Grounded Large Vision-Language Model for Remote Sensing\"},\n    {\"question\": \"Who are the authors of the document?\", \"context\": \"Kartik Kuckreja1, 2* Muhammad Sohail Danish1*Muzammal Naseer1 Abhijit Das2 Salman Khan1, 3 Fahad Shahbaz Khan1, 4\", \"answer\": \"Kartik Kuckreja, Muhammad Sohail Danish, Muzammal Naseer, Abhijit Das, Salman Khan, Fahad Shahbaz Khan\"},\n    {\"question\": \"Which institutions are the authors affiliated with?\", \"context\": \"1Mohamed bin Zayed University of AI,2Birla Institute of Technology & Science, Hyderabad 3Australian National University,4Link ¨oping University\", \"answer\": \"Mohamed bin Zayed University of AI, Birla Institute of Technology & Science, Hyderabad, Australian National University, Linköping University\"},\n    {\"question\": \"What are the email addresses provided for contact?\", \"context\": \"kartik.kuckreja@mbzuai.ac.ae, muhammad.sohail@mbzuai.ac.ae\", \"answer\": \"kartik.kuckreja@mbzuai.ac.ae, muhammad.sohail@mbzuai.ac.ae\"},\n    {\"question\": \"What type of model is GeoChat?\", \"context\": \"GeoChat : Grounded Large Vision-Language Model for Remote Sensing\", \"answer\": \"Grounded Large Vision-Language Model\"},\n    {\"question\": \"In what field is GeoChat intended to be used?\", \"context\": \"GeoChat : Grounded Large Vision-Language Model for Remote Sensing\", \"answer\": \"Remote Sensing\"},\n    {\"question\": \"What recent advancements does the abstract mention?\", \"context\": \"Recent advancements in Large Vision-Language Mod- els (VLMs) have shown great promise in natural image do-\", \"answer\": \"Advancements in Large Vision-Language Models (VLMs)\"},\n    {\"question\": \"What potential application area for VLMs is mentioned in the abstract?\", \"context\": \"Recent advancements in Large Vision-Language Mod- els (VLMs) have shown great promise in natural image do-\", \"answer\": \"Natural image domain\"}\n]\n```"}
{"uuid": "c2fc0cac-d021-4507-aa26-3d023e74b5e1", "raw_resp": "[\n    {\n        \"question\": \"What type of dialogue do VLMs allow users to have?\",\n        \"context\": \"mains, allowing users to hold a dialogue about given vi-sual content. However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inac- curate or fabricated information when presented with RS domain-specific queries.\",\n        \"answer\": \"VLMs allow users to hold a dialogue about given visual content.\"\n    },\n    {\n        \"question\": \"How do general-domain VLMs perform in Remote Sensing scenarios?\",\n        \"context\": \"However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inac- curate or fabricated information when presented with RS domain-specific queries.\",\n        \"answer\": \"General-domain VLMs perform poorly in Remote Sensing scenarios.\"\n    },\n    {\n        \"question\": \"What kind of information can result from using general-domain VLMs in Remote Sensing scenarios?\",\n        \"context\": \"However, such general-domain VLMs perform poorly for Remote Sensing (RS) scenarios, leading to inac- curate or fabricated information when presented with RS domain-specific queries.\",\n        \"answer\": \"Inaccurate or fabricated information.\"\n    },\n    {\n        \"question\": \"What introduces unique challenges for VLMs in Remote Sensing?\",\n        \"context\": \"Such a behavior emerges due to the unique challenges introduced by RS imagery. For exam- ple, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-\",\n        \"answer\": \"RS imagery introduces unique challenges for VLMs.\"\n    },\n    {\n        \"question\": \"What aspect of RS imagery makes it challenging for VLMs?\",\n        \"context\": \"Such a behavior emerges due to the unique challenges introduced by RS imagery. For exam- ple, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-\",\n        \"answer\": \"Handling high-resolution RS imagery with diverse scale changes across categories and many small objects.\"\n    },\n    {\n        \"question\": \"What example is given regarding the challenges faced by VLMs in RS?\",\n        \"context\": \"Such a behavior emerges due to the unique challenges introduced by RS imagery. For exam- ple, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-\",\n        \"answer\": \"An example given is handling high-resolution RS imagery with diverse scale changes across categories and many small objects.\"\n    },\n    {\n        \"question\": \"Why do VLMs struggle with small objects in RS imagery?\",\n        \"context\": \"Such a behavior emerges due to the unique challenges introduced by RS imagery. For exam- ple, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-\",\n        \"answer\": \"VLMs struggle because they need to handle diverse scale changes across categories and many small objects.\"\n    },\n    {\n        \"question\": \"What specific feature of RS imagery complicates its processing for VLMs?\",\n        \"context\": \"Such a behavior emerges due to the unique challenges introduced by RS imagery. For exam- ple, to handle high-resolution RS imagery with diverse scale changes across categories and many small objects, region-\",\n        \"answer\": \"High-resolution RS imagery with diverse scale changes and many small objects complicates processing for VLMs.\"\n    }\n]"}
{"uuid": "37163d64-7378-4168-9998-b89240a20c84", "raw_resp": "```json\n[\n    {\"question\": \"What is necessary alongside holistic scene interpretation?\", \"context\": \"level reasoning is necessary alongside holistic scene inter- pretation.\", \"answer\": \"Level reasoning\"},\n    {\"question\": \"What makes it hard for the models to align their behavior with user queries?\", \"context\": \"Furthermore, the lack of domain-specific mul- timodal instruction following data as well as strong back- bone models for RS make it hard for the models to align their behavior with user queries.\", \"answer\": \"The lack of domain-specific multimodal instruction following data and strong backbone models for Remote Sensing (RS)\"},\n    {\"question\": \"What is proposed to address the limitations mentioned?\", \"context\": \"To address these lim- itations, we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabil- ities with high-resolution RS images.\", \"answer\": \"GeoChat, the first versatile remote sensing VLM (Vision Language Model) that offers multitask conversational capabilities with high-resolution RS images\"},\n    {\"question\": \"What kind of conversational capabilities does GeoChat offer?\", \"context\": \"GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabil- ities with high-resolution RS images.\", \"answer\": \"Multitask conversational capabilities with high-resolution RS images\"},\n    {\"question\": \"What type of images does GeoChat work with?\", \"context\": \"GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabil- ities with high-resolution RS images.\", \"answer\": \"High-resolution RS (Remote Sensing) images\"},\n    {\"question\": \"What is GeoChat described as being the first of?\", \"context\": \"we propose GeoChat - the first versatile remote sensing VLM that offers multitask conversational capabil- ities with high-resolution RS images.\", \"answer\": \"The first versatile remote sensing VLM (Vision Language Model)\"},\n    {\"question\": \"What is the main challenge in aligning model behavior with user queries in remote sensing?\", \"context\": \"Furthermore, the lack of domain-specific mul- timodal instruction following data as well as strong back- bone models for RS make it hard for the models to align their behavior with user queries.\", \"answer\": \"The lack of domain-specific multimodal instruction following data and strong backbone models for Remote Sensing (RS)\"},\n    {\"question\": \"What is the purpose of GeoChat in remote sensing applications?\", \"context\": \"Specifically, GeoChat\", \"answer\": \"To provide multitask conversational capabilities with high-resolution RS images, addressing the limitations of current models\"}\n]\n```"}
{"uuid": "424985f5-1269-46d5-8038-0917fe4b4067", "raw_resp": "```json\n[\n    {\"question\": \"What types of queries can the model handle?\", \"context\": \"can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue.\", \"answer\": \"The model can handle image-level queries and region-specific dialogue inputs.\"},\n    {\"question\": \"How does the model refer to objects in its responses?\", \"context\": \"it can visually ground objects in its responses by re- referring to their spatial coordinates.\", \"answer\": \"The model refers to objects using their spatial coordinates.\"},\n    {\"question\": \"What is the purpose of generating a new RS multimodal instruction-following dataset?\", \"context\": \"To address the lack of domain-specific datasets, we generate a novel RS multimimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets.\", \"answer\": \"The purpose is to address the lack of domain-specific datasets.\"},\n    {\"question\": \"How was the new RS multimodal instruction-following dataset created?\", \"context\": \"we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets.\", \"answer\": \"The dataset was created by extending image-text pairs from existing diverse RS datasets.\"},\n    {\"question\": \"What is established as part of the work described in the text?\", \"context\": \"We establish a comprehensive benchmark for RS multitask conversations\", \"answer\": \"A comprehensive benchmark for RS multitask conversations is established.\"},\n    {\"question\": \"What kind of benchmark is established for RS multitask conversations?\", \"context\": \"We establish a comprehensive benchmark for RS multitask conversations\", \"answer\": \"The benchmark is comprehensive.\"},\n    {\"question\": \"What is extended to create the novel RS multimodal instruction-following dataset?\", \"context\": \"we generate a novel RS multimodal instruction-following dataset by extending image-text pairs from existing diverse RS datasets.\", \"answer\": \"Image-text pairs from existing diverse RS datasets are extended.\"},\n    {\"question\": \"What does the model extend to support region-specific dialogue?\", \"context\": \"can not only answer image-level queries but also accepts region inputs to hold region-specific dialogue.\", \"answer\": \"The model extends to accept region inputs.\"}\n]\n```"}
{"uuid": "8d02ff70-fd24-4093-89d1-307e84228ab3", "raw_resp": "```json\n[\n    {\"question\": \"What does GeoChat demonstrate?\", \"context\": \"GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conversations and referring detection.\", \"answer\": \"Robust zero-shot performance on various RS tasks including image and region captioning, visual question answering, scene classification, visually grounded conversations, and referring detection.\"},\n    {\"question\": \"Where is the code for GeoChat available?\", \"context\": \"Our code is available here.\", \"answer\": \"The code for GeoChat is available at the provided link.\"},\n    {\"question\": \"What domains are mentioned as sources of aligned image-text data?\", \"context\": \"In the natural image domain, the abundance of aligned image-text data sourced from web imagery or manual anno-\", \"answer\": \"The natural image domain, specifically web imagery and manual annotation.\"},\n    {\"question\": \"What types of tasks is GeoChat applied to?\", \"context\": \"GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conver-sations and referring detection.\", \"answer\": \"Image and region captioning, visual question answering, scene classification, visually grounded conversations, and referring detection.\"},\n    {\"question\": \"Who are the authors of this document?\", \"context\": \"*Equally contributing first authors.\", \"answer\": \"The authors are equally contributing first authors.\"},\n    {\"question\": \"What is one of the types of data used in the natural image domain?\", \"context\": \"In the natural image domain, the abundance of aligned image-text data sourced from web imagery or manual anno-\", \"answer\": \"Aligned image-text data sourced from web imagery.\"},\n    {\"question\": \"What is an example of a task that GeoChat performs?\", \"context\": \"GeoChat demonstrates robust zero-shot performance on various RS tasks, e.g., image and region captioning, visual question answering, scene classification, visually grounded conver-sations and referring detection.\", \"answer\": \"An example is image and region captioning.\"},\n    {\"question\": \"What kind of data is mentioned as being abundant in the natural image domain?\", \"context\": \"In the natural image domain, the abundance of aligned image-text data sourced from web imagery or manual anno-\", \"answer\": \"Aligned image-text data.\"}\n]\n```"}
{"uuid": "25ecb74a-918e-49ef-9502-04ae84911b10", "raw_resp": "```json\n[\n    {\"question\": \"What does the image depict?\", \"context\": \"The imagedepictsascenewithmultiplebuildings,houses,trees,andsmallvehicles.Thereare10tenniscourtsvisibleintheimage\", \"answer\": \"The image depicts a scene with multiple buildings, houses, trees, small vehicles, and 10 tennis courts visible in the image.\"},\n    {\"question\": \"How many tennis courts are visible in the image?\", \"context\": \"Howmanytenniscourtsarevisible?ASSISTANT:10tenniscourtsarepresentintheimage\", \"answer\": \"There are 10 tennis courts visible in the image.\"},\n    {\"question\": \"What object is present according to the Region Based Captioning?\", \"context\": \"Whatistheobjectpresenthere?ASSISTANT:Awhitesmallcar\", \"answer\": \"A white small car is present.\"},\n    {\"question\": \"What surrounds the tennis courts?\", \"context\": \"Canyoudescribethesurroundingsofthetenniscourts?ASSISTANT:Thetenniscourtsaresurroundedbymultiplebuildingsandhouses.Therearealsocarsparkednearby\", \"answer\": \"The tennis courts are surrounded by multiple buildings and houses. There are also cars parked nearby.\"},\n    {\"question\": \"Where is the tennis court referred to in the Referring Expression task located?\", \"context\": \"Tenniscourtatthebottomleft.ASSISTANT:Tenniscourtatthebottomleft\", \"answer\": \"The tennis court is located at the bottom left.\"},\n    {\"question\": \"Is the area depicted in the image rural or urban?\", \"context\": \"Isthisaruraloranurbanarea?ASSISTANT:Thisisanaerialviewofanurbanarea\", \"answer\": \"The area depicted is an urban area.\"},\n    {\"question\": \"What type of view is shown in the image?\", \"context\": \"Isthisaruraloranurbanarea?ASSISTANT:Thisisanaerialviewofanurbanarea\", \"answer\": \"The image shows an aerial view.\"},\n    {\"question\": \"What is the nature of the multi-turn conversation regarding the tennis courts?\", \"context\": \"Howmanytenniscourtsarethereintheimage?ASSISTANT:…………..USER:Canyoudescribethesurroundingsofthetenniscourts?ASSISTANT:Thetenniscourtsaresurroundedbymultiplebuildingsandhouses.Therearealsocarsparkednearby\", \"answer\": \"The multi-turn conversation involves asking about the number of tennis courts and describing their surroundings.\"}\n]\n```"}
{"uuid": "33890465-ae6c-42b4-85cf-b5d2e06849fc", "raw_resp": "[\n    {\n        \"question\": \"What can GeoChat accomplish for remote-sensing image comprehension?\",\n        \"context\": \"Figure 1. GeoChat can accomplish multiple tasks for remote-sensing (RS) image comprehension in a unified framework.\",\n        \"answer\": \"GeoChat can accomplish multiple tasks for remote-sensing image comprehension in a unified framework.\"\n    },\n    {\n        \"question\": \"What does GeoChat require to generate visually grounded responses?\",\n        \"context\": \"Given suitable task tokens and user queries, the model can generate visu- ally grounded responses (text with corresponding object locations - shown on top)\",\n        \"answer\": \"GeoChat requires suitable task tokens and user queries to generate visually grounded responses.\"\n    },\n    {\n        \"question\": \"What types of responses can GeoChat generate?\",\n        \"context\": \"the model can generate visu- ally grounded responses (text with corresponding object locations - shown on top), visual question answering on images and regions (top left and bottom right, respectively) as well as scene classifi- cation (top right)\",\n        \"answer\": \"GeoChat can generate visually grounded responses, visual question answering on images and regions, and scene classification.\"\n    },\n    {\n        \"question\": \"Where are the visual question answering results displayed?\",\n        \"context\": \"the model can generate visu- ally grounded responses (text with corresponding object locations - shown on top), visual question answering on images and regions (top left and bottom right, respectively) as well as scene classifi- cation (top right)\",\n        \"answer\": \"The visual question answering results are displayed on the top left and bottom right of the image.\"\n    },\n    {\n        \"question\": \"Where is the scene classification result located?\",\n        \"context\": \"the model can generate visu- ally grounded responses (text with corresponding object locations - shown on top), visual question answering on images and regions (top left and bottom right, respectively) as well as scene classifi- cation (top right)\",\n        \"answer\": \"The scene classification result is located at the top right of the image.\"\n    },\n    {\n        \"question\": \"What kind of conversations can GeoChat handle besides visual tasks?\",\n        \"context\": \"as well as scene classifi- cation (top right) and normal natural language conversations (bot-\",\n        \"answer\": \"Besides visual tasks, GeoChat can handle normal natural language conversations.\"\n    },\n    {\n        \"question\": \"How is the visually grounded response presented?\",\n        \"context\": \"the model can generate visu- ally grounded responses (text with corresponding object locations - shown on top)\",\n        \"answer\": \"The visually grounded response is presented as text with corresponding object locations shown on top.\"\n    },\n    {\n        \"question\": \"Does GeoChat work in a unified framework?\",\n        \"context\": \"Figure 1. GeoChat can accomplish multiple tasks for remote-sensing (RS) image comprehension in a unified framework.\",\n        \"answer\": \"Yes, GeoChat works in a unified framework.\"\n    }\n]"}
{"uuid": "30143afb-0d70-4766-87d8-f491ba2e7077", "raw_resp": "```json\n[\n    {\"question\": \"What makes this model the first RS VLM with grounding capability?\", \"context\": \"tom). This makes it the first RS VLM with grounding capability.\", \"answer\": \"The model has grounding capability, which is unique among RS VLMs.\"},\n    {\"question\": \"What do the tations facilitate?\", \"context\": \"tations facilitate effective self-supervised vision-language modeling, as demonstrated by multimodal GPT-4 [23] and open-source initiatives like LLaV A [19].\", \"answer\": \"Tations facilitate effective self-supervised vision-language modeling.\"},\n    {\"question\": \"Which models are mentioned as examples of effective self-supervised vision-language modeling?\", \"context\": \"tations facilitate effective self-supervised vision-language modeling, as demonstrated by multimodal GPT-4 [23] and open-source initiatives like LLaV A [19].\", \"answer\": \"Multimodal GPT-4 and LLaVA are mentioned as examples.\"},\n    {\"question\": \"How are these vision-language models developed?\", \"context\": \"These vision-language models (VLMs), developed through generative pretraining and instruction-tuning, exhibit robust zero-shot task completion across various user-oriented multimodal tasks.\", \"answer\": \"Vision-language models are developed through generative pretraining and instruction-tuning.\"},\n    {\"question\": \"What characteristic do these vision-language models exhibit?\", \"context\": \"These vision-language models (VLMs), developed through generative pretraining and instruction-tuning, exhibit robust zero-shot task completion across various user-oriented multimodal tasks.\", \"answer\": \"These models exhibit robust zero-shot task completion.\"},\n    {\"question\": \"What types of tasks do these models handle?\", \"context\": \"These vision-language models (VLMs), developed through generative pretraining and instruction-tuning, exhibit robust zero-shot task completion across various user-oriented multimodal tasks.\", \"answer\": \"These models handle various user-oriented multimodal tasks.\"},\n    {\"question\": \"What does the development of these models open the door to?\", \"context\": \"The resulting capabilities open the door to the de-\", \"answer\": \"The resulting capabilities open the door to further developments or applications, though the sentence is incomplete and does not specify exactly what it opens the door to.\"},\n    {\"question\": \"What is implied about the capabilities of these models?\", \"context\": \"The resulting capabilities open the door to the de-\", \"answer\": \"The capabilities imply potential for future advancements or applications, but the exact implication is cut off in the provided text.\"}\n]\n```"}
{"uuid": "a6e3e714-66ad-463c-a716-cbbeec985ebb", "raw_resp": "```json\n[\n    {\"question\": \"What type of application is mentioned for the development of versatile multimodal conversational assistants?\", \"context\": \"velopment of versatile multimodal conversational assistants with broad applications in real-world scenarios [12].\", \"answer\": \"Broad applications in real-world scenarios\"},\n    {\"question\": \"For what domain are the VLMs generally designed according to the text?\", \"context\": \"However, general-domain VLMs designed for natural images, exhibit poor performance when presented with re-\", \"answer\": \"Natural images\"},\n    {\"question\": \"How do general-domain VLMs perform when faced with something other than their intended domain?\", \"context\": \"However, general-domain VLMs designed for natural images, exhibit poor performance when presented with re-\", \"answer\": \"Exhibit poor performance\"},\n    {\"question\": \"Which publication is indicated as the source of this document?\", \"context\": \"This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\", \"answer\": \"CVPR paper\"},\n    {\"question\": \"Who provided the open access version of the document?\", \"context\": \"This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\", \"answer\": \"The Computer Vision Foundation\"},\n    {\"question\": \"What is the difference between the provided version and the accepted version of the paper?\", \"context\": \"Except for this watermark, it is identical to the accepted version;\", \"answer\": \"Except for the watermark, it is identical to the accepted version\"},\n    {\"question\": \"Where is the final published version of the proceedings available?\", \"context\": \"the final published version of the proceedings is available on IEEE Xplore.\", \"answer\": \"On IEEE Xplore\"},\n    {\"question\": \"What is the reference number associated with real-world scenarios in the text?\", \"context\": \"vement of versatile multimodal conversational assistants with broad applications in real-world scenarios [12].\", \"answer\": \"[12]\"}\n]\n```"}
{"uuid": "d71dac09-bdc9-4f37-8e0c-75306aff5dbf", "raw_resp": "```json\n[\n    {\"question\": \"What is the main reason for the performance disparity between remote sensing image-text pairings and publicly available web data?\", \"context\": \"motely sensed visual imagery. The performance disparity arises primarily from the distinct nature of content found in remote sensing image-text pairings compared to the publicly available web data.\", \"answer\": \"The performance disparity arises primarily from the distinct nature of content found in remote sensing image-text pairings compared to the publicly available web data.\"},\n    {\"question\": \"What issue can arise when general-domain VLMs are presented with spatial images from RS sensors?\", \"context\": \"As a result, general-domain VLMs can provide inaccurate information or hallucinate when pre-sented with spatial images from RS sensors.\", \"answer\": \"General-domain VLMs can provide inaccurate information or hallucinate when presented with spatial images from RS sensors.\"},\n    {\"question\": \"What kind of progress has been made in the field related to the topic discussed?\", \"question\": \"What kind of progress has been made in the field related to the topic discussed?\", \"context\": \"Although there has been significant progress in the field of remote sensing visual question answering (VQA) [39, 41], earlier meth-\", \"answer\": \"There has been significant progress in the field of remote sensing visual question answering (VQA).\"},\n    {\"question\": \"What is the term used for the specific area within remote sensing that deals with answering questions based on visual inputs?\", \"context\": \"Although there has been significant progress in the field of remote sensing visual question answering (VQA) [39, 41], earlier meth-\", \"answer\": \"The term used is 'remote sensing visual question answering' or 'RS VQA'.\"}\n]\n``` \n\nNote: There were only enough details in the provided text to generate four unique questions and their corresponding contexts and answers."}
{"uuid": "227dfebb-21ab-41c2-9236-45b06554f901", "raw_resp": "```json\n[\n    {\"question\": \"How have odds framed the task?\", \"context\": \"ods have framed the task as a classification problem. Here, the model chooses answers from predetermined responses found in the training data. It limits their applicability to open-ended answer generation and instruction-following.\", \"answer\": \"Odds have framed the task as a classification problem where the model selects answers from predetermined responses in the training data.\"},\n    {\"question\": \"What does the classification problem limit?\", \"context\": \"ods have framed the task as a classification problem. Here, the model chooses answers from predetermined responses found in the training data. It limits their applicability to open-ended answer generation and instruction-following.\", \"answer\": \"The classification problem limits the applicability to open-ended answer generation and instruction-following.\"},\n    {\"question\": \"What is introduced in this paper?\", \"context\": \"In this paper, we introduce GeoChat, an attempt to extend multimodal instruction-tuning to the remote sens- ing domain for training a multitask conversational assistant.\", \"answer\": \"GeoChat is introduced in this paper, aiming to extend multimodal instruction-tuning to the remote sensing domain.\"},\n    {\"question\": \"What is the purpose of introducing GeoChat?\", \"context\": \"In this paper, we introduce GeoChat, an attempt to extend multimodal instruction-tuning to the remote sens- ing domain for training a multitask conversational assistant.\", \"answer\": \"The purpose is to train a multitask conversational assistant using multimodal instruction-tuning in the remote sensing domain.\"},\n    {\"question\": \"Which domain does GeoChat aim to extend multimodal instruction-tuning to?\", \"context\": \"In this paper, we introduce GeoChat, an attempt to extend multimodal instruction-tuning to the remote sens- ing domain for training a multitask conversational assistant.\", \"answer\": \"GeoChat aims to extend multimodal instruction-tuning to the remote sensing domain.\"},\n    {\"question\": \"What is lacking in the remote-sensing domain according to the text?\", \"context\": \"However, remote-sensing domain lacks a multimodal\", \"answer\": \"The remote-sensing domain lacks a multimodal dataset or framework.\"},\n    {\"question\": \"What kind of assistant is GeoChat designed to be?\", \"context\": \"In this paper, we introduce GeoChat, an attempt to extend multimodal instruction-tuning to the remote sens- ing domain for training a multitask conversational assistant.\", \"answer\": \"GeoChat is designed to be a multitask conversational assistant.\"},\n    {\"question\": \"What challenge is mentioned regarding the remote sensing domain?\", \"context\": \"However, remote-sensing domain lacks a multimodal\", \"answer\": \"The remote sensing domain lacks a multimodal component or resource.\"}\n]\n```"}
{"uuid": "2a9bf5a3-1ce6-4fa5-9005-7fdbba289f9c", "raw_resp": "```json\n[\n    {\"question\": \"What is the inspiration behind GeoChat's instruction-tuning conversational dataset?\", \"context\": \"Inspired by re- cent work in instruction-tuning [14, 19, 29, 42], GeoChat uses Vicuna-v1.5 [7] and an automated pipeline to generate diverse remote sensing multimodal instruction-following data comprising of nearly 318kinstructions.\", \"answer\": \"GeoChat's instruction-tuning conversational dataset is inspired by recent work in instruction-tuning.\"},\n    {\"question\": \"Which version of Vicuna does GeoChat use?\", \"context\": \"Inspired by re- cent work in instruction-tuning [14, 19, 29, 42], GeoChat uses Vicuna-v1.5 [7] and an automated pipeline to generate diverse remote sensing multimodal instruction-following data comprising of nearly 318kinstructions.\", \"answer\": \"GeoChat uses Vicuna-v1.5.\"},\n    {\"question\": \"How many instructions are included in the dataset generated by GeoChat?\", \"context\": \"Inspired by re- cent work in instruction-tuning [14, 19, 29, 42], GeoChat uses Vicuna-v1.5 [7] and an automated pipeline to generate diverse remote sensing multimodal instruction-following data comprising of nearly 318kinstructions.\", \"answer\": \"The dataset generated by GeoChat comprises nearly 318k instructions.\"},\n    {\"question\": \"From which datasets does GeoChat create image-text pairs?\", \"context\": \"We create the image-text pairs from various existing remote sensing datasets developed for diverse tasks. These includes LR- BEN for VQA [20], NWPU-RESISC-45 for scene classifi-\", \"answer\": \"Image-text pairs are created from various existing remote sensing datasets including LR-BEN for VQA and NWPU-RESISC-45 for scene classification.\"},\n    {\"question\": \"What is one of the tasks that the existing remote sensing datasets are developed for?\", \"context\": \"We create the image-text pairs from various existing remote sensing datasets developed for diverse tasks. These includes LR- BEN for VQA [20], NWPU-RESISC-45 for scene classifi-\", \"answer\": \"One of the tasks is Visual Question Answering (VQA).\"},\n    {\"question\": \"What is the purpose of using an automated pipeline in GeoChat?\", \"context\": \"Inspired by re- cent work in instruction-tuning [14, 19, 29, 42], GeoChat uses Vicuna-v1.5 [7] and an automated pipeline to generate diverse remote sensing multimodal instruction-following data comprising of nearly 318kinstructions.\", \"answer\": \"The purpose is to generate diverse remote sensing multimodal instruction-following data.\"},\n    {\"question\": \"What kind of data does GeoChat aim to generate?\", \"context\": \"Inspired by re- cent work in instruction-tuning [14, 19, 29, 42], GeoChat uses Vicuna-v1.5 [7] and an automated pipeline to generate diverse remote sensing multimodal instruction-following data comprising of nearly 318kinstructions.\", \"answer\": \"GeoChat aims to generate diverse remote sensing multimodal instruction-following data.\"},\n    {\"question\": \"What is the name of the dataset used for Visual Question Answering?\", \"context\": \"We create the image-text pairs from various existing remote sensing datasets developed for diverse tasks. These includes LR- BEN for VQA [20], NWPU-RESISC-45 for scene classifi-\", \"answer\": \"The dataset used for Visual Question Answering is LR-BEN.\"}\n]\n```"}
{"uuid": "fb598345-793b-48e1-9800-48353185955b", "raw_resp": "```json\n[\n    {\"question\": \"What capability is considered crucial for GeoChat?\", \"context\": \"A crucial capability of GeoChat is the unification of mul-tiple image and region-level reasoning tasks for RS imagery within a single pipeline (see Fig. 1).\", \"answer\": \"The crucial capability of GeoChat is the unification of multiple image and region-level reasoning tasks for RS imagery within a single pipeline.\"},\n    {\"question\": \"How is the unification of tasks achieved in GeoChat?\", \"context\": \"We achieve this via distinct task tokens that help suitably direct the model’s re-sponses according to user requirements.\", \"answer\": \"The unification of tasks is achieved via distinct task tokens that help direct the model's responses according to user requirements.\"},\n    {\"question\": \"What additional feature does the model use in its inputs?\", \"context\": \"In addition, the model uses spatial location representations in its inputs to seamlessly reason about local regions and can also generate\", \"answer\": \"The model uses spatial location representations in its inputs to seamlessly reason about local regions.\"},\n    {\"question\": \"What is the purpose of using spatial location representations in the model's inputs?\", \"context\": \"In addition, the model uses spatial location representations in its inputs to seamlessly reason about local regions and can also generate\", \"answer\": \"The purpose is to seamlessly reason about local regions.\"},\n    {\"question\": \"What does GeoChat aim to do with respect to local regions?\", \"context\": \"In addition, the model uses spatial location representations in its inputs to seamlessly reason about local regions and can also generate\", \"answer\": \"GeoChat aims to seamlessly reason about local regions.\"},\n    {\"question\": \"What is mentioned as one of the capabilities of GeoChat in relation to generating outputs?\", \"context\": \"In addition, the model uses spatial location representations in its inputs to seamlessly reason about local regions and can also generate\", \"answer\": \"GeoChat can also generate, although the specific output is cut off in the provided text.\"},\n    {\"question\": \"What kind of tasks are unified for RS imagery within GeoChat's single pipeline?\", \"context\": \"A crucial capability of GeoChat is the unification of mul-tiple image and region-level reasoning tasks for RS imagery within a single pipeline (see Fig. 1).\", \"answer\": \"Multiple image and region-level reasoning tasks are unified for RS imagery within GeoChat's single pipeline.\"},\n    {\"question\": \"How does GeoChat assist users in their requirements?\", \"context\": \"We achieve this via distinct task tokens that help suitably direct the model’s re-sponses according to user requirements.\", \"answer\": \"GeoChat assists users by directing the model's responses according to user requirements through distinct task tokens.\"}\n]\n```"}
{"uuid": "bd6a14fa-35c8-4e2f-8fac-d1f47db3a818", "raw_resp": "[\n    {\n        \"question\": \"What does GeoChat enable in terms of object interaction?\",\n        \"context\": \"object locations in its responses to visually ground objects.\",\n        \"answer\": \"GeoChat enables the visual grounding of objects.\"\n    },\n    {\n        \"question\": \"What tasks are made possible by GeoChat?\",\n        \"context\": \"This enables a diverse set of tasks possible with GeoChat including referring expression detection, image/region captioning, scene classification, natural language conversations and VQA, besides visually grounded conversations.\",\n        \"answer\": \"GeoChat makes possible tasks such as referring expression detection, image/region captioning, scene classification, natural language conversations, VQA, and visually grounded conversations.\"\n    },\n    {\n        \"question\": \"What are the contributions of this work?\",\n        \"context\": \"In summary, this work has the following contributions: •RS multimodal instruction following dataset . We present a novel data generation pipeline, to leverage existing ob-\",\n        \"answer\": \"One of the contributions is the presentation of a RS multimodal instruction following dataset using a novel data generation pipeline.\"\n    },\n    {\n        \"question\": \"What type of dataset is presented in this work?\",\n        \"context\": \"In summary, this work has the following contributions: •RS multimodal instruction following dataset . We present a novel data generation pipeline, to leverage existing ob-\",\n        \"answer\": \"A RS multimodal instruction following dataset is presented.\"\n    },\n    {\n        \"question\": \"What is the purpose of the novel data generation pipeline introduced in this work?\",\n        \"context\": \"In summary, this work has the following contributions: •RS multimodal instruction following dataset . We present a novel data generation pipeline, to leverage existing ob-\",\n        \"answer\": \"The purpose is to leverage existing resources.\"\n    }\n]"}
{"uuid": "b067adf7-31fb-496f-924b-e2806dd788cb", "raw_resp": "```json\n[\n    {\"question\": \"Which dataset was used to create short descriptions of the images?\", \"context\": \"ject detection dataset [31] to create short descriptions of the images, followed by using Vicuna-v1.5 [7] to cre-ate conversations using the generated text alone.\", \"answer\": \"The object detection dataset\"},\n    {\"question\": \"What tool was used to create conversations using the generated text?\", \"context\": \"ject detection dataset [31] to create short descriptions of the images, followed by using Vicuna-v1.5 [7] to cre-ate conversations using the generated text alone.\", \"answer\": \"Vicuna-v1.5\"},\n    {\"question\": \"What additional abilities were added using their corresponding datasets?\", \"context\": \"Further, we add visual question-answering and scene classification abilities using their corresponding datasets [5, 20].\", \"answer\": \"Visual question-answering and scene classification abilities\"},\n    {\"question\": \"How many instruction pairs were created for the RS domain?\", \"context\": \"This results in a total of 318kinstruction pairs for RS domain.\", \"answer\": \"318k\"},\n    {\"question\": \"What is the name of the model that was fine-tuned to create the remote sensing-domain vision-language model?\", \"context\": \"Leveraging our dataset, we finetune LLaVA-1.5 [14] to create the remote sensing-domain vision-\", \"answer\": \"LLaVA-1.5\"},\n    {\"question\": \"What is the name of the project that leverages the created dataset?\", \"context\": \"•GeoChat . Leveraging our dataset, we finetune LLaVA-1.5 [14] to create the remote sensing-domain vision-\", \"answer\": \"GeoChat\"},\n    {\"question\": \"Which version of LLaVA was fine-tuned for the GeoChat project?\", \"context\": \"•GeoChat . Leveraging our dataset, we finetune LLaVA-1.5 [14] to create the remote sensing-domain vision-\", \"answer\": \"1.5\"},\n    {\"question\": \"For which domain was the vision-language model created?\", \"context\": \"•GeoChat . Leveraging our dataset, we finetune LLaVA-1.5 [14] to create the remote sensing-domain vision-\", \"answer\": \"Remote sensing-domain\"}\n]\n```"}
{"uuid": "9ffea684-e225-4c78-bfa6-80d144a39bb7", "raw_resp": "```json\n[\n    {\"question\": \"What is the name of the language model discussed in the text?\", \"context\": \"language model - GeoChat. Our LoRA [11] fine-tuning is efficient and avoids forgetting the necessary context em-\", \"answer\": \"The name of the language model is GeoChat.\"},\n    {\"question\": \"Which technique is used for fine-tuning GeoChat?\", \"context\": \"language model - GeoChat. Our LoRA [11] fine-tuning is efficient and avoids forgetting the necessary context em-\", \"answer\": \"LoRA fine-tuning is used for GeoChat.\"},\n    {\"question\": \"What does LoRA fine-tuning help to avoid during the training process?\", \"context\": \"Our LoRA [11] fine-tuning is efficient and avoids forgetting the necessary context em-\", \"answer\": \"LoRA fine-tuning helps to avoid forgetting the necessary context during the training process.\"},\n    {\"question\": \"Which component of the model is trained to align images into the word embedding space of the LLM?\", \"context\": \"whose MLP projection is trained to align images into the word embedding space of the LLM (Vicuna-v1.5 [7]).\", \"answer\": \"The MLP projection is trained to align images into the word embedding space of the LLM.\"},\n    {\"question\": \"Which version of the Vicuna model is referenced in the text?\", \"context\": \"whose MLP projection is trained to align images into the word embedding space of the LLM (Vicuna-v1.5 [7]).\", \"answer\": \"Vicuna-v1.5 is referenced in the text.\"},\n    {\"question\": \"What abilities of LLaVA does GeoChat retain?\", \"context\": \"This allowsGeoChat to retain the conversation and instruction following abilities of LLaV A and extend its domain-knowledge to remote sensing tasks.\", \"answer\": \"GeoChat retains the conversation and instruction following abilities of LLaVA.\"},\n    {\"question\": \"How does GeoChat extend its capabilities beyond those of LLaVA?\", \"context\": \"This allowsGeoChat to retain the conversation and instruction following abilities of LLaV A and extend its domain-knowledge to remote sensing tasks.\", \"answer\": \"GeoChat extends its domain-knowledge to remote sensing tasks.\"},\n    {\"question\": \"What aspect of remote sensing tasks is mentioned regarding GeoChat's capabilities?\", \"context\": \"This allowsGeoChat to retain the conversation and instruction following abilities of LLaV A and extend its domain-knowledge to remote sensing tasks.\", \"answer\": \"GeoChat's domain-knowledge is extended to remote sensing tasks.\"}\n]\n```"}
{"uuid": "d9dc2c4d-0a58-42ca-a2bd-874acc22baea", "raw_resp": "```json\n[\n    {\"question\": \"What is the purpose of the evaluation protocols setup by the authors?\", \"context\": \"sess the capability of existing VLMs on remote-sensing conversations. To this end, we setup evaluation proto- cols for conversation grounding in RS, as well as a setup a suite of tasks to allow comparisons with future efforts in this direction.\", \"answer\": \"The purpose is to assess the capability of existing VLMs on remote-sensing conversations and to establish a basis for comparison with future efforts.\"},\n    {\"question\": \"What type of evaluations are shown for different remote sensing tasks?\", \"context\": \"We show various supervised as well as zero-shot evaluations for different remote sensing tasks, including image captioning, visual question answering and scene classification to demonstrate the generalisabil- ity of GeoChat conversational VLM.\", \"answer\": \"Various supervised as well as zero-shot evaluations are shown for tasks such as image captioning, visual question answering, and scene classification.\"},\n    {\"question\": \"Which tasks are used to demonstrate the generalisability of GeoChat conversational VLM?\", \"context\": \"We show various supervised as well as zero-shot evaluations for different remote sensing tasks, including image captioning, visual question answering and scene classification to demonstrate the generalisabil- ity of GeoChat conversational VLM.\", \"answer\": \"Image captioning, visual question answering, and scene classification are used to demonstrate the generalisability of GeoChat conversational VLM.\"},\n    {\"question\": \"What is the name of the conversational VLM mentioned in the text?\", \"context\": \"We show various supervised as well as zero-shot evaluations for different remote sensing tasks, including image captioning, visual question answering and scene classification to demonstrate the generalisabil- ity of GeoChat conversational VLM.\", \"answer\": \"The conversational VLM mentioned is GeoChat.\"},\n    {\"question\": \"What is being evaluated through these protocols?\", \"context\": \"sess the capability of existing VLMs on remote-sensing conversations. To this end, we setup evaluation proto- cols for conversation grounding in RS, as well as a setup a suite of tasks to allow comparisons with future efforts in this direction.\", \"answer\": \"The capability of existing VLMs on remote-sensing conversations is being evaluated.\"},\n    {\"question\": \"What is the term 'RS' an abbreviation for in the text?\", \"context\": \"sess the capability of existing VLMs on remote-sensing conversations. To this end, we setup evaluation proto- cols for conversation grounding in RS, as well as a setup a suite of tasks to allow comparisons with future efforts in this direction.\", \"answer\": \"RS stands for Remote Sensing.\"},\n    {\"question\": \"What does the text suggest about the future of remote sensing research?\", \"context\": \"sess the capability of existing VLMs on remote-sensing conversations. To this end, we setup evaluation proto- cols for conversation grounding in RS, as well as a setup a suite of tasks to allow comparisons with future efforts in this direction.\", \"answer\": \"The text suggests that the setup of evaluation protocols and a suite of tasks will allow comparisons with future efforts in remote sensing research.\"},\n    {\"question\": \"How do the authors demonstrate the generalisability of their conversational VLM?\", \"context\": \"We show various supervised as well as zero-shot evaluations for different remote sensing tasks, including image captioning, visual question answering and scene classification to demonstrate the generalisabil- ity of GeoChat conversational VLM.\", \"answer\": \"The authors demonstrate the generalisability of their conversational VLM through various supervised and zero-shot evaluations for different remote sensing tasks.\"}\n]\n```"}
{"uuid": "cf678153-859c-4df6-8b0a-351d610126f6", "raw_resp": "```json\n[\n    {\"question\": \"What is the typical architecture of instruction-following Vision Language Models (VLMs)?\", \"context\": \"The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"The typical architecture consists of a pre-trained visual backbone to encode visual data, a large language model for interpreting user instructions and generating responses, and a vision-language cross-modal connector such as a linear projection layer or an MLP for fusing visual information.\"},\n    {\"question\": \"What components make up the architecture of instruction-following VLMs?\", \"context\": \"The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"The components are a pre-trained visual backbone, a large language model, and a vision-language cross-modal connector.\"},\n    {\"question\": \"What is the purpose of the pre-trained visual backbone in VLMs?\", \"context\": \"The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"The pre-trained visual backbone is used to encode visual data.\"},\n    {\"question\": \"Which component of the VLM architecture is responsible for interpreting user instructions?\", \"context\": \"The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"A large language model is responsible for interpreting user instructions and generating responses.\"},\n    {\"question\": \"What is the role of the vision-language cross-modal connector in VLMs?\", \"context\": \"The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"The vision-language cross-modal connector is used for fusing visual information.\"},\n    {\"question\": \"What types of vision-language cross-modal connectors are mentioned in the text?\", \"context\": \"The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"Linear projection layer and an MLP are mentioned as types of vision-language cross-modal connectors.\"},\n    {\"question\": \"What does VLM stand for in the context of this text?\", \"context\": \"The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"VLM stands for Vision Language Models.\"},\n    {\"question\": \"What is the main focus of the section titled 'Related Work'?\", \"context\": \"Large Vision-Language Models. The typical architecture of instruction-following Vision Language Models (VLMs) consists of utilising a pre-trained visual backbone[9] to en-code visual data, a large language model [7] for interpreting user instructions and generating responses, and a vision-language cross-modal connector, e.g., a linear projection layer [18, 42] or an MLP [17], for fusing visual information\", \"answer\": \"The main focus is on the typical architecture of instruction-following Vision Language Models (VLMs).\"}\n]\n```"}
{"uuid": "f4a6cb93-c6de-4973-8c54-53ab68d4b951", "raw_resp": "```json\n[\n    {\"question\": \"What are some examples of VLMs that show promise in language instruction following and visual reasoning ability?\", \"context\": \"with language models. The results achieved with VLMs show great promise; for example, LLaV A [18], Instruct-BLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable gains in language instruction following and visual reasoning ability for natural scenes.\", \"answer\": \"LLaV A, Instruct-BLIP, Otter, and MiniGPT-4\"},\n    {\"question\": \"What domains can VLMs be adapted to according to more recent studies?\", \"context\": \"More recent studies have shown that these models can be adapted to other domains such as videos [22], biomedical [14, 30] and remote sensing [12].\", \"answer\": \"Videos, biomedical, and remote sensing\"},\n    {\"question\": \"What does Remote Sensing VLMs refer to?\", \"context\": \"Remote Sensing VLMs. The application of generalized\", \"answer\": \"The application of generalized VLMs to the field of remote sensing\"},\n    {\"question\": \"What abilities of VLMs are particularly noteworthy according to the text?\", \"answer\": \"Language instruction following and visual reasoning ability for natural scenes\", \"context\": \"with language models. The results achieved with VLMs show great promise; for example, LLaV A [18], Instruct-BLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable gains in language instruction following and visual reasoning ability for natural scenes.\"},\n    {\"question\": \"Which domain has been specifically mentioned as an application of VLMs besides natural scenes?\", \"answer\": \"Remote sensing\", \"context\": \"More recent studies have shown that these models can be adapted to other domains such as videos [22], biomedical [14, 30] and remote sensing [12].\"},\n    {\"question\": \"What is one of the achievements of VLMs in language models?\", \"answer\": \"Remarkable gains in language instruction following and visual reasoning ability\", \"context\": \"with language models. The results achieved with VLMs show great promise; for example, LLaV A [18], Instruct-BLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable gains in language instruction following and visual reasoning ability for natural scenes.\"},\n    {\"question\": \"What is the implication of the application of VLMs to remote sensing?\", \"answer\": \"It suggests that VLMs can be used for analyzing remote sensing data\", \"context\": \"Remote Sensing VLMs. The application of generalized\"},\n    {\"question\": \"What kind of gains do LLaV A, Instruct-BLIP, Otter, and MiniGPT-4 exhibit?\", \"answer\": \"Remarkable gains in language instruction following and visual reasoning ability for natural scenes\", \"context\": \"with language models. The results achieved with VLMs show great promise; for example, LLaV A [18], Instruct-BLIP [8], Otter [13] and MiniGPT-4 [42] show remarkable gains in language instruction following and visual reasoning ability for natural scenes.\"}\n]\n```"}
{"uuid": "6f43958e-0330-4304-8473-93794cd37bc6", "raw_resp": "[\n    {\n        \"question\": \"What is currently sparse in remote sensing?\",\n        \"context\": \"VLMs in remote sensing is comparatively sparse.\",\n        \"answer\": \"VLMs (Vision-Language Models) in remote sensing\"\n    },\n    {\n        \"question\": \"What have the majority of research so far neglected?\",\n        \"context\": \"The majority of research so far has neglected the semantic understanding of the items and their relationships towards a deep visual comprehension.\",\n        \"answer\": \"The semantic understanding of the items and their relationships towards a deep visual comprehension\"\n    },\n    {\n        \"question\": \"What is beyond merely identifying the objects in an image?\",\n        \"context\": \"Beyond merely identifying the objects in an image, vision-language models are also capable of generating natural language descriptions of the image and inferring the connections between the objects.\",\n        \"answer\": \"Generating natural language descriptions of the image and inferring the connections between the objects\"\n    },\n    {\n        \"question\": \"What are vision-language models capable of doing?\",\n        \"context\": \"Beyond merely identifying the objects in an image, vision-language models are also capable of generating natural language descriptions of the image and inferring the connections between the objects.\",\n        \"answer\": \"Generating natural language descriptions of the image and inferring the connections between the objects\"\n    },\n    {\n        \"question\": \"Why are vision-language models more appropriate for certain tasks?\",\n        \"context\": \"This makes them more appropriate for tasks like text-based im-\",\n        \"answer\": \"Because they can generate natural language descriptions of the image and infer the connections between the objects\"\n    },\n    {\n        \"question\": \"What kind of tasks are vision-language models more appropriate for?\",\n        \"context\": \"This makes them more appropriate for tasks like text-based im-\",\n        \"answer\": \"Tasks like text-based image analysis\"\n    },\n    {\n        \"question\": \"What does VLM stand for?\",\n        \"context\": \"VLMs in remote sensing is comparatively sparse.\",\n        \"answer\": \"Vision-Language Models\"\n    },\n    {\n        \"question\": \"What aspect of images can VLMs infer besides generating natural language descriptions?\",\n        \"context\": \"Beyond merely identifying the objects in an image, vision-language models are also capable of generating natural language descriptions of the image and inferring the connections between the objects.\",\n        \"answer\": \"The connections between the objects\"\n    }\n]"}
{"uuid": "094e8a64-c881-4f68-a1b2-45f426f4d9b9", "raw_resp": "```json\n[\n    {\"question\": \"What are some tasks that vision language models have shown progress in for remote sensing?\", \"context\": \"age retrieval, captioning images, and answering visual ques-tions that call for both visual and linguistic knowledge. Al-though there has been progress in vision language models for remote sensing tasks, such as image captioning [43], zero-shot classification [16] and visual question answering [3, 39]\", \"answer\": \"Vision language models have shown progress in tasks such as image captioning, zero-shot classification, and visual question answering for remote sensing.\"},\n    {\"question\": \"What types of capabilities do the current vision language models lack?\", \"context\": \"these models can only perform a specific task they are trained for, lack conversational capability and do not possess generic semantic knowledge about the remote sens-\", \"answer\": \"Current vision language models lack conversational capability and do not possess generic semantic knowledge about remote sensing.\"},\n    {\"question\": \"What kind of knowledge is required for answering visual questions in remote sensing?\", \"context\": \"age retrieval, captioning images, and answering visual ques-tions that call for both visual and linguistic knowledge.\", \"answer\": \"Both visual and linguistic knowledge is required for answering visual questions in remote sensing.\"},\n    {\"question\": \"What is one of the limitations of current vision language models in remote sensing?\", \"context\": \"these models can only perform a specific task they are trained for, lack conversational capability and do not possess generic semantic knowledge about the remote sens-\", \"answer\": \"One limitation is that the models can only perform the specific task they are trained for.\"},\n    {\"question\": \"What is an example of a task where vision language models have shown progress in remote sensing?\", \"context\": \"age retrieval, captioning images, and answering visual ques-tions that call for both visual and linguistic knowledge. Al-though there has been progress in vision language models for remote sensing tasks, such as image captioning [43], zero-shot classification [16] and visual question answering [3, 39]\", \"answer\": \"An example is image captioning.\"},\n    {\"question\": \"Do the current vision language models for remote sensing possess generic semantic knowledge about remote sensing?\", \"context\": \"these models can only perform a specific task they are trained for, lack conversational capability and do not possess generic semantic knowledge about the remote sens-\", \"answer\": \"No, the current vision language models do not possess generic semantic knowledge about remote sensing.\"},\n    {\"question\": \"Can the current vision language models handle multiple tasks without retraining?\", \"context\": \"these models can only perform a specific task they are trained for, lack conversational capability and do not possess generic semantic knowledge about the remote sens-\", \"answer\": \"No, the current vision language models can only perform the specific task they are trained for and cannot handle multiple tasks without retraining.\"},\n    {\"question\": \"What does answering visual questions in remote sensing require besides linguistic knowledge?\", \"context\": \"age retrieval, captioning images, and answering visual ques-tions that call for both visual and linguistic knowledge.\", \"answer\": \"Answering visual questions in remote sensing requires visual knowledge in addition to linguistic knowledge.\"}\n]\n```"}
{"uuid": "d22efc72-1bb0-4ea2-932f-f35dbc2eaa31", "raw_resp": "```json\n[\n    {\"question\": \"What is the main gap in the remote sensing domain?\", \"context\": \"A major gap exists in the remote sensing do- main towards developing general-purpose models to solve all tasks together, while also maintaining conversation abil- ities.\", \"answer\": \"The main gap in the remote sensing domain is the lack of general-purpose models that can solve all tasks together while maintaining conversation abilities.\"},\n    {\"question\": \"What is RSGPT?\", \"context\": \"While RSGPT [12] is an initial effort that has shown good conversation ability along with solving multiple tasks,\", \"answer\": \"RSGPT is an initial effort that has shown good conversation ability along with solving multiple tasks.\"},\n    {\"question\": \"What limitation does RSGPT have?\", \"context\": \"it requires finetuning the model for each task separately, which makes it cumbersome and not generalizable.\", \"answer\": \"RSGPT requires finetuning the model for each task separately, making it cumbersome and not generalizable.\"},\n    {\"question\": \"What capability of RSGPT is highlighted?\", \"context\": \"While RSGPT [12] is an initial effort that has shown good conversation ability along with solving multiple tasks,\", \"answer\": \"RSGPT's highlighted capability is its good conversation ability along with solving multiple tasks.\"},\n    {\"question\": \"Why is RSGPT considered not fully generalizable?\", \"context\": \"it requires finetuning the model for each task separately, which makes it cumbersome and not generalizable.\", \"answer\": \"RSGPT is considered not fully generalizable because it requires separate finetuning for each task.\"},\n    {\"question\": \"What aspect of remote sensing models is emphasized as needing improvement?\", \"context\": \"A major gap exists in the remote sensing do- main towards developing general-purpose models to solve all tasks together, while also maintaining conversation abil- ities.\", \"answer\": \"The aspect emphasized as needing improvement is the development of general-purpose models that can solve all tasks together while maintaining conversation abilities.\"},\n    {\"question\": \"What does the text imply about the current state of remote sensing models?\", \"context\": \"A major gap exists in the remote sensing do- main towards developing general-purpose models to solve all tasks together, while also maintaining conversation abil- ities.\", \"answer\": \"The text implies that current remote sensing models lack the capability to solve all tasks together and maintain conversation abilities.\"},\n    {\"question\": \"What is the implication regarding the development of general-purpose models in remote sensing?\", \"context\": \"A major gap exists in the remote sensing do- main towards developing general-purpose models to solve all tasks together, while also maintaining conversation abil- ities.\", \"answer\": \"The implication is that there is a significant gap in the development of general-purpose models that can handle multiple tasks and maintain conversation abilities in the remote sensing domain.\"}\n]\n```"}
{"uuid": "e85ef725-1e7f-4182-8e79-d0ac232609a2", "raw_resp": "[\n    {\n        \"question\": \"What are the limitations of RSGPT mentioned in the text?\",\n        \"context\": \"ther, RSGPT cannot work for region-level reasoning or vi-sual grounding, which our work aims to address.\",\n        \"answer\": \"RSGPT cannot work for region-level reasoning or visual grounding.\"\n    },\n    {\n        \"question\": \"What is the main focus of GeoChat?\",\n        \"context\": \"3. GeoChat: Grounded Remote Sensing VLM Visually grounded conversations for remote sensing aim to generate textual responses interleaved with corresponding object locations.\",\n        \"answer\": \"GeoChat focuses on visually grounded conversations for remote sensing, generating textual responses with corresponding object locations.\"\n    },\n    {\n        \"question\": \"What kind of prompts can users provide to GeoChat?\",\n        \"context\": \"Further, a user can also provide visual prompts (e.g., a bounding box) besides natural language questions,\",\n        \"answer\": \"Users can provide visual prompts such as a bounding box and natural language questions.\"\n    },\n    {\n        \"question\": \"What should the model be able to do with the provided prompts?\",\n        \"context\": \"and the model should be able to answer questions about the specified Region of Interest (RoI).\",\n        \"answer\": \"The model should be able to answer questions about the specified Region of Interest (RoI).\"\n    },\n    {\n        \"question\": \"What is the capability that GeoChat aims to achieve?\",\n        \"context\": \"Such seamless\",\n        \"answer\": \"GeoChat aims to achieve seamless integration of visual prompts and natural language questions.\"\n    }\n]"}
{"uuid": "106df0eb-bfd9-4a85-98f4-06ea19411584", "raw_resp": "[\n    {\n        \"question\": \"What is the interplay between visual and language modalities dependent on?\",\n        \"context\": \"interplay between visual and language modalities necessi-\\ntates a deep comprehension of linguistic constructions that\\ndeno te particular objects or elements in a visual scene.\",\n        \"answer\": \"The interplay between visual and language modalities is dependent on a deep comprehension of linguistic constructions that denote particular objects or elements in a visual scene.\"\n    },\n    {\n        \"question\": \"What is GeoChat known as?\",\n        \"context\": \"As mentioned above, GeoChat is the first model capable\\nof holding visually grounded conversations about remotely\\nsensed images.\",\n        \"answer\": \"GeoChat is known as the first model capable of holding visually grounded conversations about remotely sensed images.\"\n    },\n    {\n        \"question\": \"What tasks can GeoChat address?\",\n        \"context\": \"By construction, GeoChat can address not only the challenging task of visually grounded conversa-\\ntions, but can also perform a spectrum of other spatial rea-\",\n        \"answer\": \"GeoChat can address the challenging task of visually grounded conversations and can also perform a spectrum of other spatial reasoning tasks.\"\n    },\n    {\n        \"question\": \"What kind of conversations can GeoChat hold?\",\n        \"context\": \"As mentioned above, GeoChat is the first model capable\\nof holding visually grounded conversations about remotely\\nsensed images.\",\n        \"answer\": \"GeoChat can hold visually grounded conversations about remotely sensed images.\"\n    },\n    {\n        \"question\": \"What does the capability of GeoChat imply about its understanding?\",\n        \"context\": \"By construction, GeoChat can address not only the challenging task of visually grounded conversa-\\ntions, but can also perform a spectrum of other spatial rea-\",\n        \"answer\": \"The capability of GeoChat implies that it has a deep understanding of visual scenes and can perform spatial reasoning tasks.\"\n    },\n    {\n        \"question\": \"What is necessary for a model to discuss elements within a visual scene?\",\n        \"context\": \"interplay between visual and language modalities necessi-\\ntates a deep comprehension of linguistic constructions that\\ndeno te particular objects or elements in a visual scene.\",\n        \"answer\": \"A deep comprehension of linguistic constructions that denote particular objects or elements in a visual scene is necessary for a model to discuss elements within a visual scene.\"\n    },\n    {\n        \"question\": \"What type of model is GeoChat?\",\n        \"context\": \"As mentioned above, GeoChat is the first model capable\\nof holding visually grounded conversations about remotely\\nsensed images.\",\n        \"answer\": \"GeoChat is a model designed for holding visually grounded conversations about remotely sensed images.\"\n    },\n    {\n        \"question\": \"What additional capabilities does GeoChat have besides visually grounded conversations?\",\n        \"context\": \"By construction, GeoChat can address not only the challenging task of visually grounded conversa-\\ntions, but can also perform a spectrum of other spatial rea-\",\n        \"answer\": \"Besides visually grounded conversations, GeoChat can also perform a spectrum of other spatial reasoning tasks.\"\n    }\n]"}
{"uuid": "637f112b-2a46-4628-99b8-3a74920123ee", "raw_resp": "```json\n[\n    {\"question\": \"What type of tasks can GeoChat perform?\", \"context\": \"soning tasks that span varying levels of granularity in vi-sual imagery understanding e.g., image/region captioning, referring object detection and image/region-level conversations about remotely sensed images.\", \"answer\": \"GeoChat can perform tasks such as image/region captioning, referring object detection, and image/region-level conversations about remotely sensed images.\"},\n    {\"question\": \"What does GeoChat process in Image-Level Conversation Tasks?\", \"question\": \"a) Image-Level Conversation Tasks. In this task, GeoChat processes an image xand a user text query qwith-out any specific spatial coordinates in its inputs or outputs.\", \"answer\": \"In Image-Level Conversation Tasks, GeoChat processes an image and a user text query without any specific spatial coordinates in its inputs or outputs.\"},\n    {\"question\": \"Does GeoChat require specific spatial coordinates for Image-Level Conversation Tasks?\", \"context\": \"a) Image-Level Conversation Tasks. In this task, GeoChat processes an image xand a user text query qwith-out any specific spatial coordinates in its inputs or outputs.\", \"answer\": \"No, GeoChat does not require specific spatial coordinates for Image-Level Conversation Tasks.\"},\n    {\"question\": \"What kind of inputs does GeoChat handle for Image-Level Conversation Tasks?\", \"context\": \"a) Image-Level Conversation Tasks. In this task, GeoChat processes an image xand a user text query qwith-out any specific spatial coordinates in its inputs or outputs.\", \"answer\": \"For Image-Level Conversation Tasks, GeoChat handles an image and a user text query as inputs.\"},\n    {\"question\": \"What is not included in the inputs or outputs of GeoChat for Image-Level Conversation Tasks?\", \"context\": \"a) Image-Level Conversation Tasks. In this task, GeoChat processes an image xand a user text query qwith-out any specific spatial coordinates in its inputs or outputs.\", \"answer\": \"Specific spatial coordinates are not included in the inputs or outputs of GeoChat for Image-Level Conversation Tasks.\"},\n    {\"question\": \"Can GeoChat handle region-level conversations about remotely sensed images?\", \"context\": \"soning tasks that span varying levels of granularity in vi-sual imagery understanding e.g., image/region captioning, referring object detection and image/region-level conversations about remotely sensed images.\", \"answer\": \"Yes, GeoChat can handle region-level conversations about remotely sensed images.\"},\n    {\"question\": \"What are the examples of tasks that GeoChat can handle at varying levels of granularity?\", \"context\": \"soning tasks that span varying levels of granularity in vi-sual imagery understanding e.g., image/region captioning, referring object detection and image/region-level conversations about remotely sensed images.\", \"answer\": \"Examples of tasks GeoChat can handle include image/region captioning, referring object detection, and image/region-level conversations about remotely sensed images.\"},\n    {\"question\": \"What is the purpose of outlining the tasks possible with GeoChat?\", \"context\": \"We formally outline the tasks possible with GeoChat below.\", \"answer\": \"The purpose is to formally outline the tasks possible with GeoChat.\"}\n]\n```"}
{"uuid": "01501c61-faf9-4056-aa0a-24f910e2c104", "raw_resp": "```json\n[\n    {\"question\": \"What is the primary goal mentioned for conversation-based tasks?\", \"context\": \"The goal is to perform conversation-based tasks at a holistic level with image-wide context, such as visual question an- swering (VQA), scene classification and image captioning.\", \"answer\": \"The primary goal is to perform conversation-based tasks at a holistic level with image-wide context, such as visual question answering (VQA), scene classification, and image captioning.\"},\n    {\"question\": \"What does 'b' refer to in the context of Region-Level Conversation Tasks?\", \"context\": \"b) Region-Level Conversation Tasks. This task in- volves providing spatial box locations bin the input to GeoChat besides xandq. Region locations bguide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level cap-\", \"answer\": \"'b' refers to providing spatial box locations in the input to GeoChat, guiding the model's attention to specific regions within the image.\"},\n    {\"question\": \"What additional information besides xandq is provided for Region-Level Conversation Tasks?\", \"context\": \"b) Region-Level Conversation Tasks. This task in- volves providing spatial box locations bin the input to GeoChat besides xandq. Region locations bguide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level cap-\", \"answer\": \"Spatial box locations are provided in addition to xandq.\"},\n    {\"question\": \"How does the model use the provided region locations?\", \"context\": \"b) Region-Level Conversation Tasks. This task in- volves providing spatial box locations bin the input to GeoChat besides xandq. Region locations bguide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level cap-\", \"answer\": \"Region locations guide the model's attention to specific regions within the image.\"},\n    {\"question\": \"What kind of tasks can be performed by focusing on specific regions within an image?\", \"context\": \"b) Region-Level Conversation Tasks. This task in- volves providing spatial box locations bin the input to GeoChat besides xandq. Region locations bguide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level cap-\", \"answer\": \"Tasks such as region-level captioning can be performed by focusing on specific regions within an image.\"},\n    {\"question\": \"What is the name of the model being discussed for region-level tasks?\", \"context\": \"b) Region-Level Conversation Tasks. This task in- volves providing spatial box locations bin the input to GeoChat besides xandq. Region locations bguide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level cap-\", \"answer\": \"The model being discussed for region-level tasks is GeoChat.\"},\n    {\"question\": \"What is the purpose of guiding the model's attention to specific regions?\", \"answer\": \"Region locations guide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level captioning.\", \"context\": \"b) Region-Level Conversation Tasks. This task in- volves providing spatial box locations bin the input to GeoChat besides xandq. Region locations bguide the model’s attention to specific regions within the image, so that the model can perform tasks such as region-level cap-\"},\n    {\"question\": \"Besides VQA, what other types of tasks are mentioned for holistic conversation-based tasks?\", \"context\": \"The goal is to perform conversation-based tasks at a holistic level with image-wide context, such as visual question an- swering (VQA), scene classification and image captioning.\", \"answer\": \"Scene classification and image captioning are also mentioned as types of tasks for holistic conversation-based tasks.\"}\n]\n```"}
{"uuid": "2ab3ee6d-aefa-43ec-8e0c-fa4a3a4ef40c", "raw_resp": "```json\n[\n    {\"question\": \"What are the special tokens used in GeoChat called?\", \"context\": \"special tokens, termed as task-specification tokens t, GeoChat can be guided to provide object locations at different granularities\", \"answer\": \"task-specification tokens t\"},\n    {\"question\": \"What can GeoChat be guided to provide at different granularities?\", \"answer\": \"object locations\"},\n    {\"question\": \"Which tasks does GeoChat assist with?\", \"context\": \"It helps in tasks including grounded image captioning/conversation, object grounding and referring expression detection.\", \"answer\": \"grounded image captioning/conversation, object grounding and referring expression detection\"},\n    {\"question\": \"What is the architecture that GeoChat follows?\", \"context\": \"GeoChat follows the architecture as of LLaV A-v1.5 [17]\", \"answer\": \"LLaVA-v1.5\"},\n    {\"question\": \"What capabilities does GeoChat maintain while providing object locations?\", \"answer\": \"conversation abilities\"},\n    {\"question\": \"What kind of conversation tasks can GeoChat perform?\", \"answer\": \"Grounded Conversation Tasks\"},\n    {\"question\": \"How is GeoChat guided to perform specific tasks?\", \"answer\": \"With the use of special tokens, termed as task-specification tokens t\"},\n    {\"question\": \"What version of the LLaVA architecture does GeoChat follow?\", \"answer\": \"LLaV A-v1.5\"}\n]\n```"}
{"uuid": "d329ad38-5f67-425c-9d12-82eadad757e9", "raw_resp": "```json\n[\n    {\"question\": \"What are the three core components mentioned in the text?\", \"context\": \"which consists of three core components, i) Global Im- age encoder, ii) an MLP adaptor (two linear layers) and iii) LLM.\", \"answer\": \"The three core components are the Global Image encoder, an MLP adaptor (two linear layers), and a LLM.\"},\n    {\"question\": \"What differentiates the described model from LLaVA?\", \"context\": \"Different to LLaV A, we add specific task prompt that indicates the type of task desired from the model i.e., grounding, image-level or region-level conversations.\", \"answer\": \"The described model adds specific task prompts for grounding, image-level, or region-level conversations, which is different from LLaVA.\"},\n    {\"question\": \"What kind of task prompts are added to the model?\", \"context\": \"we add specific task prompt that indicates the type of task desired from the model i.e., grounding, image-level or region-level conversations.\", \"answer\": \"Task prompts include grounding, image-level, and region-level conversations.\"},\n    {\"question\": \"How does the model handle spatial positions?\", \"context\": \"Additionally, we allow spatial positions within both inputs and outputs, enabling visual prompts as inputs and grounded objects in GeoChat outputs.\", \"answer\": \"The model allows spatial positions within both inputs and outputs, enabling visual prompts as inputs and grounded objects in GeoChat outputs.\"},\n    {\"question\": \"What is enabled by allowing spatial positions within inputs and outputs?\", \"context\": \"Additionally, we allow spatial positions within both inputs and outputs, enabling visual prompts as inputs and grounded objects in GeoChat outputs.\", \"answer\": \"Allowing spatial positions within inputs and outputs enables visual prompts as inputs and grounded objects in GeoChat outputs.\"},\n    {\"question\": \"What are the types of tasks that can be prompted in the model?\", \"context\": \"we add specific task prompt that indicates the type of task desired from the model i.e., grounding, image-level or region-level conversations.\", \"answer\": \"The types of tasks that can be prompted are grounding, image-level, and region-level conversations.\"},\n    {\"question\": \"What is notable about the original LLaVA according to the text?\", \"context\": \"Notably, the original LLaV A\", \"answer\": \"The text ends abruptly but suggests that there is something notable about the original LLaVA, though the detail is not provided.\"},\n    {\"question\": \"What component of the model includes two linear layers?\", \"context\": \"i) Global Im- age encoder, ii) an MLP adaptor (two linear layers) and iii) LLM.\", \"answer\": \"The MLP adaptor includes two linear layers.\"}\n]\n```"}
{"uuid": "dbeac12f-0a8e-40fe-84a1-d93ab3d8464b", "raw_resp": "```json\n[\n    {\"question\": \"What is the size response format for the model that cannot perform object grounding?\", \"context\": \"model cannot perform object grounding or accept regionData Size Response formatting prompts\", \"answer\": \"Size Response formatting prompts\"},\n    {\"question\": \"What is the description prompt with a limit of 30k tokens?\", \"context\": \"Detailed Description 30k Describe the image in detail.\", \"answer\": \"Describe the image in detail.\"},\n    {\"question\": \"What is the token limit for multi-round conversation?\", \"context\": \"Multi-Round Conversation 65k -\", \"answer\": \"65k\"},\n    {\"question\": \"What task has a token limit of 10k?\", \"context\": \"Complex Questions 10k -\", \"answer\": \"Complex Questions\"},\n    {\"question\": \"Which dataset requires answering the question using a single word or phrase?\", \"context\": \"RSVQA-LRBEN[20] 56k Answer the question using a single word or phrase.\", \"answer\": \"RSVQA-LRBEN[20]\"},\n    {\"question\": \"How many instances does the NWPU-RESISC-45 dataset contain?\", \"context\": \"NWPU-RESISC-45[5] 31.5k\", \"answer\": \"31.5k\"},\n    {\"question\": \"How many instances are there in the Floodnet dataset?\", \"context\": \"Floodnet[25] 4k\", \"answer\": \"4k\"},\n    {\"question\": \"What is the description prompt with a limit of 25k tokens and includes '[grounding]'?\", \"context\": \"Grounding Description 25k [grounding] Describe the image in detail.\", \"answer\": \"Describe the image in detail.\"}\n]\n```"}
{"uuid": "a82c1eab-01fd-422b-abcc-fc5b1215153d", "raw_resp": "```json\n[\n    {\"question\": \"What is the name of the dataset used to train GeoChat?\", \"context\": \"Table 1. Instruction following data used to train GeoChat. Instruc-tion types and format are shown. We use a 308k set for training and a separate 10k instruction-set for testing.\", \"answer\": \"GeoChat is trained using a 308k set and tested with a separate 10k instruction-set.\"},\n    {\"question\": \"How many instructions are used for training GeoChat?\", \"context\": \"Table 1. Instruction following data used to train GeoChat. Instruc-tion types and format are shown. We use a 308k set for training and a separate 10k instruction-set for testing.\", \"answer\": \"A 308k set of instructions is used for training GeoChat.\"},\n    {\"question\": \"What is the size of the instruction-set used for testing GeoChat?\", \"context\": \"Table 1. Instruction following data used to train GeoChat. Instruc-tion types and format are shown. We use a 308k set for training and a separate 10k instruction-set for testing.\", \"answer\": \"The test set consists of a separate 10k instruction-set.\"},\n    {\"question\": \"What capability does GeoChat add compared to the original LLaVA?\", \"question\": \"Further, the original LLaV A can not reason about remote sensing images which is enabled via our domain-specific dataset.\", \"answer\": \"GeoChat enables reasoning about remote sensing images, a capability not present in the original LLaVA.\"},\n    {\"question\": \"What is the unique feature of GeoChat according to the text?\", \"context\": \"Task Token: The unique quality of GeoChat is its abil-\", \"answer\": \"The unique quality of GeoChat is its ability, though the specific ability is cut off in the provided text.\"},\n    {\"question\": \"What table describes the instruction following data used to train GeoChat?\", \"context\": \"Table 1. Instruction following data used to train GeoChat. Instruc-tion types and format are shown. We use a 308k set for training and a separate 10k instruction-set for testing.\", \"answer\": \"Table 1 describes the instruction following data used to train GeoChat.\"},\n    {\"question\": \"What aspect of the architecture is described as having a unique quality?\", \"context\": \"Task Token: The unique quality of GeoChat is its abil-\", \"answer\": \"The unique quality described is associated with the Task Token aspect of the architecture.\"},\n    {\"question\": \"How does GeoChat differ from the original LLaVA in terms of reasoning capabilities?\", \"context\": \"Further, the original LLaV A can not reason about remote sensing images which is enabled via our domain-specific dataset.\", \"answer\": \"GeoChat differs by enabling reasoning about remote sensing images, a capability that the original LLaVA lacks.\"}\n]\n```"}
{"uuid": "054fbe4c-1588-4a11-bea9-ec7edfe70ded", "raw_resp": "```json\n[\n    {\"question\": \"What is the purpose of assigning a unique task identification in the approach discussed?\", \"context\": \"To eliminate uncertainty among tasks, our approach assigns a unique task identification to each one.\", \"answer\": \"The purpose is to eliminate uncertainty among tasks.\"},\n    {\"question\": \"How many distinct task identities does the approach suggest?\", \"context\": \"We suggest three distinct task identities, t∈{grounding, identify, refer }, each for grounded conversations, region captioning and referring expression comprehension.\", \"answer\": \"The approach suggests three distinct task identities.\"},\n    {\"question\": \"What are the distinct task identities suggested by the approach?\", \"context\": \"We suggest three distinct task identities, t∈{grounding, identify, refer }, each for grounded conversations, region captioning and referring expression comprehension.\", \"answer\": \"The task identities suggested are grounding, identify, and refer.\"},\n    {\"question\": \"For which tasks are the identities 'grounding', 'identify', and 'refer' assigned?\", \"context\": \"We suggest three distinct task identities, t∈{grounding, identify, refer }, each for grounded conversations, region captioning and referring expression comprehension.\", \"answer\": \"'Grounding' is for grounded conversations, 'identify' is for region captioning, and 'refer' is for referring expression comprehension.\"},\n    {\"question\": \"How does the approach handle visual question answering and scene classification?\", \"context\": \"As for the case of visual question answering and scene classification, we directly ask the model to out-\", \"answer\": \"For visual question answering and scene classification, the model is directly asked to output the result.\"},\n    {\"question\": \"What types of remote sensing visual interpretation tasks are mentioned in the text?\", \"context\": \"We suggest three distinct task identities, t∈{grounding, identify, refer }, each for grounded conversations, region captioning and referring expression comprehension. As for the case of visual question answering and scene classification,\", \"answer\": \"The types of tasks mentioned are grounded conversations, region captioning, referring expression comprehension, visual question answering, and scene classification.\"},\n    {\"question\": \"What does the 'grounding' identity correspond to in terms of tasks?\", \"context\": \"We suggest three distinct task identities, t∈{grounding, identify, refer }, each for grounded conversations, region captioning and referring expression comprehension.\", \"answer\": \"'Grounding' corresponds to grounded conversations.\"},\n    {\"question\": \"Which task is associated with the 'refer' identity?\", \"context\": \"We suggest three distinct task identities, t∈{grounding, identify, refer }, each for grounded conversations, region captioning and referring expression comprehension.\", \"answer\": \"The 'refer' identity is associated with referring expression comprehension.\"}\n]\n```"}
{"uuid": "042843e2-cc9b-4679-b2f8-4efd552fbd0d", "raw_resp": "```json\n[\n    {\"question\": \"What is required for the answer format according to the text?\", \"context\": \"put the answer in a single word or phrase, as shown in Ta-\", \"answer\": \"a single word or phrase\"},\n    {\"question\": \"Does the approach use task identification tokens for vision-irrelevant commands?\", \"context\": \"Our approach does not employ any task identifica-tion tokens for vision-irrelevant commands.\", \"answer\": \"No\"},\n    {\"question\": \"How is the unified approach supported?\", \"context\": \"This unified approach is supported by a modular design that efficiently integrates spatial data, giving the model flexibility in its rea-soning about visual content.\", \"answer\": \"By a modular design that efficiently integrates spatial data\"},\n    {\"question\": \"What kind of flexibility does the modular design provide to the model?\", \"context\": \"This unified approach is supported by a modular design that efficiently integrates spatial data, giving the model flexibility in its rea-soning about visual content.\", \"answer\": \"Flexibility in reasoning about visual content\"},\n    {\"question\": \"What must the model precisely identify?\", \"context\": \"Spatial Location Representation. Our model must pre-cisely identify the spatial position of the referenced items\", \"answer\": \"The spatial position of the referenced items\"},\n    {\"question\": \"What is the section title that indicates what the model must do?\", \"context\": \"Spatial Location Representation. Our model must pre-cisely identify the spatial position of the referenced items\", \"answer\": \"Spatial Location Representation\"},\n    {\"question\": \"What does the model need to do with the referenced items?\", \"context\": \"Spatial Location Representation. Our model must pre-cisely identify the spatial position of the referenced items\", \"answer\": \"Precisely identify their spatial position\"},\n    {\"question\": \"How does the model handle the referenced items spatially?\", \"context\": \"Spatial Location Representation. Our model must pre-cisely identify the spatial position of the referenced items\", \"answer\": \"Identify their spatial position\"}\n]\n```"}
{"uuid": "cb067fb7-ef34-4ea3-93b8-81f9947b1dd7", "raw_resp": "```json\n[\n    {\"question\": \"What tasks does the represented box location help with?\", \"context\": \"for tasks such as grounded conversations, referring expres-sion generation, and comprehension.\", \"answer\": \"Grounded conversations, referring expression generation, and comprehension.\"},\n    {\"question\": \"How is the box location represented in a textual format?\", \"context\": \"we repre-sent the box locations in a textual format to express the ge-graphical position: b={bxleft, bytop, bxright, bybottom|θ}.\", \"answer\": \"b={bxleft, bytop, bxright, bybottom|θ}\"},\n    {\"question\": \"What do bxleft and bytop denote in the box representation?\", \"context\": \"Here, bxleft, bytopdenote the top left corner point of box\", \"answer\": \"The top left corner point of the box\"},\n    {\"question\": \"What do bxright and bybottom represent in the box representation?\", \"context\": \"while the bxright, bybottom represent the bottom right corner coordinates.\", \"answer\": \"The bottom right corner coordinates\"},\n    {\"question\": \"What does the angle θ represent in the box representation?\", \"context\": \"The angle θrepresents the angle of rotation for the bounding box, from the lower edge.\", \"answer\": \"The angle of rotation for the bounding box, from the lower edge\"},\n    {\"question\": \"What elements are included in the representation of the geographical position of the box?\", \"context\": \"Here, bxleft, bytopdenote the top left corner point of box while the bxright, bybottom represent the bottom right corner coordinates. The angle θrepresents the angle of rotation for the bounding box, from the lower edge.\", \"answer\": \"bxleft, bytop, bxright, bybottom, and θ\"},\n    {\"question\": \"Which element indicates the rotation of the bounding box?\", \"context\": \"The angle θrepresents the angle of rotation for the bounding box, from the lower edge.\", \"answer\": \"θ\"},\n    {\"question\": \"What numerical values are mentioned in the text?\", \"context\": \"Numerical values\", \"answer\": \"No specific numerical values are provided; the term 'numerical values' is mentioned but not elaborated upon.\"}\n]\n```"}
{"uuid": "faa82d57-2e4c-4ade-8839-63cd3c9bb14f", "raw_resp": "```json\n[\n    {\"question\": \"What interval are the x and y coordinates normalised within?\", \"context\": \"normalised within the interval [0, 100] are used to represent the x and y coordinates.\", \"answer\": \"The x and y coordinates are normalised within the interval [0, 100].\"},\n    {\"question\": \"What format is used to interact with the model via its inputs and outputs?\", \"context\": \"Region locations in this format are used to interact with the model via its inputs and outputs.\", \"answer\": \"Region locations in the specified format are used to interact with the model via its inputs and outputs.\"},\n    {\"question\": \"Which pre-trained vision backbone does GeoChat adapt?\", \"context\": \"Visual Backbone. GeoChat adapts the pretrained vision backbone of CLIP-ViT(L-14) [28], which has an input res-olution of 336 ×336.\", \"answer\": \"GeoChat adapts the pretrained vision backbone of CLIP-ViT(L-14).\"},\n    {\"question\": \"What is the input resolution of the adapted vision backbone?\", \"context\": \"Visual Backbone. GeoChat adapts the pretrained vision backbone of CLIP-ViT(L-14) [28], which has an input res-olution of 336 ×336.\", \"answer\": \"The input resolution of the adapted vision backbone is 336 × 336.\"},\n    {\"question\": \"How many patches per image result from using the specified input resolution?\", \"context\": \"This results in effectively 576 patches per image.\", \"answer\": \"Effectively 576 patches per image result from using the specified input resolution.\"},\n    {\"question\": \"Is the resolution of 336 × 336 considered sufficient to understand details in remote sensing imagery?\", \"context\": \"Since this resolution is not sufficient to un-derstand details presented in remote sensing imagery (e.g.,\", \"answer\": \"No, the resolution of 336 × 336 is not considered sufficient to understand details presented in remote sensing imagery.\"},\n    {\"question\": \"What kind of details might be insufficiently understood with the given resolution?\", \"answer\": \"Details presented in remote sensing imagery might be insufficiently understood with the given resolution.\", \"context\": \"Since this resolution is not sufficient to un-derstand details presented in remote sensing imagery (e.g.,\"},\n    {\"question\": \"What is the implication of the insufficiency of the resolution for remote sensing imagery?\", \"answer\": \"The implication is that finer details in remote sensing imagery may not be adequately captured or understood.\", \"context\": \"Since this resolution is not sufficient to un-derstand details presented in remote sensing imagery (e.g.,\"}\n]\n```"}
{"uuid": "7a2d75df-ef26-44aa-929a-249cd555b9d1", "raw_resp": "```json\n[\n    {\"question\": \"What is being interpolated in the transformer-based CLIP model?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"The positional encoding is being interpolated in the transformer-based CLIP model.\"},\n    {\"question\": \"With what input image size is the positional encoding scaled?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"The positional encoding is scaled with input image sizes of 504 ×504.\"},\n    {\"question\": \"Which model's positional encoding is being scaled with the input image size?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"The transformer-based CLIP model's positional encoding is being scaled with the input image size.\"},\n    {\"question\": \"What is the purpose of interpolating the positional encoding in the CLIP model?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"The purpose is to handle small objects and object details by scaling with input image sizes of 504 ×504.\"},\n    {\"question\": \"What aspect of the CLIP model is being adjusted to handle small objects and details?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"The positional encoding is being adjusted to handle small objects and details.\"},\n    {\"question\": \"What does CLIP stand for in the context of the transformer-based model mentioned?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"CLIP stands for Contrastive Language-Image Pre-training.\"},\n    {\"question\": \"What reference number is associated with the CLIP model mentioned?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"The reference number associated with the CLIP model is [28].\"},\n    {\"question\": \"What specific feature of the CLIP model is being modified to improve its performance with different image sizes?\", \"context\": \"small objects and object details), we interpolate the positional encoding in the transformer-based CLIP [28] model to scale with input image sizes of 504 ×504.\", \"answer\": \"The positional encoding is being modified to improve the model's performance with different image sizes.\"}\n]\n```"}
{"uuid": "52d0e8b2-9a45-40ac-aaa7-b845fe0decfd", "raw_resp": "```json\n[\n    {\"question\": \"What is mentioned as present in the area?\", \"context\": \"Football ground on the right side, surrounded by trees.\", \"answer\": \"A football ground surrounded by trees.\"},\n    {\"question\": \"What task-specific prompts are indicated in the document?\", \"context\": \"<im_end> [task specific prompts] <im_start>[USER]: [identify] What is present in this area?[USER]:[refer] Football ground on the right side, surrounded by trees.\", \"answer\": \"[identify] What is present in this area? [refer] Football ground on the right side, surrounded by trees.\"},\n    {\"question\": \"Which model is mentioned in the document?\", \"context\": \"Large Language Model\", \"answer\": \"Large Language Model\"},\n    {\"question\": \"What components are listed under the Large Language Model?\", \"context\": \"🔥MLP Adaptor \\\\n❄CLIP-ViT\\\\n❄\", \"answer\": \"🔥MLP Adaptor and ❄CLIP-ViT\"},\n    {\"question\": \"What kind of expressions does 'Referring Expression' refer to?\", \"context\": \"Referring Expression\\\\nImage Description\\\\nRegion  captioning\\\\nVisual QA\", \"answer\": \"It refers to Image Description, Region captioning, and Visual QA\"},\n    {\"question\": \"What is listed after Referring Expression?\", \"context\": \"Referring Expression\\\\nImage Description\\\\nRegion  captioning\\\\nVisual QA\", \"answer\": \"Image Description, Region captioning, and Visual QA\"},\n    {\"question\": \"What is an example of a task that could use the referring expression?\", \"context\": \"[USER]: [identify] What is present in this area?[USER]:[refer] Football ground on the right side, surrounded by trees.\", \"answer\": \"Identifying what is present in the area using a description of the scene\"},\n    {\"question\": \"What is the purpose of the region captioning?\", \"context\": \"Referring Expression\\\\nImage Description\\\\nRegion  captioning\\\\nVisual QA\", \"answer\": \"To describe the contents of a specific region within an image\"}\n]\n```"}
{"uuid": "e5e75933-606c-46ad-88e1-8f30cbf6ab17", "raw_resp": "```json\n[\n    {\"question\": \"What is the main focus of the provided text?\", \"context\": \"Region  captioning\\nVisual QA\", \"answer\": \"The main focus of the provided text is region captioning and visual question answering.\"},\n    {\"question\": \"What kind of view does the image provide?\", \"context\": \"The image is an aerial view of a sports complex, with two football grounds at the center and one basketball ground at the bottom.\", \"answer\": \"The image provides an aerial view of a sports complex.\"},\n    {\"question\": \"What is located at the center of the image?\", \"context\": \"The image is an aerial view of a sports complex, with two football grounds at the center and one basketball ground at the bottom.\", \"answer\": \"Two football grounds are located at the center of the image.\"},\n    {\"question\": \"What surrounds the football grounds?\", \"context\": \"The football grounds are surrounded by trees…….\", \"answer\": \"Trees surround the football grounds.\"},\n    {\"question\": \"Where is the basketball ground positioned in the image?\", \"context\": \"A basketball field at the bottom of the image.\", \"answer\": \"The basketball ground is positioned at the bottom of the image.\"},\n    {\"question\": \"How many football grounds are present in the image?\", \"context\": \"[USER]: How many football grounds and basketball courts are present in the image?\\nThere are two football grounds and one basketball ground present.\", \"answer\": \"There are two football grounds present in the image.\"},\n    {\"question\": \"How many basketball courts are present in the image?\", \"context\": \"[USER]: How many football grounds and basketball courts are present in the image?\\nThere are two football grounds and one basketball ground present.\", \"answer\": \"There is one basketball court present in the image.\"},\n    {\"question\": \"What is the type of query being demonstrated in the text?\", \"context\": \"[USER]: Can you please describe the image in detail?\\nThe image is an aerial view of a sports complex, with two football grounds at the center and one basketball ground at the bottom. The football grounds are surrounded by trees…….\", \"answer\": \"The type of query being demonstrated is a request for a detailed description of the image.\"}\n]\n```"}
{"uuid": "94b6816d-3791-43e5-a3e3-6a3068a63c8c", "raw_resp": "```json\n[\n    {\"question\": \"What is the chat described in the System Message between?\", \"context\": \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to thehuman's questions.\", \"answer\": \"A chat between a curious human and an artificial intelligence assistant.\"},\n    {\"question\": \"What is the name given to the model described?\", \"context\": \"Figure 2. An overview of GeoChat - the first grounded large vision-language model for remote sensing.\", \"answer\": \"GeoChat\"},\n    {\"question\": \"What is GeoChat described as being?\", \"context\": \"Figure 2. An overview of GeoChat - the first grounded large vision-language model for remote sensing.\", \"answer\": \"the first grounded large vision-language model for remote sensing\"},\n    {\"question\": \"What is used first in the process of GeoChat?\", \"context\": \"Given an image input together with a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\", \"answer\": \"a visual backbone\"},\n    {\"question\": \"What does the visual backbone do in GeoChat?\", \"context\": \"Given an image input together with a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\", \"answer\": \"encode patch-level tokens at a higher resolution via interpolating positional encodings\"},\n    {\"question\": \"What kind of encodings are interpolated by the visual backbone in GeoChat?\", \"context\": \"Given an image input together with a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\", \"answer\": \"positional encodings\"},\n    {\"question\": \"What is processed along with the image input in GeoChat?\", \"context\": \"Given an image input together with a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\", \"answer\": \"a user query\"},\n    {\"question\": \"How are the patch-level tokens encoded in GeoChat?\", \"context\": \"Given an image input together with a user query, a visual backbone is first used to encode patch-level tokens at a higher resolution via interpolating positional encodings.\", \"answer\": \"at a higher resolution via interpolating positional encodings\"}\n]\n```"}
{"uuid": "b4e28466-3582-4ae6-b986-09e7589604da", "raw_resp": "```json\n[\n    {\"question\": \"What is used to adapt vision-tokens to language space?\", \"context\": \"A multi-layer perceptron (MLP) is used to adapt vision-tokens to language space suitable for input to a Large Language Model (Vicuna 1.5).\", \"answer\": \"A multi-layer perceptron (MLP)\"},\n    {\"question\": \"What can be input to the model besides visual inputs?\", \"context\": \"Besides visual inputs, region locations can also be input to the model together with task-specific prompts that specify the desired task required by the user.\", \"answer\": \"Region locations\"},\n    {\"question\": \"What can be specified by task-specific prompts?\", \"context\": \"Besides visual inputs, region locations can also be input to the model together with task-specific prompts that specify the desired task required by the user.\", \"answer\": \"The desired task required by the user\"},\n    {\"question\": \"How can the LLM generate responses?\", \"context\": \"Given this context, the LLM can generate natural language responses interleaved with corresponding object locations.\", \"answer\": \"Generate natural language responses interleaved with corresponding object locations\"},\n    {\"question\": \"What is the name of the Large Language Model mentioned?\", \"context\": \"A multi-layer perceptron (MLP) is used to adapt vision-tokens to language space suitable for input to a Large Language Model (Vicuna 1.5).\", \"answer\": \"Vicuna 1.5\"},\n    {\"question\": \"What information can be interleaved with the natural language responses?\", \"context\": \"Given this context, the LLM can generate natural language responses interleaved with corresponding object locations.\", \"answer\": \"Corresponding object locations\"},\n    {\"question\": \"What does MLP stand for?\", \"context\": \"A multi-layer perceptron (MLP) is used to adapt vision-tokens to language space suitable for input to a Large Language Model (Vicuna 1.5).\", \"answer\": \"Multi-layer perceptron\"},\n    {\"question\": \"What kind of prompts can be given to the model?\", \"context\": \"Besides visual inputs, region locations can also be input to the model together with task-specific prompts that specify the desired task required by the user.\", \"answer\": \"Task-specific prompts\"}\n]\n```"}
{"uuid": "ef7dfbfb-f407-4baa-a301-34a23b37e63b", "raw_resp": "```json\n[\n    {\"question\": \"What tasks can GeoChat perform?\", \"context\": \"GeoChat can perform multiple tasks as shown on top e.g., scene classification, image/region captioning, VQA and grounded conversations.\", \"answer\": \"GeoChat can perform scene classification, image/region captioning, VQA, and grounded conversations.\"},\n    {\"question\": \"What effect does the increase have on the number of patches?\", \"context\": \"leads to an increase in the number of patches to almost dou- ble (i.e., 1296 per image), this enhanced resolution allows us to handle larger image sizes and also supports better visual grounding in high-resolution RS images.\", \"answer\": \"The increase leads to almost double the number of patches, i.e., 1296 per image.\"},\n    {\"question\": \"How does the enhanced resolution benefit the handling of images?\", \"context\": \"leads to an increase in the number of patches to almost dou- ble (i.e., 1296 per image), this enhanced resolution allows us to handle larger image sizes and also supports better visual grounding in high-resolution RS images.\", \"answer\": \"Enhanced resolution allows handling larger image sizes and supports better visual grounding in high-resolution RS images.\"},\n    {\"question\": \"What is the dimension of the output tokens from the frozen CLIP-ViT?\", \"context\": \"MLP Cross-modal Adaptor. From the frozen CLIP- ViT[28], we project the output tokens ( ∈R1296×1024) with\", \"answer\": \"The dimension of the output tokens from the frozen CLIP-ViT is R1296×1024.\"},\n    {\"question\": \"What component projects the output tokens from the frozen CLIP-ViT?\", \"context\": \"MLP Cross-modal Adaptor. From the frozen CLIP- ViT[28], we project the output tokens ( ∈R1296×1024) with\", \"answer\": \"The MLP Cross-modal Adaptor projects the output tokens from the frozen CLIP-ViT.\"},\n    {\"question\": \"What is the resolution of the output tokens after projection?\", \"context\": \"MLP Cross-modal Adaptor. From the frozen CLIP- ViT[28], we project the output tokens ( ∈R1296×1024) with\", \"answer\": \"The resolution of the output tokens after projection is 1296×1024.\"},\n    {\"question\": \"What does the enhanced resolution support in high-resolution RS images?\", \"context\": \"leads to an increase in the number of patches to almost dou- ble (i.e., 1296 per image), this enhanced resolution allows us to handle larger image sizes and also supports better visual grounding in high-resolution RS images.\", \"answer\": \"The enhanced resolution supports better visual grounding in high-resolution RS images.\"},\n    {\"question\": \"How many patches per image are there after the enhancement?\", \"context\": \"leads to an increase in the number of patches to almost dou- ble (i.e., 1296 per image), this enhanced resolution allows us to handle larger image sizes and also supports better visual grounding in high-resolution RS images.\", \"answer\": \"After the enhancement, there are 1296 patches per image.\"}\n]\n```"}
{"uuid": "ffb10c2c-e465-4e41-9e07-279e85b62f9b", "raw_resp": "```json\n[\n    {\"question\": \"What is the input dimensionality of the adaptor used in the model?\", \"context\": \"dimensions 1024 onto the language model space, using an MLP adaptor with one hidden layer. The adaptor has an input dimensionality of 1024 and outputs a vector of size 4096, corresponding to the input size of the LLM [7].\", \"answer\": \"The input dimensionality of the adaptor is 1024.\"},\n    {\"question\": \"What is the output size of the adaptor?\", \"context\": \"dimensions 1024 onto the language model space, using an MLP adaptor with one hidden layer. The adaptor has an input dimensionality of 1024 and outputs a vector of size 4096, corresponding to the input size of the LLM [7].\", \"answer\": \"The output size of the adaptor is 4096.\"},\n    {\"question\": \"Which activation function is used in the adaptor?\", \"context\": \"A GeLU [10] is used as the activation function.\", \"answer\": \"GeLU is used as the activation function.\"},\n    {\"question\": \"What is the name of the large language model used as the foundation for GeoChat?\", \"context\": \"The open source Vicuna-v1.5(7B) [7] large language model is utilised as the foundation for GeoChat.\", \"answer\": \"Vicuna-v1.5(7B) is the large language model used as the foundation for GeoChat.\"},\n    {\"question\": \"What is the role of the language model in GeoChat?\", \"context\": \"The language model functions as a single interface for diverse vision-language inputs in our frame-\", \"answer\": \"The language model functions as a single interface for diverse vision-language inputs in GeoChat.\"},\n    {\"question\": \"How many hidden layers does the MLP adaptor have?\", \"context\": \"dimensions 1024 onto the language model space, using an MLP adaptor with one hidden layer.\", \"answer\": \"The MLP adaptor has one hidden layer.\"},\n    {\"question\": \"What is the size of the input to the LLM?\", \"context\": \"dimensions 1024 onto the language model space, using an MLP adaptor with one hidden layer. The adaptor has an input dimensionality of 1024 and outputs a vector of size 4096, corresponding to the input size of the LLM [7].\", \"answer\": \"The input size of the LLM is 4096.\"},\n    {\"question\": \"Is the language model used in GeoChat open source?\", \"context\": \"The open source Vicuna-v1.5(7B) [7] large language model is utilised as the foundation for GeoChat.\", \"answer\": \"Yes, the language model used in GeoChat is open source.\"}\n]\n```"}
{"uuid": "d0451443-62fd-49f5-95b2-a0dc418d861f", "raw_resp": "```json\n[\n    {\"question\": \"What version of Vicuna is used for vision-language tasks?\", \"context\": \"To accomplish different vision-language tasks, we directly depend on the Vicuna-v1.5(7B) [7] language to-kens.\", \"answer\": \"Vicuna-v1.5(7B)\"},\n    {\"question\": \"How do they use the language model for visual grounding tasks?\", \"context\": \"We explicitly interact with the language model to construct textual representations of bounding boxes to ex-press their spatial coordinates for the visual grounding tasks that require the production of spatial locations.\", \"answer\": \"They construct textual representations of bounding boxes to express their spatial coordinates.\"},\n    {\"question\": \"What is needed for the visual grounding tasks?\", \"context\": \"We explicitly interact with the language model to construct textual representations of bounding boxes to ex-press their spatial coordinates for the visual grounding tasks that require the production of spatial locations.\", \"answer\": \"The production of spatial locations\"},\n    {\"question\": \"How is the safe, aligned, and effective behavior of LLM ensured?\", \"context\": \"Similarly, the safe, aligned and effective behavior of LLM is ensured via system prompts appended together with given inputs.\", \"answer\": \"Via system prompts appended together with given inputs\"},\n    {\"question\": \"What does the system rely on to perform vision-language tasks?\", \"context\": \"To accomplish different vision-language tasks, we directly depend on the Vicuna-v1.5(7B) [7] language to-kens.\", \"answer\": \"The system relies on the Vicuna-v1.5(7B) language tokens\"},\n    {\"question\": \"What is the purpose of interacting with the language model?\", \"context\": \"We explicitly interact with the language model to construct textual representations of bounding boxes to ex-press their spatial coordinates for the visual grounding tasks that require the production of spatial locations.\", \"answer\": \"The purpose is to construct textual representations of bounding boxes for visual grounding tasks\"},\n    {\"question\": \"What ensures the effective behavior of the LLM?\", \"context\": \"Similarly, the safe, aligned and effective behavior of LLM is ensured via system prompts appended together with given inputs.\", \"answer\": \"System prompts appended with given inputs ensure the effective behavior of the LLM\"},\n    {\"question\": \"What does the interaction with the language model produce?\", \"context\": \"We explicitly interact with the language model to construct textual representations of bounding boxes to ex-press their spatial coordinates for the visual grounding tasks that require the production of spatial locations.\", \"answer\": \"It produces textual representations of bounding boxes expressing their spatial coordinates\"}\n]\n```"}
{"uuid": "7d8f01da-3650-484f-a4b5-400f5cf48664", "raw_resp": "```json\n[\n    {\"question\": \"What figure is mentioned in the text?\", \"context\": \"[ASSISTANT]Figure 3. Multi-task instruction template for GeoChat.\", \"answer\": \"Figure 3\"},\n    {\"question\": \"What is the name of the multi-task instruction template?\", \"context\": \"[ASSISTANT]Figure 3. Multi-task instruction template for GeoChat.\", \"answer\": \"Multi-task instruction template for GeoChat\"},\n    {\"question\": \"Which adaptation strategy is mentioned in the text?\", \"context\": \"[ASSISTANT]Low-Rank Adaptation (LoRA) [11] based strategy is used for fine-tuning the LLM.\", \"answer\": \"Low-Rank Adaptation (LoRA)\"},\n    {\"question\": \"What is the name of the pre-trained model mentioned?\", \"context\": \"[ASSISTANT]While training, instead of finetun- ing all of the weights that comprise the weight matrix of the pre-trained Vicuna-v1.5[7], we finetune two smaller matrices in LoRA [11] that approximate the original larger matrix.\", \"answer\": \"Vicuna-v1.5\"},\n    {\"question\": \"Instead of fine-tuning all weights, what does the strategy involve fine-tuning?\", \"context\": \"[ASSISTANT]While training, instead of finetun- ing all of the weights that comprise the weight matrix of the pre-trained Vicuna-v1.5[7], we finetune two smaller matrices in LoRA [11] that approximate the original larger matrix.\", \"answer\": \"two smaller matrices in LoRA\"},\n    {\"question\": \"What happens after the fine-tuning process?\", \"context\": \"[ASSISTANT]After that, the fine-tuned adaptor is fed into the pre-\", \"answer\": \"the fine-tuned adaptor is fed into the pre-\"},\n    {\"question\": \"What is the task identifier mentioned in the text?\", \"context\": \"[USER] <im_start> Image Features <im_end> [Task Identifier]\", \"answer\": \"Task Identifier\"},\n    {\"question\": \"What is the system or tool mentioned in relation to the task?\", \"context\": \"[USER] <im_start> Image Features <im_end> [Task Identifier] [ASSISTANT]Figure 3. Multi-task instruction template for GeoChat.\", \"answer\": \"GeoChat\"}\n]\n```"}
{"uuid": "bfede987-8b5e-41d1-b378-3c4d9c7776ef", "raw_resp": "```json\n[\n    {\"question\": \"What adaptation method is mentioned in the text that ensures faster training and prevents forgetting original knowledge?\", \"context\": \"trained model and utilised for inference. The LoRA adap-tation ensures faster training and avoids forgetting original knowledge embedded in the LLM trained and fine-tuned on generic natural language instructions.\", \"answer\": \"LoRA adaptation\"},\n    {\"question\": \"What does LoRA allow the model to do within the remote-sensing reasoning framework of GeoChat?\", \"context\": \"This is an important feature since it allows the model to bring in external con-text about generic object types, landmarks and affordances in the remote-sensing reasoning framework of GeoChat.\", \"answer\": \"bring in external context about generic object types, landmarks and affordances\"},\n    {\"question\": \"What is the purpose of using LoRA adaptation according to the text?\", \"question\": \"This is an important feature since it allows the model to bring in external con-text about generic object types, landmarks and affordances in the remote-sensing reasoning framework of GeoChat.\", \"answer\": \"to ensure faster training, avoid forgetting original knowledge, and allow the model to bring in external context\"},\n    {\"question\": \"What is the name of the remote-sensing reasoning framework mentioned in the text?\", \"context\": \"This is an important feature since it allows the model to bring in external con-text about generic object types, landmarks and affordances in the remote-sensing reasoning framework of GeoChat.\", \"answer\": \"GeoChat\"},\n    {\"question\": \"What kind of instructions was the LLM trained and fine-tuned on?\", \"context\": \"The LoRA adap-tation ensures faster training and avoids forgetting original knowledge embedded in the LLM trained and fine-tuned on generic natural language instructions.\", \"answer\": \"generic natural language instructions\"},\n    {\"question\": \"Which section of the document provides details about training?\", \"context\": \"3.2. Training Details To enhance the effectiveness of our model on general visual\", \"answer\": \"3.2. Training Details\"},\n    {\"question\": \"What is the goal of the training details mentioned in the text?\", \"context\": \"3.2. Training Details To enhance the effectiveness of our model on general visual\", \"answer\": \"To enhance the effectiveness of our model on general visual\"},\n    {\"question\": \"What aspect of the model's performance is aimed to be improved by the training details provided?\", \"context\": \"3.2. Training Details To enhance the effectiveness of our model on general visual\", \"answer\": \"effectiveness on general visual\"}\n]\n```"}
{"uuid": "2a86211b-46d4-47bd-a3f7-665a8de20246", "raw_resp": "```json\n[\n    {\"question\": \"What strategy is employed to optimize training efficiency?\", \"context\": \"tasks and optimize training efficiency, we employ a strategy that involves initializing the network with pre-trained weights and fine-tuning specific segments for remote sens-\", \"answer\": \"The strategy involves initializing the network with pre-trained weights and fine-tuning specific segments.\"},\n    {\"question\": \"How is the network initialized in the described strategy?\", \"context\": \"tasks and optimize training efficiency, we employ a strategy that involves initializing the network with pre-trained weights and fine-tuning specific segments for remote sens-\", \"answer\": \"The network is initialized with pre-trained weights.\"},\n    {\"question\": \"What is done after initializing the network with pre-trained weights?\", \"context\": \"tasks and optimize training efficiency, we employ a strategy that involves initializing the network with pre-trained weights and fine-tuning specific segments for remote sens-\", \"answer\": \"Specific segments are fine-tuned.\"},\n    {\"question\": \"For what purpose is the strategy involving pre-trained weights and fine-tuning used?\", \"context\": \"tasks and optimize training efficiency, we employ a strategy that involves initializing the network with pre-trained weights and fine-tuning specific segments for remote sens-\", \"answer\": \"To optimize training efficiency.\"},\n    {\"question\": \"Which field is the strategy particularly applied to?\", \"context\": \"tasks and optimize training efficiency, we employ a strategy that involves initializing the network with pre-trained weights and fine-tuning specific segments for remote sens-\", \"answer\": \"Remote sensing.\"},\n    {\"question\": \"What number appears in the text?\", \"context\": \"-27834\", \"answer\": \"27834\"},\n    {\"question\": \"Where does the number 27834 appear in the text?\", \"context\": \"4\\n27834\", \"answer\": \"It appears right after the number 4 on a new line.\"}\n]\n```"}
{"uuid": "a5a5f5ef-a0a5-40f1-80d3-efcf925cfed4", "raw_resp": "```json\n[\n    {\"question\": \"Where is the white ship present?\", \"context\": \"Whiteshipispresentatthetopleft.\", \"answer\": \"The white ship is present at the top left.\"},\n    {\"question\": \"What is the color of the ship anchored at the harbor?\", \"context\": \"Theshipanchoredattheharborisgreyincolor.\", \"answer\": \"The ship anchored at the harbor is grey in color.\"},\n    {\"question\": \"What does the presence of 4 harbors and 2 ships indicate?\", \"context\": \"Thepresenceof4harborsand2shipsconstituteabustlingmaritimeenvironment.Thepresenceofsuchharborsmayindicateawaterareawiththepossibilityofexport/importsofgoodsaswellastourism.\", \"answer\": \"The presence of 4 harbors and 2 ships indicates a bustling maritime environment with the possibility of exports/imports of goods as well as tourism.\"},\n    {\"question\": \"What does the satellite image show in detail?\", \"context\": \"Thissatelliteimageshowsdepictsariverscenewithmultipleshipsandharborspresent.Thereareatotaloftwoshipsandfourharbors.Thegreyshipatthebottomrightisanchoredtotheharbor………..\", \"answer\": \"The satellite image shows a river scene with multiple ships and harbors. There are a total of two ships and four harbors. The grey ship at the bottom right is anchored to the harbor.\"},\n    {\"question\": \"Describe the image in detail according to the grounded description.\", \"context\": \"Thissatelliteimagedepictsagreyshipanchoredatalargeharbor.Thereisanotherharboratthetopright,whichisadjacenttosometrees.Thereisanotherwhiteshipclosetoaharborattheright.\", \"answer\": \"The satellite image depicts a grey ship anchored at a large harbor. There is another harbor at the top right, which is adjacent to some trees. There is another white ship close to a harbor at the right.\"},\n    {\"question\": \"How many harbors are shown in the satellite image?\", \"context\": \"Thereareatotaloftwoshipsandfourharbors.\", \"answer\": \"There are a total of four harbors shown in the satellite image.\"},\n    {\"question\": \"Where is the other harbor located besides the one where the grey ship is anchored?\", \"context\": \"Thereisanotherharboratthetopright,whichisadjacenttosometrees.\", \"answer\": \"The other harbor is located at the top right, adjacent to some trees.\"},\n    {\"question\": \"What is near the harbor at the right?\", \"context\": \"Thereisanotherwhiteshipclosetoaharborattheright.\", \"answer\": \"A white ship is close to the harbor at the right.\"}\n]\n```"}
{"uuid": "712afef9-3566-474b-bf17-675d58097d79", "raw_resp": "```json\n[\n    {\"question\": \"What attributes and locations are specified for the ship?\", \"context\": \"1.Ship{white,small,top-left},{grey,small,bottom-right}\", \"answer\": \"The ship has two instances: one that is white, small, and located at the top-left; and another that is grey, small, and located at the bottom-right.\"},\n    {\"question\": \"What details are provided about the harbor?\", \"context\": \"2.Harbor{small,right},{large,bottom-left},{small,topright},{small,left}\", \"answer\": \"There are four harbors described: a small one on the right, a large one at the bottom-left, and two small ones, one at the top-right and another on the left.\"},\n    {\"question\": \"Where are the trees located?\", \"context\": \"3.Trees{top-right}\", \"answer\": \"The trees are located at the top-right.\"},\n    {\"question\": \"How is the referring expression for the small grey ship described?\", \"context\": \"1.SmallGreyshipanchoredatlargeharboronbottom-right\", \"answer\": \"The small grey ship is anchored at the large harbor on the bottom-right.\"},\n    {\"question\": \"What is the description of the trees in the referring expression?\", \"context\": \"2.SomeTreesatthetop-right\", \"answer\": \"Some trees are located at the top-right.\"},\n    {\"question\": \"Where is the white ship positioned according to the referring expression?\", \"context\": \"3.Whiteshipatthetopofasmallharborontheright\", \"answer\": \"The white ship is at the top of a small harbor on the right.\"},\n    {\"question\": \"What is the format of the region captioning?\", \"context\": \"1.Objectlocatedat{}.2.Sizeofobjectat.{}issmall.\", \"answer\": \"The format of the region captioning includes placeholders such as 'Object located at {}' and 'Size of object at {} is small.'\"},\n    {\"question\": \"What type of information does Figure 4 provide?\", \"context\": \"Figure 4. Types of annotations available in the GeoChat instruction-set. Top-row: For a given RS image, we obtain object attribute and\", \"answer\": \"Figure 4 provides types of annotations available in the GeoChat instruction-set, including object attributes for a given RS (Remote Sensing) image.\"}\n]\n```"}
{"uuid": "3ec8d1fa-5d9e-46d4-9c4f-4eaa5b4bfc15", "raw_resp": "```json\n[\n    {\"question\": \"What type of information is shown over the image?\", \"context\": \"relationship information, referring expressions and region captions along with their corresponding region annotations (shown over the image).\", \"answer\": \"Relationship information, referring expressions, and region captions along with their corresponding region annotations.\"},\n    {\"question\": \"How many image-instruction pairs were created using the structured information?\", \"context\": \"Bottom-row: This structured information is used to create the rich instruction-set with a total of 318k image-instruction pairs.\", \"answer\": \"A total of 318k image-instruction pairs were created.\"},\n    {\"question\": \"Which encoder was used in the study?\", \"context\": \"We use a pre-trained CLIP-ViT(L-14) encoder[28],trained on large amounts of textual and visual data,\", \"answer\": \"Pre-trained CLIP-ViT(L-14) encoder was used.\"},\n    {\"question\": \"On what kind of data was the encoder trained?\", \"context\": \"We use a pre-trained CLIP-ViT(L-14) encoder[28],trained on large amounts of textual and visual data,\", \"answer\": \"The encoder was trained on large amounts of textual and visual data.\"},\n    {\"question\": \"Which pretrained model was used as an adaptor?\", \"context\": \"a pretrained MLP adaptor[17], pretrained on a 558K subset of the LAION-CC-SBU [26] dataset with BLIP [15]\", \"answer\": \"Pretrained MLP adaptor was used as an adaptor.\"},\n    {\"question\": \"On which dataset subset was the pretrained MLP adaptor trained?\", \"context\": \"a pretrained MLP adaptor[17], pretrained on a 558K subset of the LAION-CC-SBU [26] dataset with BLIP [15]\", \"answer\": \"The pretrained MLP adaptor was trained on a 558K subset of the LAION-CC-SBU dataset.\"},\n    {\"question\": \"Which additional tool was used alongside the dataset subset?\", \"context\": \"a pretrained MLP adaptor[17], pretrained on a 558K subset of the LAION-CC-SBU [26] dataset with BLIP [15]\", \"answer\": \"BLIP was used alongside the dataset subset.\"},\n    {\"question\": \"What was the purpose of using these tools and datasets?\", \"context\": \"ing related tasks.\", \"answer\": \"The purpose was to perform related tasks.\"}\n]\n```"}
{"uuid": "ea517f2f-e86b-47c5-916a-437af825d149", "raw_resp": "```json\n[\n    {\"question\": \"Which model was used to initialize the system being discussed?\", \"context\": \"captions, and Vicuna-v1.5[7] to initialize our model.\", \"answer\": \"Vicuna-v1.5\"},\n    {\"question\": \"What technique is applied to adapt the model to remote sensing images?\", \"context\": \"To adapt our model to remote sensing images, we subsequently LoRA [11] fine-tune the LLM, while keeping the MLP adaptor and the CLIP encoder [28] frozen during training.\", \"answer\": \"LoRA fine-tuning\"},\n    {\"question\": \"Which components were kept frozen during the training process?\", \"context\": \"To adapt our model to remote sensing images, we subsequently LoRA [11] fine-tune the LLM, while keeping the MLP adaptor and the CLIP encoder [28] frozen during training.\", \"answer\": \"MLP adaptor and the CLIP encoder\"},\n    {\"question\": \"What is the purpose of the dataset created using LLM Vicuna?\", \"context\": \"By using LLM Vicuna [7], we align the model to follow a range of instructions by presenting and curating varied instruction-following data with multi-round conversations regarding remote sensing imagery (Table 1).\", \"answer\": \"To align the model to follow a range of instructions\"},\n    {\"question\": \"How is the dataset curated?\", \"context\": \"By using LLM Vicuna [7], we align the model to follow a range of instructions by presenting and curating varied instruction-following data with multi-round conversations regarding remote sensing imagery (Table 1).\", \"answer\": \"By presenting and curating varied instruction-following data with multi-round conversations regarding remote sensing imagery\"},\n    {\"question\": \"What type of data does the RS Multimodal Instruction Dataset contain?\", \"context\": \"By using LLM Vicuna [7], we align the model to follow a range of instructions by presenting and curating varied instruction-following data with multi-round conversations regarding remote sensing imagery (Table 1).\", \"answer\": \"Varied instruction-following data with multi-round conversations regarding remote sensing imagery\"},\n    {\"question\": \"Which table provides additional details about the RS Multimodal Instruction Dataset?\", \"context\": \"By using LLM Vicuna [7], we align the model to follow a range of instructions by presenting and curating varied instruction-following data with multi-round conversations regarding remote sensing imagery (Table 1).\", \"answer\": \"Table 1\"},\n    {\"question\": \"What is the main focus of the multi-round conversations in the dataset?\", \"context\": \"By using LLM Vicuna [7], we align the model to follow a range of instructions by presenting and curating varied instruction-following data with multi-round conversations regarding remote sensing imagery (Table 1).\", \"answer\": \"Remote sensing imagery\"}\n]\n```"}
{"uuid": "c83dbd1d-fbe4-4d51-9579-52501c8958a6", "raw_resp": "```json\n[\n    {\"question\": \"What does the system use to generate multi-round question and answer pairs?\", \"context\": \"cally provide system instructions as prompts that ask Vi-\\ncuna [7] to generate multi-round question and answer pairs\\nin a manner as if it could visualize the image (although it\\nonly has access to the text).\", \"answer\": \"The system uses Vicuna [7] to generate multi-round question and answer pairs.\"},\n    {\"question\": \"How is the generation of instruction-response pairs achieved?\", \"answer\": \"This is achieved by providing few-shot in-context examples manually composed within the prompt.\", \"context\": \"This is achieved by providing few-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied.\"},\n    {\"question\": \"What does the system simulate when generating QA pairs?\", \"answer\": \"The system simulates being able to visualize the image even though it only has access to the text.\", \"context\": \"cally provide system instructions as prompts that ask Vi-\\ncuna [7] to generate multi-round question and answer pairs\\nin a manner as if it could visualize the image (although it\\nonly has access to the text).\"},\n    {\"question\": \"What is provided within the prompt to guide Vicuna [7]?\", \"answer\": \"Few-shot in-context examples are provided within the prompt.\", \"context\": \"This is achieved by providing few-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied.\"},\n    {\"question\": \"Based on what information does Vicuna [7] build the instruction-response pairs?\", \"answer\": \"Vicuna [7] builds the instruction-response pairs based on the caption and information supplied.\", \"context\": \"This is achieved by providing few-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied.\"},\n    {\"question\": \"What kind of examples are manually composed within the prompt?\", \"answer\": \"Few-shot in-context examples are manually composed within the prompt.\", \"context\": \"This is achieved by providing few-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied.\"},\n    {\"question\": \"What is the purpose of the few-shot in-context examples?\", \"answer\": \"The purpose is to show Vicuna [7] how to build high-quality instruction-response pairs.\", \"context\": \"This is achieved by providing few-shot in-context examples manually composed within\\nthe prompt to show Vicuna [7] how to build high-quality\\ninstruction-response pairs based on the caption and infor-\\nmation supplied.\"},\n    {\"question\": \"From what do the short descriptions come?\", \"answer\": \"The short descriptions come from our descriptions.\", \"context\": \"Specifically, from our short descriptions\"}\n]\n```"}
{"uuid": "0286b743-2dfa-4d3c-a394-cb4a3a5acec5", "raw_resp": "```json\n[\n    {\"question\": \"How many images were sampled to create multi-round conversations?\", \"context\": \"created using the below pipeline, we randomly sample 65k images to create multi-round conversations,\", \"answer\": \"65k\"},\n    {\"question\": \"How many images were used to generate complex question answers?\", \"context\": \"10k images to generate complex question answers and 30k images to gen- erate detailed descriptions for the given short descriptions.\", \"answer\": \"10k\"},\n    {\"question\": \"How many images were utilized to generate detailed descriptions?\", \"context\": \"10k images to generate complex question answers and 30k images to gen- erate detailed descriptions for the given short descriptions.\", \"answer\": \"30k\"},\n    {\"question\": \"What is the total number of image-instruction pairs obtained for training?\", \"context\": \"In combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs for training and 10k for testing.\", \"answer\": \"nearly 308k\"},\n    {\"question\": \"How many image-instruction pairs were set aside for testing?\", \"context\": \"In combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs for training and 10k for testing.\", \"answer\": \"10k\"},\n    {\"question\": \"What is the purpose of converting images to instruction format?\", \"context\": \"In combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs for training and 10k for testing. Next, we outline the instruction-set creation process.\", \"answer\": \"to obtain image-instruction pairs for training and testing\"},\n    {\"question\": \"What is the next step after obtaining the image-instruction pairs?\", \"context\": \"In combination, after conversion to instruction format,we obtain a total of nearly 308k image-instruction pairs for training and 10k for testing. Next, we outline the instruction-set creation process.\", \"answer\": \"outlining the instruction-set creation process\"},\n    {\"question\": \"What details about the dataset category are mentioned in the text?\", \"context\": \"Dataset Category # Classes # Images Image Size\", \"answer\": \"The text mentions # Classes, # Images, and Image Size as categories within the dataset.\"}\n]\n```"}
{"uuid": "b3ff2467-9a43-49e1-ab85-bda122797ba6", "raw_resp": "```json\n[\n    {\"question\": \"What is the dataset category for DOTA?\", \"context\": \"Dataset Category # Classes # Images Image Size\\nDOTA Object Detection 18 17,480 1024 ×1024\", \"answer\": \"Object Detection\"},\n    {\"question\": \"How many classes does the DIOR dataset contain?\", \"context\": \"DIOR Object Detection 20 23,463 800 ×800\", \"answer\": \"20\"},\n    {\"question\": \"What is the image size for FAIR1M dataset?\", \"context\": \"FAIR1M Object Detection 37 64,147 600 ×600\", \"answer\": \"600 ×600\"},\n    {\"question\": \"Which dataset is used for Visual Question Answering and has an image size of 256 ×256?\", \"context\": \"LRBEN(rsvqa) Visual Question Answering - 600 256 ×256\", \"answer\": \"LRBEN(rsvqa)\"},\n    {\"question\": \"How many images are included in the Floodnet dataset?\", \"context\": \"Floodnet Visual Question Answering - 4056 3000 ×4000\", \"answer\": \"4056\"},\n    {\"question\": \"For NWPU-RESISC-45 dataset, what is the task category?\", \"context\": \"NWPU-RESISC-45 Scene Classification 45 31,500 256 ×256\", \"answer\": \"Scene Classification\"},\n    {\"question\": \"What is the number of classes in the NWPU-RESISC-45 dataset?\", \"context\": \"NWPU-RESISC-45 Scene Classification 45 31,500 256 ×256\", \"answer\": \"45\"},\n    {\"question\": \"What is the total number of images in the DOTA dataset?\", \"context\": \"DOTA Object Detection 18 17,480 1024 ×1024\", \"answer\": \"17,480\"}\n]\n```"}
{"uuid": "0125040e-62c0-42d3-81cd-7e171b76480e", "raw_resp": "```json\n[\n    {\"question\": \"What types of datasets are incorporated into the instruction set?\", \"context\": \"Constituent Datasets: In the compilation of our in-struction set, we incorporate three distinct types of datasets, encompassing the ones designed for object detection, scene classification, and visual question answering (VQA).\", \"answer\": \"Three distinct types of datasets: those designed for object detection, scene classification, and visual question answering (VQA).\"},\n    {\"question\": \"What are the specific datasets integrated for object detection?\", \"context\": \"Specifically, we integrate three object detection (DOTA [35], DIOR [6], and FAIR1M [27] which together form\", \"answer\": \"DOTA, DIOR, and FAIR1M.\"},\n    {\"question\": \"What does VQA stand for in the context of the constituent datasets?\", \"context\": \"Constituent Datasets: In the compilation of our in-struction set, we incorporate three distinct types of datasets, encompassing the ones designed for object detection, scene classification, and visual question answering (VQA).\", \"answer\": \"Visual Question Answering\"},\n    {\"question\": \"How many types of datasets are mentioned in the instruction set compilation?\", \"context\": \"Constituent Datasets: In the compilation of our in-struction set, we incorporate three distinct types of datasets, encompassing the ones designed for object detection, scene classification, and visual question answering (VQA).\", \"answer\": \"Three distinct types of datasets are mentioned.\"},\n    {\"question\": \"Which dataset is cited with the reference [27]?\", \"context\": \"Specifically, we integrate three object detection (DOTA [35], DIOR [6], and FAIR1M [27] which together form\", \"answer\": \"FAIR1M is cited with the reference [27].\"},\n    {\"question\": \"What is one of the purposes of incorporating different datasets in the instruction set?\", \"answer\": \"To ensure diversity with varying image sizes and types of classes.\", \"context\": \"visual question answering and scene classification datasets with varying image sizes and types of classes to ensure diversity.\"},\n    {\"question\": \"Name one dataset used for object detection that is referenced with [35].\", \"context\": \"Specifically, we integrate three object detection (DOTA [35], DIOR [6], and FAIR1M [27] which together form\", \"answer\": \"DOTA is referenced with [35].\"},\n    {\"question\": \"Why are multiple datasets from different categories included in the instruction set?\", \"context\": \"Constituent Datasets: In the compilation of our in-struction set, we incorporate three distinct types of datasets, encompassing the ones designed for object detection, scene classification, and visual question answering (VQA).\", \"answer\": \"To include datasets from object detection, scene classification, and visual question answering to ensure a diverse set of data.\"}\n]\n```"}
{"uuid": "c4ac6f83-bcba-4a7f-b19e-7766a3de786a", "raw_resp": "```json\n[\n    {\"question\": \"What type of dataset does SAMRS [31] represent?\", \"context\": \"the SAMRS [31] dataset)\", \"answer\": \"The SAMRS [31] dataset represents an object detection dataset.\"},\n    {\"question\": \"What does NWPU-RESISC-45 [5] refer to?\", \"context\": \"one scene classification (NWPU-RESISC-45 [5]),\", \"answer\": \"NWPU-RESISC-45 [5] refers to a scene classification dataset.\"},\n    {\"question\": \"What capability do the object detection datasets provide?\", \"context\": \"The object detec-tion datasets allow region-level reasoning capability as they offer segmentation masks along with bounding boxes.\", \"answer\": \"The object detection datasets provide region-level reasoning capability by offering segmentation masks along with bounding boxes.\"},\n    {\"question\": \"What additional classes were added to the object detection databases?\", \"context\": \"Addition of Missing Classes: Although a wide variety of object classes are included in the object detection databases, several essential categories like buildings, roads,\", \"answer\": \"Essential categories like buildings and roads were added to the object detection databases.\"},\n    {\"question\": \"Which VQA dataset is mentioned in the text?\", \"context\": \"one VQA (LRBEN[20]), and one flood detection [25] VQA dataset (see Table 2).\", \"answer\": \"The VQA datasets mentioned are LRBEN[20] and a flood detection VQA dataset.\"},\n    {\"question\": \"What information is provided about the flood detection dataset?\", \"context\": \"one flood detection [25] VQA dataset (see Table 2).\", \"answer\": \"A flood detection VQA dataset is referenced, but more details can be found in Table 2.\"},\n    {\"question\": \"What is the reference number for the flood detection VQA dataset?\", \"context\": \"one flood detection [25] VQA dataset (see Table 2).\", \"answer\": \"The reference number for the flood detection VQA dataset is [25].\"},\n    {\"question\": \"What is the number associated with the text '5'?\", \"context\": \"several essential categories like buildings, roads, 5\", \"answer\": \"The number '5' appears after the mention of essential categories like buildings and roads, but its specific association is unclear from the given context.\"}\n]\n```"}
{"uuid": "ccc1f456-e73f-42f3-a160-ffb0acecc545", "raw_resp": "```json\n[\n    {\"question\": \"What needs to be given for the Boeing 747?\", \"context\": \"Give bounding box for the Boeing 747 at the bottom left.\", \"answer\": \"The bounding box for the Boeing 747 at the bottom left needs to be given.\"},\n    {\"question\": \"What is located near some buildings at the center of the scene?\", \"context\": \"Inthesatelliteimage,therearefourbaseballfieldslocatednearsomebuildingsatthecenterofthescene.\", \"answer\": \"Four baseball fields are located near some buildings at the center of the scene.\"},\n    {\"question\": \"How are the baseball fields arranged?\", \"question\": \"The baseball fields are arranged in a close proximity to each other.\", \"answer\": \"The baseball fields are arranged in a close proximity to each other.\"},\n    {\"question\": \"What is the status of the road at the center?\", \"context\": \"Yes, the road at the center is completely flooded.\", \"answer\": \"The road at the center is completely flooded.\"},\n    {\"question\": \"Is the entire road flooded?\", \"context\": \"Is the whole road flooded?\", \"answer\": \"Yes, the whole road is flooded.\"},\n    {\"question\": \"How many buildings are flooded?\", \"context\": \"How many buildings are flooded?Three buildings are flooded and are present at the top left.\", \"answer\": \"Three buildings are flooded and are present at the top left.\"}\n]\n``` \n\nNote that two of the questions are nearly identical because the provided text has overlapping information. If you prefer unique questions, please let me know, and I can adjust accordingly."}
{"uuid": "2d4b64d7-8302-4f94-b5e3-1f321a10c54c", "raw_resp": "[\n    {\n        \"question\": \"What figure demonstrates the qualitative results of GeoChat?\",\n        \"context\": \"Figure 5. Qualitative results of GeoChat.\",\n        \"answer\": \"Figure 5\"\n    },\n    {\n        \"question\": \"What kind of tasks are demonstrated in the GeoChat qualitative results?\",\n        \"context\": \"( left-right ) Results are shown on grounding, referring object detection, and disaster/damage detection.\",\n        \"answer\": \"grounding, referring object detection, and disaster/damage detection\"\n    },\n    {\n        \"question\": \"How can users influence the model's response in GeoChat?\",\n        \"context\": \"The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior.\",\n        \"answer\": \"Users can provide task-specific tokens to shape the model's response\"\n    },\n    {\n        \"question\": \"What type of token can a user provide as an example to influence model responses?\",\n        \"context\": \"The user can provide task-specific tokens (e.g., [grounding] ) to shape model responses according to the desired behavior.\",\n        \"answer\": \"[grounding]\"\n    },\n    {\n        \"question\": \"What types of responses can the model generate in GeoChat?\",\n        \"context\": \"The model can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left ).\",\n        \"answer\": \"textual responses, only visual grounding, and both text and object groundings interleaved together\"\n    },\n    {\n        \"question\": \"Where are the textual responses shown in the GeoChat results?\",\n        \"context\": \"The model can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left ).\",\n        \"answer\": \"on the right\"\n    },\n    {\n        \"question\": \"Where are the visual groundings shown in the GeoChat results?\",\n        \"context\": \"The model can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left ).\",\n        \"answer\": \"in the center\"\n    },\n    {\n        \"question\": \"Where are both text and object groundings shown in the GeoChat results?\",\n        \"context\": \"The model can generate textual responses ( right ), only visual grounding ( center ) and both text and object groundings interleaved together ( left ).\",\n        \"answer\": \"on the left\"\n    }\n]"}
{"uuid": "eb304208-b52b-4226-874c-01a2e3a10f36", "raw_resp": "```json\n[\n    {\"question\": \"What can the model specify?\", \"context\": \"The model can also specify object types, object counts, object attributes and object relationships.\", \"answer\": \"Object types, object counts, object attributes, and object relationships.\"},\n    {\"question\": \"What does attribute a1 represent?\", \"context\": \"a1 category (e.g. “plane, ship”)\", \"answer\": \"Category, such as 'plane, ship'\"},\n    {\"question\": \"What does attribute a2 represent?\", \"context\": \"a2 color (e.g. “gray, white”)\", \"answer\": \"Color, such as 'gray, white'\"},\n    {\"question\": \"What does attribute a3 represent?\", \"context\": \"a3 relative size (e.g. “small, large”)\", \"answer\": \"Relative size, such as 'small, large'\"},\n    {\"question\": \"What does attribute a4 represent?\", \"context\": \"a4 relative location (e.g. “top right, bottom”)\", \"answer\": \"Relative location, such as 'top right, bottom'\"},\n    {\"question\": \"What does attribute a5 represent?\", \"context\": \"a5 relation (e.g. “parked at, driving through”)\", \"answer\": \"Relation, such as 'parked at, driving through'\"},\n    {\"question\": \"How are attributes used?\", \"context\": \"Attributes are used to obtain referring expressions e.g., small-sized plane to the left.\", \"answer\": \"To obtain referring expressions, such as 'small-sized plane to the left'\"},\n    {\"question\": \"What is an example of categories?\", \"context\": \"Categories Example\", \"answer\": \"An example is provided but not specified in the text. It likely follows the pattern of providing specific items such as 'plane, ship' under the category attribute.\"}\n]\n```"}
{"uuid": "1c477cd0-505e-4fc2-9341-925366306d0a", "raw_resp": "```json\n[\n    {\"question\": \"What are some examples of relationships mentioned for ships and harbors?\", \"context\": \"Ships and Harbors (e.g. “anchored at, parked at”)\", \"answer\": \"Examples include 'anchored at' and 'parked at'.\"},\n    {\"question\": \"What types of fields are mentioned and what relationships are given for them?\", \"context\": \"Track Field and Soccer Field (e.g. “Surrounded by, Inside”)\", \"answer\": \"Track Field and Soccer Field are mentioned, with relationships such as 'Surrounded by' and 'Inside'. \"},\n    {\"question\": \"Which infrastructure and vehicles are listed together, and what are their relationship examples?\", \"context\": \"Vehicles, Bridge, Road, Roundabout (e.g. “passing through, passing through”)\", \"answer\": \"Vehicles, Bridge, Road, and Roundabout are listed together, with relationships such as 'passing through'. \"},\n    {\"question\": \"What is an example relationship given for vehicles and buildings?\", \"context\": \"Vehicles and Building (e.g. “parked”)\", \"answer\": \"An example relationship given is 'parked'. \"},\n    {\"question\": \"What two entities are paired together with the example relationship of 'parked'? \", \"context\": \"Airport and Plane (e.g. “parked”)\", \"answer\": \"Airport and Plane are paired together with the example relationship of 'parked'. \"},\n    {\"question\": \"What relationship is provided between a ship and a helipad?\", \"context\": \"Ship and Helipad (e.g. “on, contains”)\", \"answer\": \"The relationships provided are 'on' and 'contains'. \"},\n    {\"question\": \"What does the text suggest is missing in the current dataset?\", \"context\": \"and trees are missing. To address this, we propose to uti-\", \"answer\": \"Trees are suggested to be missing in the current dataset. \"},\n    {\"question\": \"What is proposed to be done about the missing elements in the dataset?\", \"context\": \"and trees are missing. To address this, we propose to uti-\", \"answer\": \"To address the missing elements, it is proposed to utilize something, though the exact method is cut off in the text.\"}\n]\n```"}
{"uuid": "a9b32226-617f-4e75-a77f-c6f40f31acaa", "raw_resp": "```json\n[\n    {\"question\": \"What model is utilized and pre-trained on which dataset?\", \"context\": \"lize ViTAE-RVSA [32] model, pre-trained on the LoveDA dataset [33], which encompasses the required important classes.\", \"answer\": \"The ViTAE-RVSA model is utilized and pre-trained on the LoveDA dataset.\"},\n    {\"question\": \"On which dataset does the model infer the required classes?\", \"context\": \"The model [32] is used to infer these classes on the SAMRS [31] dataset, yielding pseudo labels.\", \"answer\": \"The model infers the required classes on the SAMRS dataset.\"},\n    {\"question\": \"What is produced by using the model on the SAMRS dataset?\", \"context\": \"The model [32] is used to infer these classes on the SAMRS [31] dataset, yielding pseudo labels.\", \"answer\": \"Pseudo labels are yielded by using the model on the SAMRS dataset.\"},\n    {\"question\": \"How is the potential noise in the predictions mitigated?\", \"context\": \"To mitigate potential noise in these predictions, we remove the predictions of ViTAE-RVSA [32] for which we already have ground truth from the SAMRS [31] dataset to refine the results.\", \"answer\": \"Potential noise in the predictions is mitigated by removing the predictions for which we already have ground truth from the SAMRS dataset.\"},\n    {\"question\": \"What is removed to refine the results?\", \"context\": \"To mitigate potential noise in these predictions, we remove the predictions of ViTAE-RVSA [32] for which we already have ground truth from the SAMRS [31] dataset to refine the results.\", \"answer\": \"The predictions of ViTAE-RVSA for which we already have ground truth from the SAMRS dataset are removed to refine the results.\"},\n    {\"question\": \"What is the purpose of attribute extraction?\", \"context\": \"Attribute extraction: For referring expression annota-\", \"answer\": \"The purpose of attribute extraction is for referring expression annotation.\"},\n    {\"question\": \"Which classes does the LoveDA dataset encompass?\", \"context\": \"lize ViTAE-RVSA [32] model, pre-trained on the LoveDA dataset [33], which encompasses the required important classes.\", \"answer\": \"The LoveDA dataset encompasses the required important classes.\"},\n    {\"question\": \"What is the source of the ground truth data used for refining the results?\", \"context\": \"To mitigate potential noise in these predictions, we remove the predictions of ViTAE-RVSA [32] for which we already have ground truth from the SAMRS [31] dataset to refine the results.\", \"answer\": \"The ground truth data used for refining the results comes from the SAMRS dataset.\"}\n]\n```"}
{"uuid": "11d94f3e-6b27-469b-8f2f-a394461b19d8", "raw_resp": "```json\n[\n    {\"question\": \"What are the five distinct types of attributes selected for deriving in RS images?\", \"context\": \"tions, it is important to derive a variety of attributes in RS images. To this end, we have selected five distinct types of attributes, as outlined in Table 3.\", \"answer\": \"The five distinct types of attributes are outlined in Table 3.\"},\n    {\"question\": \"Where can object category information be directly obtained from?\", \"context\": \"Object category informa- tion can be directly obtained from the SAMRS dataset.\", \"answer\": \"The SAMRS dataset.\"},\n    {\"question\": \"Which algorithm is used for color extraction?\", \"context\": \"For color extraction, we use the K-Means clustering algorithm.\", \"answer\": \"The K-Means clustering algorithm.\"},\n    {\"question\": \"How are an object's pixels extracted from the image?\", \"question\": \"How are an object's pixels extracted from the image?\", \"context\": \"Specifically, we extract the object’s pixels from the image using ground-truth box and cluster them into Kgroups.\", \"answer\": \"Using ground-truth box.\"},\n    {\"question\": \"What is done after clustering the object’s pixels into K groups?\", \"context\": \"The center of the largest cluster is then selected as the object’s\", \"answer\": \"The center of the largest cluster is then selected as the object’s [attribute].\"},\n    {\"question\": \"How many steps are involved in the color extraction process?\", \"context\": \"Specifically, we extract the object’s pixels from the image using ground-truth box and cluster them into Kgroups. The center of the largest cluster is then selected as the object’s\", \"answer\": \"Two steps: first, extracting the object’s pixels from the image using ground-truth box and clustering them into K groups; second, selecting the center of the largest cluster as the object’s attribute.\"},\n    {\"question\": \"What does SAMRS stand for?\", \"context\": \"Object category informa- tion can be directly obtained from the SAMRS dataset.\", \"answer\": \"The text does not provide the full form of SAMRS.\"},\n    {\"question\": \"Is the process of color extraction described in detail?\", \"context\": \"Specifically, we extract the object’s pixels from the image using ground-truth box and cluster them into Kgroups. The center of the largest cluster is then selected as the object’s\", \"answer\": \"Yes, the process involves extracting the object’s pixels from the image using ground-truth box and clustering them into K groups, then selecting the center of the largest cluster as the object’s attribute.\"}\n]\n```"}
{"uuid": "df69d168-4c98-4b1f-8eb0-5bf903d2049f", "raw_resp": "```json\n[\n    {\"question\": \"How many size categories are used to classify objects?\", \"context\": \"To specify the relative size of the object, we categorize objects into three sizes: small, normal, and large.\", \"answer\": \"Three\"},\n    {\"question\": \"What percentile is used to define an object as 'large'?\", \"context\": \"This categorization is determined by measuring the area of all instances of a class in the entire dataset and assigning the 80th percentile as the large label.\", \"answer\": \"80th percentile\"},\n    {\"question\": \"Which size category is assigned to objects that fall between the 20th and 80th percentiles?\", \"context\": \"Similarly, the 20th percentile is designated as small size, with the remaining falling into the normal category.\", \"answer\": \"Normal\"},\n    {\"question\": \"What is the smallest size category for objects based on the given criteria?\", \"context\": \"Similarly, the 20th percentile is designated as small size, with the remaining falling into the normal category.\", \"answer\": \"Small\"},\n    {\"question\": \"How is the 'small' size category defined?\", \"context\": \"Similarly, the 20th percentile is designated as small size, with the remaining falling into the normal category.\", \"answer\": \"Objects at the 20th percentile\"},\n    {\"question\": \"What method is used to measure the size of objects?\", \"context\": \"This categorization is determined by measuring the area of all instances of a class in the entire dataset and assigning the 80th percentile as the large label.\", \"answer\": \"Measuring the area of all instances of a class in the entire dataset\"},\n    {\"question\": \"How is the relative position of objects within images determined?\", \"context\": \"To determine the object’s relative position within the images, we partition the entire image into\", \"answer\": \"By partitioning the entire image into sections\"},\n    {\"question\": \"What does the categorization of object sizes depend on?\", \"context\": \"This categorization is determined by measuring the area of all instances of a class in the entire dataset and assigning the 80th percentile as the large label.\", \"answer\": \"The area measurement of all instances of a class in the dataset\"}\n]\n```"}
{"uuid": "ceb5217e-a212-40ce-acc9-8fb6645d7f58", "raw_resp": "```json\n[\n    {\"question\": \"What does the 3×3 grid define?\", \"context\": \"a 3×3 grid, defining regions such as Top Right, Top, Top Left, Left, Center, Right, Bottom Right, Bottom Left, and Bottom.\", \"answer\": \"The 3×3 grid defines regions such as Top Right, Top, Top Left, Left, Center, Right, Bottom Right, Bottom Left, and Bottom.\"},\n    {\"question\": \"How are objects' positions assigned?\", \"context\": \"Based on the object’s center pixel coordinates, we assign its relative position accordingly.\", \"answer\": \"Objects' positions are assigned based on the object's center pixel coordinates.\"},\n    {\"question\": \"What is used to group different objects?\", \"context\": \"To define the relation between objects in a given image, we group different objects based on their distance between the bounding boxes,\", \"answer\": \"Different objects are grouped based on their distance between the bounding boxes.\"},\n    {\"question\": \"How are relationships between objects defined?\", \"context\": \"and for each sub-graph, we assign different relationships between objects based on their class la-\", \"answer\": \"For each sub-graph, different relationships between objects are assigned based on their class labels.\"},\n    {\"question\": \"What is the purpose of using a 3×3 grid?\", \"context\": \"a 3×3 grid, defining regions such as Top Right, Top, Top Left, Left, Center, Right, Bottom Right, Bottom Left, and Bottom. Based on the object’s center pixel coordinates, we assign its relative position accordingly.\", \"answer\": \"The purpose of using a 3×3 grid is to define regions and assign relative positions to objects based on their center pixel coordinates.\"},\n    {\"question\": \"What information is used to define the relationship between objects?\", \"context\": \"To define the relation between objects in a given image, we group different objects based on their distance between the bounding boxes, and for each sub-graph, we assign different relationships between objects based on their class la-\", \"answer\": \"The relationship between objects is defined using the distance between bounding boxes and their class labels.\"},\n    {\"question\": \"How are sub-graphs created?\", \"context\": \"To define the relation between objects in a given image, we group different objects based on their distance between the bounding boxes, and for each sub-graph, we assign different relationships between objects based on their class la-\", \"answer\": \"Sub-graphs are created by grouping different objects based on their distance between bounding boxes.\"},\n    {\"question\": \"What are the components used in defining object relationships in an image?\", \"context\": \"To define the relation between objects in a given image, we group different objects based on their distance between the bounding boxes, and for each sub-graph, we assign different relationships between objects based on their class la-\", \"answer\": \"The components used in defining object relationships in an image include the distance between bounding boxes and the class labels of the objects.\"}\n]\n```"}
{"uuid": "2974c710-db73-4a39-9600-38bec3b8a782", "raw_resp": "```json\n[\n    {\"question\": \"What does Table 4 present?\", \"context\": \"bels. Table 4 presents various examples of object relation-ships. To establish relationships like “surrounded by,” we cross-reference pixel-level coordinates to verify if one ob-ject is entirely contained within another object.\", \"answer\": \"Table 4 presents various examples of object relationships, including how to establish relationships such as 'surrounded by' through cross-referencing pixel-level coordinates.\"},\n    {\"question\": \"How do they verify if one object is surrounded by another?\", \"context\": \"bels. Table 4 presents various examples of object relation-ships. To establish relationships like “surrounded by,” we cross-reference pixel-level coordinates to verify if one ob-ject is entirely contained within another object.\", \"answer\": \"They verify if one object is surrounded by another by cross-referencing pixel-level coordinates.\"},\n    {\"question\": \"What method is used to emulate natural language expressions?\", \"context\": \"Expression Generation: To emulate natural language expressions, we employ predefined textual templates based on [40].\", \"answer\": \"Predefined textual templates are employed to emulate natural language expressions.\"},\n    {\"question\": \"On what basis are the predefined textual templates created?\", \"context\": \"Expression Generation: To emulate natural language expressions, we employ predefined textual templates based on [40].\", \"answer\": \"The predefined textual templates are based on [40].\"},\n    {\"question\": \"Which attributes are included in the phrase template?\", \"context\": \"The phrase template encompasses the attributes {a1, . . . , a5 }from Table 3.\", \"answer\": \"The phrase template includes the attributes {a1, . . . , a5} from Table 3.\"},\n    {\"question\": \"How many attributes are mentioned in the phrase template?\", \"context\": \"The phrase template encompasses the attributes {a1, . . . , a5 }from Table 3.\", \"answer\": \"Five attributes are mentioned in the phrase template.\"},\n    {\"question\": \"What is the source of the attributes in the phrase template?\", \"context\": \"The phrase template encompasses the attributes {a1, . . . , a5 }from Table 3.\", \"answer\": \"The attributes in the phrase template are from Table 3.\"},\n    {\"question\": \"What number appears at the end of the given text?\", \"context\": \"6 27836\", \"answer\": \"The number that appears at the end of the given text is 27836.\"}\n]\n```"}
{"uuid": "6ee0362f-2585-43ad-89b9-5d78d19b9576", "raw_resp": "```json\n[\n    {\"question\": \"What is the zero-shot scene classification accuracy of Qwen-VL on the UCMerced dataset?\", \"context\": \"Model UCMerced AID\\nQwen-VL [1] 62.90 52.60\", \"answer\": \"62.90\"},\n    {\"question\": \"What is the zero-shot scene classification accuracy of MiniGPTv2 on the AID dataset?\", \"context\": \"MiniGPTv2 [4] 4.76 12.90\", \"answer\": \"12.90\"},\n    {\"question\": \"What is the zero-shot scene classification accuracy of LLaVA-1.5 on the UCMerced dataset?\", \"context\": \"LLaV A-1.5 [17] 68.00 51.00\", \"answer\": \"68.00\"},\n    {\"question\": \"What is the zero-shot scene classification accuracy of GeoChat on the AID dataset?\", \"context\": \"GeoChat 84.43 72.03\", \"answer\": \"72.03\"},\n    {\"question\": \"How does GeoChat perform compared to other generic VLMs in terms of zero-shot scene classification?\", \"context\": \"Table 5. Zero-shot scene classification accuracy comparison on\\nAID [34] and UCMerced [36] datasets. In comparison to other\\ngeneric VLMs, GeoChat performs favorably well.\", \"answer\": \"GeoChat performs favorably well compared to other generic VLMs.\"},\n    {\"question\": \"What are the attributes that may be absent in the sentence structure formula provided?\", \"context\": \"Attributes that may be absent are enclosed in 〈〉, and at-\\ntributes {a2, a3}can be arranged in any sequence.\", \"answer\": \"{a2, a3}\"},\n    {\"question\": \"How can the attributes {a2, a3} be arranged in the sentence structure formula?\", \"context\": \"Attributes that may be absent are enclosed in 〈〉, and at-\\ntributes {a2, a3}can be arranged in any sequence.\", \"answer\": \"{a2, a3} can be arranged in any sequence.\"},\n    {\"question\": \"What is the sentence structure formula for describing objects of the same class?\", \"context\": \"objects of the same class is formulated as:\\n”The/A ⟨a3⟩ ⟨a2⟩a1⟨in/on the a4⟩.”\", \"answer\": \"”The/A ⟨a3⟩ ⟨a2⟩a1⟨in/on the a4⟩.”\"}\n]\n```"}
{"uuid": "c6f26a2f-9aed-48ad-aa0d-6a7a2ef8f0cd", "raw_resp": "```json\n[\n    {\"question\": \"What can be arranged in any sequence?\", \"context\": \"tributes {a2, a3}can be arranged in any sequence.\", \"answer\": \"tributes {a2, a3}\"},\n    {\"question\": \"What does the sentence template incorporate to establish connections between two objects?\", \"context\": \"Similarly, the sentence template incorporates the rela-tional attributes a5 to establish connections between two objects through this structure:\", \"answer\": \"relational attributes a5\"},\n    {\"question\": \"What is the structure used to establish connections between two objects?\", \"context\": \"”The/A ⟨ai3⟩ ⟨ai2⟩ai1ai5aj1⟨in/on the aj4⟩.”\", \"answer\": \"”The/A ⟨ai3⟩ ⟨ai2⟩ai1ai5aj1⟨in/on the aj4⟩”\"},\n    {\"question\": \"What do the indicies i and j represent?\", \"context\": \"Here, the indicies iandjrepresent the ithandjthobject.\", \"answer\": \"the ith and jth object\"},\n    {\"question\": \"What is mentioned about referring expression datasets in the natural image domain?\", \"context\": \"Visual Grounding: Although referring expression datasets are available in the natural image domain [37, 38], they lack for the remote sensing domain.\", \"answer\": \"they are available\"},\n    {\"question\": \"What is stated about referring expression datasets in the remote sensing domain?\", \"context\": \"Visual Grounding: Although referring expression datasets are available in the natural image domain [37, 38], they lack for the remote sensing domain.\", \"answer\": \"they lack\"},\n    {\"question\": \"What is the purpose of the sentence structure mentioned in the text?\", \"context\": \"Similarly, the sentence template incorporates the rela-tional attributes a5 to establish connections between two objects through this structure:\", \"answer\": \"to establish connections between two objects\"},\n    {\"question\": \"What domain lacks referring expression datasets according to the text?\", \"context\": \"Visual Grounding: Although referring expression datasets are available in the natural image domain [37, 38], they lack for the remote sensing domain.\", \"answer\": \"remote sensing domain\"}\n]\n```"}
{"uuid": "56288e4f-caba-4fca-9af7-c6518459ae87", "raw_resp": "```json\n[\n    {\"question\": \"What are the three kinds of question answering pairs created using short descriptions?\", \"context\": \"use our short descriptions as referring expressions to cre-ate three different kinds of question answering pairs, i.e.grounding image description, referring expression, and re-gion level captioning, as described in Table 1.\", \"answer\": \"The three kinds of question answering pairs are grounding image description, referring expression, and region level captioning.\"},\n    {\"question\": \"Which pretrained model is used to initialize the weights of their model?\", \"context\": \"We initialize the weights of our model with the pretrained CLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA [11] finetuning.\", \"answer\": \"The pretrained model used to initialize the weights is CLIP-ViT.\"},\n    {\"question\": \"What method is applied for finetuning the model?\", \"context\": \"We initialize the weights of our model with the pretrained CLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA [11] finetuning.\", \"answer\": \"LoRA is applied for finetuning the model.\"},\n    {\"question\": \"Which version of LLM is mentioned in the text?\", \"context\": \"We initialize the weights of our model with the pretrained CLIP-ViT [24], and LLM (Vicuna-v1.5 [7] and apply LoRA [11] finetuning.\", \"answer\": \"Vicuna-v1.5 is the version of LLM mentioned in the text.\"},\n    {\"question\": \"What parameters are refined through low-rank adaptation using LoRA?\", \"context\": \"Utilizing LoRA, we refine the parameters WqandWvthrough low-rank adaptation, with a designated\", \"answer\": \"The parameters Wq and Wv are refined through low-rank adaptation using LoRA.\"},\n    {\"question\": \"What section does this information come from?\", \"context\": \"5. Experiments\\n5.1. Implementation Details\", \"answer\": \"This information comes from the 'Implementation Details' section under 'Experiments'.\"},\n    {\"question\": \"What technique is used to create the question answering pairs?\", \"context\": \"use our short descriptions as referring expressions to cre-ate three different kinds of question answering pairs, i.e.grounding image description, referring expression, and re-gion level captioning, as described in Table 1.\", \"answer\": \"Short descriptions are used as referring expressions to create the question answering pairs.\"},\n    {\"question\": \"Where is the detailed description of the three kinds of question answering pairs provided?\", \"context\": \"use our short descriptions as referring expressions to cre-ate three different kinds of question answering pairs, i.e.grounding image description, referring expression, and re-gion level captioning, as described in Table 1.\", \"answer\": \"The detailed description of the three kinds of question answering pairs is provided in Table 1.\"}\n]\n```"}
{"uuid": "3e17f0b1-7136-4312-85db-68f4bed9d275", "raw_resp": "```json\n[\n    {\"question\": \"What was the rankrset set to in the implementation?\", \"context\": \"rankrset to 64 in our implementation.\", \"answer\": \"The rankrset was set to 64 in the implementation.\"},\n    {\"question\": \"At what image resolution did the model undergo training?\", \"context\": \"The model under- goes training consistently at an image resolution of 504 × 504 throughout the whole process.\", \"answer\": \"The model underwent training consistently at an image resolution of 504 × 504.\"},\n    {\"question\": \"What were incorporated into each training step?\", \"context\": \"Each training step incor- porates specifically crafted multi-modal instructional tem- plates designed for a variety of vision-language tasks during the training process.\", \"answer\": \"Specifically crafted multi-modal instructional templates designed for a variety of vision-language tasks were incorporated into each training step.\"},\n    {\"question\": \"Which optimizer and learning rate scheduler were used to train the model?\", \"context\": \"We use AdamW [21] optimizer with a cosine learning rate scheduler to train our model.\", \"answer\": \"AdamW optimizer with a cosine learning rate scheduler was used to train the model.\"},\n    {\"question\": \"What was the global batch size kept at?\", \"context\": \"We keep the global batch size as 144.\", \"answer\": \"The global batch size was kept at 144.\"},\n    {\"question\": \"How many stages was the model trained in?\", \"context\": \"We train our model in two\", \"answer\": \"The model was trained in two stages.\"},\n    {\"question\": \"What kind of tasks were the instructional templates designed for?\", \"context\": \"Each training step incor- porates specifically crafted multi-modal instructional tem- plates designed for a variety of vision-language tasks during the training process.\", \"answer\": \"The instructional templates were designed for a variety of vision-language tasks.\"},\n    {\"question\": \"Was the training process consistent throughout the whole process?\", \"context\": \"The model under- goes training consistently at an image resolution of 504 × 504 throughout the whole process.\", \"answer\": \"Yes, the training process was consistent throughout the whole process.\"}\n]\n```"}
{"uuid": "81bd261d-09f2-4453-929a-9d2496989a29", "raw_resp": "```json\n[\n    {\"question\": \"How many steps are taken during the first stage of training?\", \"context\": \"stages, first, we train using all of our datasets for 1 epoch, correspondingly 2144 steps, followed by stage 2, where we only train on the grounding dataset for 1600 more steps(at 128 batch size).\", \"answer\": \"2144 steps\"},\n    {\"question\": \"What is the batch size used in the second stage of training?\", \"context\": \"stages, first, we train using all of our datasets for 1 epoch, correspondingly 2144 steps, followed by stage 2, where we only train on the grounding dataset for 1600 more steps(at 128 batch size).\", \"answer\": \"128\"},\n    {\"question\": \"How many additional steps are taken in the second stage of training?\", \"context\": \"stages, first, we train using all of our datasets for 1 epoch, correspondingly 2144 steps, followed by stage 2, where we only train on the grounding dataset for 1600 more steps(at 128 batch size).\", \"answer\": \"1600 more steps\"},\n    {\"question\": \"Which datasets were used for evaluating the model for scene classification?\", \"context\": \"Datasets for evaluation. For scene classification, we evaluate our model using AID [34] and UCMerced [36].\", \"answer\": \"AID [34] and UCMerced [36]\"},\n    {\"question\": \"What is the average accuracy of LLaV A-1.5 for scene classification?\", \"context\": \"AIDMethod Presence Comparison Rural/Urban Avg. Accuracy LLaV A-1.5[17] 55.46 68.20 59.00 62.77\", \"answer\": \"62.77\"},\n    {\"question\": \"What is the urban accuracy of Qwen-vl-Chat for scene classification?\", \"context\": \"AIDMethod Presence Comparison Rural/Urban Avg. Accuracy Qwen-vl-Chat [1] 38.57 67.59 61.00 55.35\", \"answer\": \"61.00\"},\n    {\"question\": \"What is the rural accuracy of LLaV A-1.5 for scene classification?\", \"context\": \"AIDMethod Presence Comparison Rural/Urban Avg. Accuracy LLaV A-1.5[17] 55.46 68.20 59.00 62.77\", \"answer\": \"59.00\"},\n    {\"question\": \"What is the presence comparison accuracy of Qwen-vl-Chat for scene classification?\", \"context\": \"AIDMethod Presence Comparison Rural/Urban Avg. Accuracy Qwen-vl-Chat [1] 38.57 67.59 61.00 55.35\", \"answer\": \"67.59\"}\n]\n```"}
{"uuid": "51580020-2258-4d2f-a38e-400da7ab9920", "raw_resp": "```json\n[\n    {\"question\": \"What is the highest score for the 'EasyToHard' model in the VQA task?\", \"context\": \"EasyToHard[39] 90.66 87.49 91.67 89.94\", \"answer\": \"91.67\"},\n    {\"question\": \"Which model achieved 91.17 in one of its scores?\", \"context\": \"RSGPT[12] 91.17 91.70 94.00 92.29\", \"answer\": \"RSGPT\"},\n    {\"question\": \"What is the lowest score for 'RSVQA' in the comparison table?\", \"context\": \"RSVQA[20] 87.47 81.50 90.00 86.32\", \"answer\": \"81.50\"},\n    {\"question\": \"How many scores did 'SHRNet' achieve above 90.00?\", \"context\": \"SHRNet [41] 91.03 90.48 94.00 91.84\", \"answer\": \"3\"},\n    {\"question\": \"Which model had the lowest overall score in the RS-VQA specialized models section?\", \"context\": \"GeoChat 91.09 90.33 94.00 90.70\", \"answer\": \"GeoChat\"},\n    {\"question\": \"What is the second highest score for the 'Bi-Modal' model?\", \"context\": \"Bi-Modal[2] 91.06 91.16 92.66 91.63\", \"answer\": \"91.16\"},\n    {\"question\": \"Which model is evaluated in the zero-shot setting according to [1, 4, 17]?\", \"context\": \"[1, 4, 17] are evaluated in zero-shot setting.\", \"answer\": \"Models referenced by [1, 4, 17]\"},\n    {\"question\": \"What is the highest score listed for any model in the table?\", \"context\": \"EasyToHard[39] 90.66 87.49 91.67 89.94\\nBi-Modal[2] 91.06 91.16 92.66 91.63\\nSHRNet [41] 91.03 90.48 94.00 91.84\\nRSGPT[12] 91.17 91.70 94.00 92.29\\nGeoChat 91.09 90.33 94.00 90.70\", \"answer\": \"94.00\"}\n]\n```"}
{"uuid": "8db34f6a-4819-4a24-9cbe-3d447ab9b24d", "raw_resp": "```json\n[\n    {\"question\": \"How does the model described in the text compare to other zero-shot models?\", \"context\": \"outperforms other zero-shot models and performs competitively to SoTA-supervised models like RSGPT which are specifically fine- tuned on target dataset (while ours is a generic model not specifi- cally finetuned on target dataset).\", \"answer\": \"The model outperforms other zero-shot models and competes with state-of-the-art supervised models that are specifically fine-tuned on the target dataset.\"},\n    {\"question\": \"What is the nature of the model being discussed in terms of its tuning?\", \"context\": \"outperforms other zero-shot models and performs competitively to SoTA-supervised models like RSGPT which are specifically fine- tuned on target dataset (while ours is a generic model not specifi- cally finetuned on target dataset).\", \"answer\": \"The model is a generic model and is not specifically fine-tuned on the target dataset.\"},\n    {\"question\": \"What is the source of the large-scale aerial image collection mentioned in the text?\", \"context\": \"[34] is a large-scale aerial image collection compiled from Google Earth imagery, with 30 classes, such as a river, dense residential area, etc. The images are labeled by specialists in the field of remote sensing image interpretation.\", \"answer\": \"The large-scale aerial image collection is compiled from Google Earth imagery.\"},\n    {\"question\": \"How many classes are included in the large-scale aerial image collection?\", \"context\": \"[34] is a large-scale aerial image collection compiled from Google Earth imagery, with 30 classes, such as a river, dense residential area, etc. The images are labeled by specialists in the field of remote sensing image interpretation.\", \"answer\": \"The collection includes 30 classes.\"},\n    {\"question\": \"Can you provide an example of a class included in the large-scale aerial image collection?\", \"context\": \"[34] is a large-scale aerial image collection compiled from Google Earth imagery, with 30 classes, such as a river, dense residential area, etc. The images are labeled by specialists in the field of remote sensing image interpretation.\", \"answer\": \"An example of a class is a river.\"},\n    {\"question\": \"Who labels the images in the large-scale aerial image collection?\", \"context\": \"[34] is a large-scale aerial image collection compiled from Google Earth imagery, with 30 classes, such as a river, dense residential area, etc. The images are labeled by specialists in the field of remote sensing image interpretation.\", \"answer\": \"The images are labeled by specialists in the field of remote sensing image interpretation.\"},\n    {\"question\": \"What is the specific purpose of the specialists involved in the large-scale aerial image collection?\", \"context\": \"[34] is a large-scale aerial image collection compiled from Google Earth imagery, with 30 classes, such as a river, dense residential area, etc. The images are labeled by specialists in the field of remote sensing image interpretation.\", \"answer\": \"The specialists are involved in labeling the images in the field of remote sensing image interpretation.\"},\n    {\"question\": \"Is the model described specifically fine-tuned on the target dataset?\", \"context\": \"outperforms other zero-shot models and performs competitively to SoTA-supervised models like RSGPT which are specifically fine- tuned on target dataset (while ours is a generic model not specifi- cally finetuned on target dataset).\", \"answer\": \"No, the model is a generic model not specifically fine-tuned on the target dataset.\"}\n]\n```"}
{"uuid": "743e5079-821b-4534-acaa-2689bb5ca548", "raw_resp": "```json\n[\n    {\"question\": \"How many images are in the AID dataset?\", \"context\": \"In total, the AID [34] dataset has 10,000 images within 30 classes.\", \"answer\": \"The AID dataset has 10,000 images.\"},\n    {\"question\": \"How many classes does the AID dataset contain?\", \"context\": \"In total, the AID [34] dataset has 10,000 images within 30 classes.\", \"answer\": \"The AID dataset contains 30 classes.\"},\n    {\"question\": \"What conditions do the images in the AID dataset cover?\", \"context\": \"The images have been taken from different countries as well as different weather conditions.\", \"answer\": \"The images cover different countries and weather conditions.\"},\n    {\"question\": \"What proportion of the AID dataset is used for evaluation?\", \"context\": \"For evaluation, we use a 20% split of the AID [34] dataset.\", \"answer\": \"A 20% split of the AID dataset is used for evaluation.\"},\n    {\"question\": \"How many images and classes are in the UCMerced dataset?\", \"context\": \"UCMerced [36] is a Land Use scene classification dataset, with 2,100 images and 21 classes.\", \"answer\": \"The UCMerced dataset has 2,100 images and 21 classes.\"},\n    {\"question\": \"What is the size of each image in the UCMerced dataset?\", \"context\": \"Each image is of size 256 ×256.\", \"answer\": \"Each image in the UCMerced dataset is of size 256 × 256.\"},\n    {\"question\": \"How is the UCMerced dataset used in the study?\", \"context\": \"We use the whole UCMerced [36] dataset as a zero-shot test set.\", \"answer\": \"The whole UCMerced dataset is used as a zero-shot test set.\"},\n    {\"question\": \"What method is used to interact with the models?\", \"context\": \"We prompt the models with all of the\", \"answer\": \"The models are prompted with all of the provided information.\"}\n]\n```"}
{"uuid": "5b27f9cd-6f1d-4c8a-b117-054b458a7b25", "raw_resp": "```json\n[\n    {\"question\": \"What type of prompts were used for the models?\", \"context\": \"We prompt the models with all of the classes and prompt to classify the image using just one word/phrase. For example, we input a prompt like ”Classify the image within one of the given classes: dense residential area, . . . , school. Answer with one word or short phrase.”\", \"answer\": \"Prompts that ask the models to classify images using a single word or short phrase within given classes.\"},\n    {\"question\": \"How was the zero-shot accuracy calculated?\", \"context\": \"We calculate zero-shot accuracy on both AID and UCMerced.\", \"answer\": \"Zero-shot accuracy was calculated on both the AID and UCMerced datasets.\"},\n    {\"question\": \"Which model showed significant outperformance?\", \"context\": \"GeoChat significantly outperforms other VLM’s with an accuracy of 84.43% on UCMerced [36] and 72.03% on AID [34]\", \"answer\": \"GeoChat significantly outperformed other VLM's.\"},\n    {\"question\": \"What was the accuracy of GeoChat on the UCMerced dataset?\", \"context\": \"GeoChat significantly outperforms other VLM’s with an accuracy of 84.43% on UCMerced [36] and 72.03% on AID [34]\", \"answer\": \"GeoChat achieved an accuracy of 84.43% on the UCMerced dataset.\"},\n    {\"question\": \"What was the accuracy of GeoChat on the AID dataset?\", \"context\": \"GeoChat significantly outperforms other VLM’s with an accuracy of 84.43% on UCMerced [36] and 72.03% on AID [34]\", \"answer\": \"GeoChat achieved an accuracy of 72.03% on the AID dataset.\"},\n    {\"question\": \"Which datasets were used to evaluate the models?\", \"context\": \"We calculate zero-shot accuracy on both AID and UCMerced.\", \"answer\": \"Both AID and UCMerced datasets were used to evaluate the models.\"},\n    {\"question\": \"What is the reference citation for the UCMerced dataset?\", \"context\": \"GeoChat significantly outperforms other VLM’s with an accuracy of 84.43% on UCMerced [36] and 72.03% on AID [34]\", \"answer\": \"The reference citation for the UCMerced dataset is [36].\"},\n    {\"question\": \"What is the reference citation for the AID dataset?\", \"context\": \"GeoChat significantly outperforms other VLM’s with an accuracy of 84.43% on UCMerced [36] and 72.03% on AID [34]\", \"answer\": \"The reference citation for the AID dataset is [34].\"}\n]\n```"}
{"uuid": "2352554c-6dc9-48dc-b229-5e3b502d30e4", "raw_resp": "```json\n[\n    {\"question\": \"What is the accuracy of MiniGPT-4-v2 when using Vicuna-v1.5 to check its output?\", \"context\": \"Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol- low the instructions provided for this specific task and re- turns unrelated classes that are not a part of the dataset. It’s accuracy is close to 5% if we pass the answers from Vicuna- v1.5 [7] and ask it to check if the output sentence refers to the ground truth class or not.\", \"answer\": \"The accuracy of MiniGPT-4-v2 is close to 5% when using Vicuna-v1.5 to check if the output sentence refers to the ground truth class.\"}, \n    {\"question\": \"Which models perform well in instruction following but lack domain knowledge compared to GeoChat?\", \"context\": \"In comparison, Qwen-VL and LLaVa-1.5 perform well in instruction following, but fall short to GeoChat, due to lack of domain knowledge.\", \"answer\": \"Qwen-VL and LLaVa-1.5 perform well in instruction following but lack domain knowledge compared to GeoChat.\"}, \n    {\"question\": \"What issue does MiniGPT-4-v2 have according to the text?\", \"context\": \"Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol- low the instructions provided for this specific task and re- turns unrelated classes that are not a part of the dataset.\", \"answer\": \"MiniGPT-4-v2 fails to follow the instructions provided for the specific task and returns unrelated classes that are not a part of the dataset.\"}, \n    {\"question\": \"How does GeoChat compare to Qwen-VL and LLaVa-1.5?\", \"context\": \"In comparison, Qwen-VL and LLaVa-1.5 perform well in instruction following, but fall short to GeoChat, due to lack of domain knowledge.\", \"answer\": \"GeoChat outperforms Qwen-VL and LLaVa-1.5 due to its domain knowledge.\"}, \n    {\"question\": \"What is the main reason Qwen-VL and LLaVa-1.5 fall short compared to GeoChat?\", \"context\": \"In comparison, Qwen-VL and LLaVa-1.5 perform well in instruction following, but fall short to GeoChat, due to lack of domain knowledge.\", \"answer\": \"The main reason is the lack of domain knowledge.\"}, \n    {\"question\": \"Which model is mentioned as having issues with returning unrelated classes?\", \"context\": \"Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol- low the instructions provided for this specific task and re- turns unrelated classes that are not a part of the dataset.\", \"answer\": \"MiniGPT-4-v2 is mentioned as having issues with returning unrelated classes.\"}, \n    {\"question\": \"What task does MiniGPT-4-v2 fail to perform correctly?\", \"context\": \"Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol- low the instructions provided for this specific task and re- turns unrelated classes that are not a part of the dataset.\", \"answer\": \"MiniGPT-4-v2 fails to follow the instructions provided for the specific task.\"}, \n    {\"question\": \"Which model's performance is checked against the ground truth class using Vicuna-v1.5?\", \"context\": \"Table 5. Notably, the recent MiniGPT-4-v2[4] fails to fol- low the instructions provided for this specific task and re- turns unrelated classes that are not a part of the dataset. It’s accuracy is close to 5% if we pass the answers from Vicuna- v1.5 [7] and ask it to check if the output sentence refers to the ground truth class or not.\", \"answer\": \"MiniGPT-4-v2's performance is checked against the ground truth class using Vicuna-v1.5.\"}\n]\n```"}
{"uuid": "56c2211c-158d-4f9f-b943-9bf6e8daab58", "raw_resp": "```json\n[\n    {\"question\": \"What does RSVQA-HRBEN comprise?\", \"context\": \"RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-vided into training, validation, test 1, and test 2 sets, re-spectively.\", \"answer\": \"10,569 high-resolution photos and 1,066,316 question-answer pairs\"},\n    {\"question\": \"How many question-answer pairs are in RSVQA-HRBEN?\", \"context\": \"RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-vided into training, validation, test 1, and test 2 sets, re-spectively.\", \"answer\": \"1,066,316 question-answer pairs\"},\n    {\"question\": \"What percentage of RSVQA-HRBEN is allocated for training?\", \"context\": \"RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-vided into training, validation, test 1, and test 2 sets, re-spectively.\", \"answer\": \"61.5%\"},\n    {\"question\": \"What percentage of RSVQA-HRBEN is allocated for validation?\", \"context\": \"RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-vided into training, validation, test 1, and test 2 sets, re-spectively.\", \"answer\": \"11.2%\"},\n    {\"question\": \"What percentage of RSVQA-HRBEN is allocated for test 1?\", \"context\": \"RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-vided into training, validation, test 1, and test 2 sets, re-spectively.\", \"answer\": \"20.5%\"},\n    {\"question\": \"What percentage of RSVQA-HRBEN is allocated for test 2?\", \"context\": \"RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-vided into training, validation, test 1, and test 2 sets, re-spectively.\", \"answer\": \"6.8%\"},\n    {\"question\": \"What types of questions does RSVQA-HRBEN have?\", \"context\": \"This dataset has three question types: presence, 7 27837\", \"answer\": \"Three question types: presence, (the text cuts off here)\"},\n    {\"question\": \"How many photos are included in RSVQA-HRBEN?\", \"context\": \"RSVQA-HRBEN [20] comprises 10,569 high-resolution photos and 1,066,316 question-answer pairs, with 61.5%, 11.2%, 20.5%, and 6.8% di-vided into training, validation, test 1, and test 2 sets, re-spectively.\", \"answer\": \"10,569 high-resolution photos\"}\n]\n```"}
{"uuid": "240047c1-0ba5-4a5f-af4e-eb88226410d5", "raw_resp": "```json\n[\n    {\"question\": \"What does the 'Small' classification refer to in Table 7?\", \"context\": \"Small, medium and large refer to the size of the objects based on the bounding box area.\", \"answer\": \"The 'Small' classification refers to the size of the objects based on the bounding box area.\"},\n    {\"question\": \"How does MiniGPTv2 perform on single-object grounding for the medium-sized objects?\", \"context\": \"MiniGPTv2 [4] 1.7 9.9 21.9 9.1 3.6 8.2 2.6 7.6\", \"answer\": \"MiniGPTv2 performs at 9.9 (acc@0.5%) on single-object grounding for medium-sized objects.\"},\n    {\"question\": \"What is the acc@0.5% score for GeoChat on multi-object grounding for large objects?\", \"context\": \"GeoChat 2.9 13.6 21.7 16.0 4.3 10.5 11.8 10.6\", \"answer\": \"The acc@0.5% score for GeoChat on multi-object grounding for large objects is 10.6.\"},\n    {\"question\": \"What does '[refer]' indicate in the table?\", \"context\": \"[refer] : object\", \"answer\": \"'[refer]' indicates an object in the table.\"},\n    {\"question\": \"What is the overall performance of MiniGPTv2 for small objects?\", \"context\": \"MiniGPTv2 [4] 1.7 9.9 21.9 9.1 3.6 8.2 2.6 7.6\", \"answer\": \"The overall performance of MiniGPTv2 for small objects is 7.6 (acc@0.5%).\"},\n    {\"question\": \"For medium-sized objects, what is the multi-object grounding performance of GeoChat?\", \"context\": \"GeoChat 2.9 13.6 21.7 16.0 4.3 10.5 11.8 10.6\", \"answer\": \"The multi-object grounding performance of GeoChat for medium-sized objects is 10.5 (acc@0.5%).\"},\n    {\"question\": \"What is the acc@0.5% score for GeoChat on single-object grounding for small objects?\", \"context\": \"GeoChat 2.9 13.6 21.7 16.0 4.3 10.5 11.8 10.6\", \"answer\": \"The acc@0.5% score for GeoChat on single-object grounding for small objects is 2.9.\"},\n    {\"question\": \"What does 'Multi-object grounding' refer to in the table?\", \"context\": \"Single/multi-object refer to how many objects the question asks the model to predict.\", \"answer\": \"'Multi-object grounding' refers to when the question asks the model to predict multiple objects.\"}\n]\n```"}
{"uuid": "a01e4504-ca5e-414a-b72b-43fcadd41b25", "raw_resp": "```json\n[\n    {\"question\": \"What does '[grounding]' refer to in the given text?\", \"context\": \"referenced using a combination of attributes from a1-a5 in Table 3. [grounding] : objects referenced using a combination of attributes from a1-a5 in Table 3.\", \"answer\": \"Objects referenced using a combination of attributes from a1-a5 in Table 3.\"},\n    {\"question\": \"What is the average accuracy for Qwen-VL[1]?\", \"context\": \"Qwen-VL[1] 66.44 60.41 63.06\", \"answer\": \"63.06\"},\n    {\"question\": \"What is the average accuracy for LLaV A-1.5[17]?\", \"context\": \"LLaV A-1.5[17] 69.83 67.29 68.40\", \"answer\": \"68.40\"},\n    {\"question\": \"What is the average accuracy for MiniGPTv2[4]?\", \"context\": \"MiniGPTv2[4] 40.79 50.91 46.46\", \"answer\": \"46.46\"},\n    {\"question\": \"What is the average accuracy for GeoChat?\", \"context\": \"GeoChat 58.45 83.19 72.30\", \"answer\": \"72.30\"},\n    {\"question\": \"How does GeoChat perform compared to the baseline?\", \"context\": \"Overall, GeoChat outperforms the baseline, but there is still significant room for further improvement on this complex task.\", \"answer\": \"GeoChat outperforms the baseline, but there is still significant room for further improvement.\"},\n    {\"question\": \"Which model has the highest average accuracy?\", \"context\": \"LLaV A-1.5[17] 69.83 67.29 68.40\\nGeoChat 58.45 83.19 72.30\\nQwen-VL[1] 66.44 60.41 63.06\\nMiniGPTv2[4] 40.79 50.91 46.46\", \"answer\": \"LLaV A-1.5[17]\"},\n    {\"question\": \"Which model has the lowest average accuracy?\", \"context\": \"LLaV A-1.5[17] 69.83 67.29 68.40\\nGeoChat 58.45 83.19 72.30\\nQwen-VL[1] 66.44 60.41 63.06\\nMiniGPTv2[4] 40.79 50.91 46.46\", \"answer\": \"MiniGPTv2[4]\"}\n]\n```"}
{"uuid": "37c378f9-8660-4b13-8b66-0d9b68163c46", "raw_resp": "```json\n[\n    {\"question\": \"Which dataset is mentioned for visual QA?\", \"context\": \"HRBEN [20] dataset for visual qa.\", \"answer\": \"HRBEN [20] dataset\"},\n    {\"question\": \"What is the status of the models regarding the target dataset?\", \"context\": \"All models here have not been trained on the target dataset.\", \"answer\": \"All models have not been trained on the target dataset.\"},\n    {\"question\": \"How does GeoChat perform compared to generic VLMs?\", \"context\": \"GeoChat performs favorably well compared to generic VLMs.\", \"answer\": \"GeoChat performs favorably well.\"},\n    {\"question\": \"Which set is used for evaluation and how many question-answer pairs does it contain?\", \"context\": \"For evaluation, we use the test set-2 for RSVQA-HRBEN [20] with 47k question answer pairs.\", \"answer\": \"test set-2 for RSVQA-HRBEN [20] with 47k question-answer pairs\"},\n    {\"question\": \"How many images does RSVQA-LR [20] consist of?\", \"context\": \"RSVQA-LR [20] is made up of 772 low-resolution im- ages\", \"answer\": \"772 low-resolution images\"},\n    {\"question\": \"What is the distribution of RSVQA-LR [20] for training, validation, and testing?\", \"context\": \"77.8%, 11.1%, and 11.1% used for training, validation, and testing, respectively.\", \"answer\": \"77.8% for training, 11.1% for validation, and 11.1% for testing\"},\n    {\"question\": \"How many question-answer pairs are there in RSVQA-LR [20]?\", \"context\": \"RSVQA-LR [20] is made up of 772 low-resolution im- ages and 77,232 question-answer pairs,\", \"answer\": \"77,232 question-answer pairs\"},\n    {\"question\": \"What are the four different categories of questions in the dataset?\", \"context\": \"There are four different categories of questions:\", \"answer\": \"The text does not specify the four categories.\"}\n]\n```"}
{"uuid": "20ffe3c5-f23d-4eb4-bfaf-499a72fc8824", "raw_resp": "```json\n[\n    {\"question\": \"Why were area and count questions omitted during evaluation?\", \"context\": \"We omitted area and count questions during evaluation because the responses are numerical and quantifiable into numerous categories.\", \"answer\": \"Area and count questions were omitted because their responses are numerical and can be quantified into numerous categories.\"},\n    {\"question\": \"What dataset was used as an example for quantifying counting questions?\", \"context\": \"In the RSVQA-LRBEN [20] dataset, for example, counting questions are quantified into five categories: 0, between 1 and 10, between 11 and 100, between 101 and 1000, and greater than 1000.\", \"answer\": \"The RSVQA-LRBEN [20] dataset was used as an example for quantifying counting questions.\"},\n    {\"question\": \"How many categories are used to quantify counting questions in the RSVQA-LRBEN dataset?\", \"context\": \"In the RSVQA-LRBEN [20] dataset, for example, counting questions are quantified into five categories: 0, between 1 and 10, between 11 and 100, between 101 and 1000, and greater than 1000.\", \"answer\": \"Counting questions in the RSVQA-LRBEN dataset are quantified into five categories.\"},\n    {\"question\": \"What are the categories used for quantifying counting questions in the RSVQA-LRBEN dataset?\", \"context\": \"In the RSVQA-LRBEN [20] dataset, for example, counting questions are quantified into five categories: 0, between 1 and 10, between 11 and 100, between 101 and 1000, and greater than 1000.\", \"answer\": \"The categories are 0, between 1 and 10, between 11 and 100, between 101 and 1000, and greater than 1000.\"},\n    {\"question\": \"Which set of the RSVQA-LRBEN dataset was used for evaluation?\", \"context\": \"For evaluation, we use the test set of RSVQA-LRBEN [20] with 7k question-answer pairs.\", \"answer\": \"The test set of RSVQA-LRBEN [20] was used for evaluation.\"},\n    {\"question\": \"How many question-answer pairs are in the test set of the RSVQA-LRBEN dataset?\", \"context\": \"For evaluation, we use the test set of RSVQA-LRBEN [20] with 7k question-answer pairs.\", \"answer\": \"There are 7k question-answer pairs in the test set of the RSVQA-LRBEN dataset.\"},\n    {\"question\": \"What types of questions were excluded from the evaluation process?\", \"context\": \"We omitted area and count questions during evaluation because the responses are numerical and quantifiable into numerous categories.\", \"answer\": \"Area and count questions were excluded from the evaluation process.\"},\n    {\"question\": \"Why might numerical responses pose a challenge for evaluation?\", \"context\": \"We omitted area and count questions during evaluation because the responses are numerical and quantifiable into numerous categories.\", \"answer\": \"Numerical responses can be quantified into numerous categories, which may pose a challenge for evaluation.\"}\n]\n```"}
{"uuid": "f7d0f6a9-deb0-4c17-b77a-7b589d2d8915", "raw_resp": "```json\n[\n    {\"question\": \"What type of prompt was added to the questions to constrain the answers?\", \"context\": \"To constrain the answers to a simple yes/no and for rural/urban question types, we add a suitable prompt at the end of each question.\", \"answer\": \"A suitable prompt was added at the end of each question to constrain the answers to a simple yes/no and for rural/urban question types.\"},\n    {\"question\": \"How does GeoChat perform compared to SOTA specialist models on the RSVQA-LRBEN test set?\", \"context\": \"GeoChat performs close to the SOTA specialist models on RSVQA-LRBEN test set, which is RSGPT [12], finetuned on the target dataset for 5 iterations in comparison.\", \"answer\": \"GeoChat performs close to the state-of-the-art (SOTA) specialist models on the RSVQA-LRBEN test set, specifically when compared to RSGPT finetuned on the target dataset for 5 iterations.\"},\n    {\"question\": \"Which model did GeoChat outperform on the RSVQA-HRBEN test set?\", \"answer\": \"GeoChat outperformed other VLMs in the zero-shot setting on the RSVQA-HRBEN test set.\"},\n    {\"question\": \"By how much did GeoChat improve the average accuracy compared to other VLMs on RSVQA-HRBEN?\", \"context\": \"For RSVQA-HRBEN, GeoChat outperforms other VLM’s in zero-shot setting on average accuracy by 3.9%\", \"answer\": \"GeoChat improved the average accuracy by 3.9% compared to other VLMs on the RSVQA-HRBEN test set.\"},\n    {\"question\": \"What is the specific improvement of GeoChat over LLaVA-v1.5 on the Comparison subset of RSVQA-HRBEN?\", \"context\": \"while beating the Comparison subset by 15.9% on LLaVA-v1.5 [17], as shown in Table 8.\", \"answer\": \"GeoChat beat the Comparison subset by 15.9% compared to LLaVA-v1.5 on the RSVQA-HRBEN test set.\"},\n    {\"question\": \"How many iterations were used to finetune RSGPT on the target dataset?\", \"context\": \"GeoChat performs close to the SOTA specialist models on RSVQA-LRBEN test set, which is RSGPT [12], finetuned on the target dataset for 5 iterations in comparison.\", \"answer\": \"RSGPT was finetuned on the target dataset for 5 iterations.\"},\n    {\"question\": \"What is the name of the test set where GeoChat performed close to SOTA specialist models?\", \"context\": \"GeoChat performs close to the SOTA specialist models on RSVQA-LRBEN test set, which is RSGPT [12], finetuned on the target dataset for 5 iterations in comparison.\", \"answer\": \"The test set where GeoChat performed close to SOTA specialist models is the RSVQA-LRBEN test set.\"},\n    {\"question\": \"Which version of LLaVA was used in the comparison with GeoChat on the RSVQA-HRBEN test set?\", \"context\": \"while beating the Comparison subset by 15.9% on LLaVA-v1.5 [17], as shown in Table 8.\", \"answer\": \"LLaVA-v1.5 was used in the comparison with GeoChat on the RSVQA-HRBEN test set.\"}\n]\n```"}
{"uuid": "2124535a-a4d8-4192-93d9-cc2857e75cd4", "raw_resp": "```json\n[\n    {\"question\": \"What is the percentage mentioned on LLaV A-v1.5?\", \"context\": \"15.9% on LLaV A-v1.5 [17], as shown in Table 8.\", \"answer\": \"15.9%\"},\n    {\"question\": \"Which section does the text mention about Visual Grounding?\", \"context\": \"5.4. Visual Grounding\", \"answer\": \"5.4\"},\n    {\"question\": \"How many referring questions are included in the proposed benchmark?\", \"context\": \"We propose a new benchmark that contains different referring and grounding tasks. We use the validation set from [31] and used the same dataset creation pipeline as in Sec. 4 to construct the test benchmark. There are a total of 7593 [refer], 560 [grounding], and 495 grounding description questions, as well as 2793 for region captioning.\", \"answer\": \"7593\"},\n    {\"question\": \"How many grounding questions are included in the proposed benchmark?\", \"context\": \"We propose a new benchmark that contains different referring and grounding tasks. We use the validation set from [31] and used the same dataset creation pipeline as in Sec. 4 to construct the test benchmark. There are a total of 7593 [refer], 560 [grounding], and 495 grounding description questions, as well as 2793 for region captioning.\", \"answer\": \"560\"},\n    {\"question\": \"How many grounding description questions are included in the proposed benchmark?\", \"context\": \"We propose a new benchmark that contains different referring and grounding tasks. We use the validation set from [31] and used the same dataset creation pipeline as in Sec. 4 to construct the test benchmark. There are a total of 7593 [refer], 560 [grounding], and 495 grounding description questions, as well as 2793 for region captioning.\", \"answer\": \"495\"},\n    {\"question\": \"How many questions are included for region captioning in the proposed benchmark?\", \"context\": \"We propose a new benchmark that contains different referring and grounding tasks. We use the validation set from [31] and used the same dataset creation pipeline as in Sec. 4 to construct the test benchmark. There are a total of 7593 [refer], 560 [grounding], and 495 grounding description questions, as well as 2793 for region captioning.\", \"answer\": \"2793\"},\n    {\"question\": \"From where did the authors use the validation set for the benchmark construction?\", \"context\": \"For the evaluation of grounding tasks, we propose a new benchmark that contains different referring and grounding tasks. We use the validation set from [31] and used the same dataset creation pipeline as in Sec. 4 to construct the test benchmark.\", \"answer\": \"from [31]\"},\n    {\"question\": \"What pipeline was used to construct the test benchmark?\", \"context\": \"For the evaluation of grounding tasks, we propose a new benchmark that contains different referring and grounding tasks. We use the validation set from [31] and used the same dataset creation pipeline as in Sec. 4 to construct the test benchmark.\", \"answer\": \"the same dataset creation pipeline as in Sec. 4\"}\n]\n```"}
{"uuid": "1ce24cb0-e65f-4e94-bb50-2741f4dfa0bc", "raw_resp": "```json\n[\n    {\"question\": \"What evaluation metric is used in the study?\", \"context\": \"We use accuracy@0.5 as the evaluation metric. Accuracy is calculated if the predicted box has an overlap of more thanModel acc@0.5 acc@.25 METEOR\", \"answer\": \"accuracy@0.5\"},\n    {\"question\": \"What is the definition of accuracy in the context of the evaluation metric?\", \"context\": \"Accuracy is calculated if the predicted box has an overlap of more thanModel acc@0.5 acc@.25 METEOR\", \"answer\": \"Accuracy is calculated if the predicted box has an overlap of more than 0.5 IoU with the ground-truth box.\"},\n    {\"question\": \"What is the accuracy@0.5 score for MiniGPTv2?\", \"context\": \"MiniGPTv2[4] 10.8 30.9 16.4\", \"answer\": \"10.8\"},\n    {\"question\": \"What is the METEOR score for MiniGPTv2 in the grounding description task?\", \"context\": \"MiniGPTv2[4] 10.8 30.9 16.4\", \"answer\": \"16.4\"},\n    {\"question\": \"What is the ROUGE-1 score for GeoChat in the region level captioning performance?\", \"context\": \"GeoChat 87.3 87.2 83.9\", \"answer\": \"87.3\"},\n    {\"question\": \"What is the ROUGE-L score for GeoChat in the region level captioning performance?\", \"context\": \"GeoChat 87.3 87.2 83.9\", \"answer\": \"87.2\"},\n    {\"question\": \"What is the METEOR score for GeoChat in the region level captioning performance?\", \"context\": \"GeoChat 87.3 87.2 83.9\", \"answer\": \"83.9\"},\n    {\"question\": \"Which table shows the performance of the method?\", \"context\": \"Results. Table 7 shows the performance of our method\", \"answer\": \"Table 7\"}\n]\n```"}
{"uuid": "58eff65a-541e-4bf4-8074-b921545dd641", "raw_resp": "```json\n[\n    {\"question\": \"Which model performs better on medium size images?\", \"context\": \"Compared to MiniGPT-4-v2[4], our model works better on medium size images.\", \"answer\": \"The proposed model works better on medium size images compared to MiniGPT-4-v2.\"},\n    {\"question\": \"How does the performance of the models vary with object size?\", \"question\": \"Overall, the model performance is low on small objects or when it has to predict multiple boxes.\", \"answer\": \"Model performance is generally low on small objects or when predicting multiple boxes.\"},\n    {\"question\": \"What metric is used to evaluate the grounding description task?\", \"context\": \"On the grounding description task, we calculate both, the IoU for the multiple bounding boxes generated as well as the text answer generated.\", \"answer\": \"IoU for multiple bounding boxes and the text answer are used to evaluate the grounding description task.\"},\n    {\"question\": \"Which model offers a better description in the grounding description task?\", \"context\": \"Our model provides a better description with slightly better box accuracy than MiniGPT-4-v2\", \"answer\": \"The proposed model provides a better description with slightly better box accuracy.\"},\n    {\"question\": \"How does the proposed model compare to MiniGPT-4-v2 in terms of box accuracy?\", \"context\": \"Our model provides a better description with slightly better box accuracy than MiniGPT-4-v2\", \"answer\": \"The proposed model has slightly better box accuracy than MiniGPT-4-v2.\"},\n    {\"question\": \"What aspects of the model's performance are mentioned in the comparison with MiniGPT-4-v2?\", \"context\": \"Overall, the model performance is low on small objects or when it has to predict multiple boxes. Compared to MiniGPT-4-v2[4], our model works better on medium size images. On the grounding description task, we calculate both, the IoU for the multiple bounding boxes generated as well as the text answer generated. Our model provides a better description with slightly better box accuracy than MiniGPT-4-v2\", \"answer\": \"The aspects mentioned include performance on small objects, prediction of multiple boxes, performance on medium-sized images, and grounding description task metrics such as IoU and text answers.\"},\n    {\"question\": \"What is the overall trend in model performance across different object sizes?\", \"context\": \"Overall, the model performance is low on small objects or when it has to predict multiple boxes.\", \"answer\": \"The overall trend shows low performance on small objects and when predicting multiple boxes.\"},\n    {\"question\": \"What tasks are evaluated in the comparison between the proposed model and MiniGPT-4-v2?\", \"context\": \"On the grounding description task, we calculate both, the IoU for the multiple bounding boxes generated as well as the text answer generated.\", \"answer\": \"The grounding description task is evaluated, including the calculation of IoU for multiple bounding boxes and the text answer.\"}\n]\n```"}
{"uuid": "95dfeb25-0568-457a-b7bc-9553ce1f5274", "raw_resp": "```json\n[\n    {\"question\": \"What was the basis for evaluating the models in region-level captioning?\", \"context\": \"(Table 9). As for region-level captioning, we evaluate both models based on the text accuracy with ground truth region-level captions (Table 10).\", \"answer\": \"The evaluation was based on the text accuracy with ground truth region-level captions.\"},\n    {\"question\": \"Which metrics were used to compare the models' performance in region-level captioning?\", \"context\": \"Our model significantly outperforms MiniGPT-4-v2 in terms of ROUGE and ME- TEOR score.\", \"answer\": \"ROUGE and METEOR scores were used to compare the models.\"},\n    {\"question\": \"How did the authors' model perform compared to MiniGPT-4-v2?\", \"context\": \"Our model significantly outperforms MiniGPT-4-v2 in terms of ROUGE and ME- TEOR score.\", \"answer\": \"The authors' model significantly outperformed MiniGPT-4-v2 in terms of ROUGE and METEOR scores.\"},\n    {\"question\": \"What domain are the recent advancements in large Vision-Language Models (VLMs) showing promise in?\", \"context\": \"Although recent advancements in large Vision-Language Models (VLMs) have shown promise in nature image do- mains, their performance in Remote Sensing (RS) scenarios is still limited due to the unique domain-specific chal-\", \"answer\": \"Recent advancements in large Vision-Language Models (VLMs) have shown promise in nature image domains.\"},\n    {\"question\": \"Why is the performance of VLMs in Remote Sensing scenarios still limited?\", \"context\": \"Although recent advancements in large Vision-Language Models (VLMs) have shown promise in nature image do- mains, their performance in Remote Sensing (RS) scenarios is still limited due to the unique domain-specific chal-\", \"answer\": \"The performance is limited due to the unique domain-specific challenges in Remote Sensing scenarios.\"},\n    {\"question\": \"What section does the provided text come from?\", \"context\": \"6. Conclusion\", \"answer\": \"The provided text comes from the Conclusion section.\"},\n    {\"question\": \"What is the abbreviation 'VLMs' stand for?\", \"context\": \"Although recent advancements in large Vision-Language Models (VLMs) have shown promise in nature image do- mains, their performance in Remote Sensing (RS) scenarios is still limited due to the unique domain-specific chal-\", \"answer\": \"VLMs stands for Vision-Language Models.\"},\n    {\"question\": \"What is the abbreviation 'RS' stand for?\", \"context\": \"Although recent advancements in large Vision-Language Models (VLMs) have shown promise in nature image do- mains, their performance in Remote Sensing (RS) scenarios is still limited due to the unique domain-specific chal-\", \"answer\": \"RS stands for Remote Sensing.\"}\n]\n```"}
{"uuid": "eaaff66d-e614-4a89-ace1-eeeee146d1cf", "raw_resp": "```json\n[\n    {\"question\": \"What is GeoChat?\", \"context\": \"lenges. Addressing this gap, we present GeoChat, the first unified remote sensing VLM that excels in multitask conversational capabilities with high-resolution RS images.\", \"answer\": \"GeoChat is the first unified remote sensing VLM that excels in multitask conversational capabilities with high-resolution RS images.\"},\n    {\"question\": \"What type of queries does GeoChat handle?\", \"context\": \"GeoChat not only answers image-level queries but also en-gages in region-specific dialogue, grounding responses with precise spatial coordinates.\", \"answer\": \"GeoChat handles image-level queries and region-specific dialogue, grounding responses with precise spatial coordinates.\"},\n    {\"question\": \"What does GeoChat provide in its responses?\", \"context\": \"GeoChat not only answers image-level queries but also en-gages in region-specific dialogue, grounding responses with precise spatial coordinates.\", \"answer\": \"GeoChat grounds its responses with precise spatial coordinates.\"},\n    {\"question\": \"What dataset was created for GeoChat?\", \"context\": \"We create a novel RS mul-timodal instruction-following dataset comprising of 318k image-instruction pairs with a diverse multitask format.\", \"answer\": \"A novel RS multimodal instruction-following dataset comprising 318k image-instruction pairs with a diverse multitask format was created for GeoChat.\"},\n    {\"question\": \"How many image-instruction pairs are in the dataset?\", \"context\": \"We create a novel RS mul-timodal instruction-following dataset comprising of 318k image-instruction pairs with a diverse multitask format.\", \"answer\": \"The dataset comprises 318k image-instruction pairs.\"},\n    {\"question\": \"What is the format of the dataset?\", \"context\": \"We create a novel RS mul-timodal instruction-following dataset comprising of 318k image-instruction pairs with a diverse multitask format.\", \"answer\": \"The dataset has a diverse multitask format.\"},\n    {\"question\": \"What is the purpose of GeoChat?\", \"context\": \"Addressing this gap, we present GeoChat, the first unified remote sensing VLM that excels in multitask conversational capabilities with high-resolution RS images.\", \"answer\": \"The purpose of GeoChat is to address the gap in remote sensing by providing multitask conversational capabilities with high-resolution RS images.\"},\n    {\"question\": \"What kind of remote sensing model is GeoChat?\", \"context\": \"Addressing this gap, we present GeoChat, the first unified remote sensing VLM that excels in multitask conversational capabilities with high-resolution RS images.\", \"answer\": \"GeoChat is a unified remote sensing VLM.\"}\n]\n```"}
{"uuid": "62bea293-9d52-4aea-a37d-eb283b5b2bd0", "raw_resp": "```json\n[\n    {\"question\": \"What does GeoChat achieve in terms of performance?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"GeoChat achieves robust zero-shot performance across various RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection.\"},\n    {\"question\": \"What are the specific remote sensing tasks where GeoChat demonstrates its capabilities?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"GeoChat demonstrates its capabilities in scene classification, VQA, multi-turn dialogue, visual grounding, and referring object detection.\"},\n    {\"question\": \"How does GeoChat contribute to the field of remote sensing?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"GeoChat contributes by establishing a comprehensive benchmark through its robust zero-shot performance across various RS tasks.\"},\n    {\"question\": \"What is the term used to describe GeoChat's performance?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"The term used to describe GeoChat's performance is 'robust zero-shot performance'.\"},\n    {\"question\": \"What is established as a result of GeoChat's performance?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"A comprehensive benchmark is established as a result of GeoChat's performance.\"},\n    {\"question\": \"Is GeoChat designed to handle multiple types of remote sensing tasks?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"Yes, GeoChat is designed to handle multiple types of remote sensing tasks.\"},\n    {\"question\": \"Does GeoChat perform well across different RS tasks?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"Yes, GeoChat performs well across different RS tasks, achieving robust zero-shot performance.\"},\n    {\"question\": \"What kind of dialogue capability does GeoChat have?\", \"context\": \"GeoChat achieves robust zero-shot performance across var-ious RS tasks including scene classification, VQA, multi-turn dialogue, visual grounding and referring object detection, thus establishing a comprehensive benchmark.\", \"answer\": \"GeoChat has the capability for multi-turn dialogue.\"}\n]\n```"}
{"uuid": "9d89dea7-13f9-4c5a-b4a8-86bf591cdd66", "raw_resp": "```json\n[\n    {\"question\": \"Who are the authors of Qwen-VL?\", \"context\": \"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7, 8\", \"answer\": \"The authors of Qwen-VL are Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.\"},\n    {\"question\": \"What is the title of the research by Jinze Bai et al.?\", \"context\": \"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7, 8\", \"answer\": \"The title of the research is 'Qwen-vl: A frontier large vision-language model with versatile abilities'.\"},\n    {\"question\": \"Where was the paper on Qwen-VL published?\", \"context\": \"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7, 8\", \"answer\": \"The paper on Qwen-VL was published as an arXiv preprint with the identifier arXiv:2308.12966.\"},\n    {\"question\": \"In what year was the Qwen-VL paper published?\", \"context\": \"Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966 , 2023. 7, 8\", \"answer\": \"The Qwen-VL paper was published in 2023.\"},\n    {\"question\": \"Who are the authors of the paper on the bi-modal transformer-based approach?\", \"context\": \"Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mohamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and Farid Melgani. Bi-modal transformer-based approach for visual question answering in remote sensing imagery. IEEE\", \"answer\": \"The authors of the paper on the bi-modal transformer-based approach are Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mohamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and Farid Melgani.\"},\n    {\"question\": \"What is the title of the research by Yakoub Bazi et al.?\", \"context\": \"Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mohamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and Farid Melgani. Bi-modal transformer-based approach for visual question answering in remote sensing imagery. IEEE\", \"answer\": \"The title of the research is 'Bi-modal transformer-based approach for visual question answering in remote sensing imagery'.\"},\n    {\"question\": \"Which journal published the paper by Yakoub Bazi et al.?\", \"context\": \"Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mohamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and Farid Melgani. Bi-modal transformer-based approach for visual question answering in remote sensing imagery. IEEE\", \"answer\": \"The paper by Yakoub Bazi et al. was published in IEEE.\"},\n    {\"question\": \"What field does the paper by Yakoub Bazi et al. focus on?\", \"context\": \"Yakoub Bazi, Mohamad Mahmoud Al Rahhal, Mohamed Lamine Mekhalfi, Mansour Abdulaziz Al Zuair, and Farid Melgani. Bi-modal transformer-based approach for visual question answering in remote sensing imagery. IEEE\", \"answer\": \"The paper focuses on visual question answering in remote sensing imagery using a bi-modal transformer-based approach.\"}\n]\n```"}
{"uuid": "2fe8a064-ddde-45de-96ff-51dc00c9b662", "raw_resp": "```json\n[\n    {\"question\": \"Who are the authors of the paper titled 'Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering'?\", \"context\": \"Christel Chappuis, Val ´erie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1372–1381, 2022. 2\", \"answer\": \"The authors are Christel Chappuis, Valérie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia.\"},\n    {\"question\": \"In which conference was the paper 'Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering' published?\", \"context\": \"Christel Chappuis, Val ´erie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1372–1381, 2022. 2\", \"answer\": \"It was published in the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"},\n    {\"question\": \"What is the title of the paper authored by Christel Chappuis, Valérie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia?\", \"context\": \"Christel Chappuis, Val ´erie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1372–1381, 2022. 2\", \"answer\": \"The title of the paper is 'Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering'.\"},\n    {\"question\": \"What are the page numbers for the paper 'Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering'?\", \"context\": \"Christel Chappuis, Val ´erie Zermatten, Sylvain Lobry, Bertrand Le Saux, and Devis Tuia. Prompt-rsvqa: Prompting visual context to a language model for remote sensing visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 1372–1381, 2022. 2\", \"answer\": \"The paper appears on pages 1372–1381.\"},\n    {\"question\": \"Which journal published the paper 'Transactions on Geoscience and Remote Sensing'?\", \"context\": \"Transactions on Geoscience and Remote Sensing , 60:1–11, 2022. 7\", \"answer\": \"The paper was published in Transactions on Geoscience and Remote Sensing.\"},\n    {\"question\": \"What is the volume and issue number of the journal Transactions on Geoscience and Remote Sensing that published the mentioned paper?\", \"context\": \"Transactions on Geoscience and Remote Sensing , 60:1–11, 2022. 7\", \"answer\": \"The volume and issue number is 60:1–11.\"},\n    {\"question\": \"In which year was the paper 'Transactions on Geoscience and Remote Sensing' published?\", \"context\": \"Transactions on Geoscience and Remote Sensing , 60:1–11, 2022. 7\", \"answer\": \"The paper was published in 2022.\"},\n    {\"question\": \"What is the page number for the paper 'Transactions on Geoscience and Remote Sensing'?\", \"context\": \"Transactions on Geoscience and Remote Sensing , 60:1–11, 2022. 7\", \"answer\": \"The paper spans from page 1 to page 11.\"}\n]\n```"}
{"uuid": "eedade6d-d3fd-4bc0-8de0-6a2e55569d61", "raw_resp": "```json\n[\n    {\"question\": \"Who are the authors of Minigpt-v2?\", \"context\": \"Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 , 2023. 7, 8\", \"answer\": \"The authors of Minigpt-v2 are Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\"},\n    {\"question\": \"What is the title of the research by Liu et al.?\", \"context\": \"Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 , 2023. 7, 8\", \"answer\": \"The title of the research is 'Minigpt-v2: large language model as a unified interface for vision-language multi-task learning'.\"},\n    {\"question\": \"Where was the research on Minigpt-v2 published?\", \"context\": \"Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 , 2023. 7, 8\", \"answer\": \"The research on Minigpt-v2 was published as an arXiv preprint, arXiv:2310.09478, in 2023.\"},\n    {\"question\": \"Which journal did Gong Cheng, Junwei Han, and Xiaoqiang Lu publish their work on remote sensing image scene classification?\", \"context\": \"Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens- ing image scene classification: Benchmark and state of the art.Proceedings of the IEEE , 105(10):1865–1883, 2017. 2, 3, 5\", \"answer\": \"Their work was published in Proceedings of the IEEE.\"},\n    {\"question\": \"What is the title of the paper by Gong Cheng, Junwei Han, and Xiaoqiang Lu?\", \"context\": \"Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens- ing image scene classification: Benchmark and state of the art.Proceedings of the IEEE , 105(10):1865–1883, 2017. 2, 3, 5\", \"answer\": \"The title of the paper is 'Remote sensing image scene classification: Benchmark and state of the art'.\"},\n    {\"question\": \"In which year was the paper on remote sensing image scene classification published?\", \"context\": \"Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens- ing image scene classification: Benchmark and state of the art.Proceedings of the IEEE , 105(10):1865–1883, 2017. 2, 3, 5\", \"answer\": \"The paper was published in 2017.\"},\n    {\"question\": \"What is the page range of the article on remote sensing image scene classification?\", \"context\": \"Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens- ing image scene classification: Benchmark and state of the art.Proceedings of the IEEE , 105(10):1865–1883, 2017. 2, 3, 5\", \"answer\": \"The article spans pages 1865 to 1883.\"},\n    {\"question\": \"Who are the co-authors of the paper on remote sensing image scene classification?\", \"context\": \"Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sens- ing image scene classification: Benchmark and state of the art.Proceedings of the IEEE , 105(10):1865–1883, 2017. 2, 3, 5\", \"answer\": \"The co-authors of the paper are Gong Cheng, Junwei Han, and Xiaoqiang Lu.\"}\n]\n```"}
{"uuid": "70faf861-235e-41a5-a6cb-25064b5ec1d8", "raw_resp": "```json\n[]\n```\n\nThe provided text appears to be a fragment of a bibliography or reference list, containing citation information for academic papers. It does not contain substantial content from which meaningful questions can be generated. Therefore, an empty array is returned."}
{"uuid": "f61d89ab-02c9-454e-8842-2e78a2520e27", "raw_resp": "```json\n[]\n``` \n\nThe provided text appears to be a segment from a bibliography or reference list, containing author names, publication titles, and years. It does not contain substantive information that can be used to generate meaningful questions and answers."}
{"uuid": "c6345f79-af83-444c-9873-66351f7252c9", "raw_resp": "```json\n[\n    {\"question\": \"In which conference was the LoRA paper presented?\", \"context\": \"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. 2, 4, 5, 7\", \"answer\": \"International Conference on Learning Representations\"},\n    {\"question\": \"Who are the authors of the LoRA paper?\", \"context\": \"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. 2, 4, 5, 7\", \"answer\": \"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen\"},\n    {\"question\": \"What is the publication year of the LoRA paper?\", \"context\": \"Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations , 2022. 2, 4, 5, 7\", \"answer\": \"2022\"},\n    {\"question\": \"Which paper introduced Gaussian Error Linear Units (GeLUs)?\", \"context\": \"Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\", \"answer\": \"Gaussian error linear units (gelus) by Dan Hendrycks and Kevin Gimpel\"},\n    {\"question\": \"In what year was the GeLUs paper published?\", \"context\": \"Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\", \"answer\": \"2016\"},\n    {\"question\": \"Where was the GeLUs paper published?\", \"context\": \"Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\", \"answer\": \"arXiv preprint arXiv:1606.08415\"},\n    {\"question\": \"Who are the authors of the GeLUs paper?\", \"context\": \"Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016. 4\", \"answer\": \"Dan Hendrycks and Kevin Gimpel\"},\n    {\"question\": \"What is the title of the paper by Yuan Hu et al.?\", \"context\": \"Yuan Hu, Jianlong Yuan, Congcong Wen, Xiaonan Lu, and\", \"answer\": \"The provided text does not include the title of the paper by Yuan Hu et al.\"}\n]\n```"}
{"uuid": "4fb0f285-bf87-4329-af36-26198b987bc8", "raw_resp": "```json\n[\n    {\"question\": \"Who is the author of Rsgpt?\", \"context\": \"Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1, 2, 7, 8\", \"answer\": \"Xiang Li\"},\n    {\"question\": \"What is the title of the work by Xiang Li?\", \"context\": \"Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1, 2, 7, 8\", \"answer\": \"Rsgpt: A remote sensing vision language modeland benchmark\"},\n    {\"question\": \"Where was the work Rsgpt published?\", \"context\": \"Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1, 2, 7, 8\", \"answer\": \"arXiv preprint arXiv:2307.15266\"},\n    {\"question\": \"In what year was Rsgpt published?\", \"context\": \"Xiang Li. Rsgpt: A remote sensing vision language modeland benchmark. arXiv preprint arXiv:2307.15266 , 2023. 1, 2, 7, 8\", \"answer\": \"2023\"},\n    {\"question\": \"Who are the authors of Otter?\", \"context\": \"[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023. 2\", \"answer\": \"Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu\"},\n    {\"question\": \"What is the title of the work by Bo Li et al.?\", \"context\": \"[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023. 2\", \"answer\": \"Otter: A multi-modal model with in-context instruction tuning\"},\n    {\"question\": \"Where was the work Otter published?\", \"context\": \"[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023. 2\", \"answer\": \"arXiv preprint arXiv:2305.03726\"},\n    {\"question\": \"In what year was Otter published?\", \"context\": \"[13] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 , 2023. 2\", \"answer\": \"2023\"}\n]\n```"}
{"uuid": "e1592506-175f-48a4-b396-2c6ee34f6f6f", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the preprint by the author discussing a vision assistant for biomedicine?\", \"context\": \"and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890 , 2023.\", \"answer\": \"The title of the preprint is 'and-vision assistant for biomedicine in one day'\"},\n    {\"question\": \"What is the reference number for the paper that discusses a vision assistant for biomedicine?\", \"context\": \"and-vision assistant for biomedicine in one day. arXiv preprint arXiv:2306.00890 , 2023. 2\", \"answer\": \"The reference number is 2\"},\n    {\"question\": \"Who are the authors of the paper titled 'Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation'?\", \"context\": \"[15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uni- fied vision-language understanding and generation. In International Conference on Machine Learning , pages 12888– 12900. PMLR, 2022. 5\", \"answer\": \"The authors are Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi\"},\n    {\"question\": \"Where was the paper 'Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation' published?\", \"context\": \"[15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uni- fied vision-language understanding and generation. In International Conference on Machine Learning , pages 12888– 12900. PMLR, 2022. 5\", \"answer\": \"It was published in the International Conference on Machine Learning (ICML)\"},\n    {\"question\": \"What is the page range of the paper 'Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation'?\", \"context\": \"[15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uni- fied vision-language understanding and generation. In International Conference on Machine Learning , pages 12888– 12900. PMLR, 2022. 5\", \"answer\": \"The page range is from 12888 to 12900\"},\n    {\"question\": \"What is the reference number for the paper that discusses bootstrapping language-image pre-training?\", \"context\": \"[15] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uni- fied vision-language understanding and generation. In International Conference on Machine Learning , pages 12888– 12900. PMLR, 2022. 5\", \"answer\": \"The reference number is 15\"},\n    {\"question\": \"Who are the authors of the paper titled 'Rs-clip: Zero shot remote sensing scene classification via contrastive'?\", \"context\": \"[16] Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou. Rs-clip: Zero shot remote sensing scene classification via contrastive\", \"answer\": \"The authors are Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou\"},\n    {\"question\": \"What is the title of the paper authored by Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou?\", \"context\": \"[16] Xiang Li, Congcong Wen, Yuan Hu, and Nan Zhou. Rs-clip: Zero shot remote sensing scene classification via contrastive\", \"answer\": \"The title of the paper is 'Rs-clip: Zero shot remote sensing scene classification via contrastive'\"}\n]\n```"}
{"uuid": "9588d12d-a7c6-49db-a06e-02092a4ae1a7", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the publication by Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee in 2023?\", \"context\": \"Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\", \"answer\": \"Improved baselines with visual instruction tuning\"},\n    {\"question\": \"In which journal was the paper about vision-language supervision published?\", \"context\": \"vision-language supervision. International Journal of Ap- plied Earth Observation and Geoinformation , 124:103497, 2023. 2\", \"answer\": \"International Journal of Applied Earth Observation and Geoinformation\"},\n    {\"question\": \"What is the arXiv preprint number of the paper 'Improved baselines with visual instruction tuning'?\", \"context\": \"Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744 , 2023. 2, 3, 5, 7, 8\", \"answer\": \"arXiv:2310.03744\"},\n    {\"question\": \"Who are the authors of the paper 'Visual instruction tuning' from 2023?\", \"context\": \"Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485 , 2023. 2\", \"answer\": \"Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee\"},\n    {\"question\": \"What is the arXiv preprint number of the paper 'Visual instruction tuning'?\", \"context\": \"Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485 , 2023. 2\", \"answer\": \"arXiv:2304.08485\"},\n    {\"question\": \"Which year was the paper 'vision-language supervision' published?\", \"context\": \"vision-language supervision. International Journal of Ap- plied Earth Observation and Geoinformation , 124:103497, 2023. 2\", \"answer\": \"2023\"},\n    {\"question\": \"What is the volume number of the journal where the paper 'vision-language supervision' was published?\", \"context\": \"vision-language supervision. International Journal of Ap- plied Earth Observation and Geoinformation , 124:103497, 2023. 2\", \"answer\": \"124\"},\n    {\"question\": \"What is the article number of the paper 'vision-language supervision' in the International Journal of Applied Earth Observation and Geoinformation?\", \"context\": \"vision-language supervision. International Journal of Ap- plied Earth Observation and Geoinformation , 124:103497, 2023. 2\", \"answer\": \"103497\"}\n]\n```"}
{"uuid": "b75373ce-9d75-4d18-aaaf-10589779bedf", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the work by Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia?\", \"context\": \"Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. Rsvqa: Visual question answering for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing , 58 (12):8555–8566, 2020. 2, 3, 5, 7, 8\", \"answer\": \"Rsvqa: Visual question answering for remote sensing data\"},\n    {\"question\": \"In which journal was the research by Sylvain Lobry and his colleagues published?\", \"context\": \"Visual instruction tuning, 2023. 1, 2\\n[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. Rsvqa: Visual question answering for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing , 58 (12):8555–8566, 2020. 2, 3, 5, 7, 8\", \"answer\": \"IEEE Transactions on Geoscience and Remote Sensing\"},\n    {\"question\": \"Who are the authors of the paper 'Decoupled weight decay regularization'?\", \"context\": \"[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017. 7\", \"answer\": \"Ilya Loshchilov and Frank Hutter\"},\n    {\"question\": \"What is the publication year of the paper 'Decoupled weight decay regularization'?\", \"context\": \"[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 , 2017. 7\", \"answer\": \"2017\"},\n    {\"question\": \"Which authors are associated with the work titled 'Video-chatgpt: Towards detailed video'? \", \"context\": \"[22] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fhad Shahbaz Khan. Video-chatgpt: Towards detailed video\", \"answer\": \"Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan\"},\n    {\"question\": \"In what year was the 'Visual instruction tuning' work mentioned?\", \"context\": \"Visual instruction tuning, 2023. 1, 2\", \"answer\": \"2023\"},\n    {\"question\": \"What is the page range of the article by Sylvain Lobry et al. in the IEEE Transactions on Geoscience and Remote Sensing?\", \"context\": \"[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. Rsvqa: Visual question answering for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing , 58 (12):8555–8566, 2020. 2, 3, 5, 7, 8\", \"answer\": \"8555–8566\"},\n    {\"question\": \"Which volume and issue number of the IEEE Transactions on Geoscience and Remote Sensing contains the article by Sylvain Lobry et al.?\", \"context\": \"[20] Sylvain Lobry, Diego Marcos, Jesse Murray, and Devis Tuia. Rsvqa: Visual question answering for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing , 58 (12):8555–8566, 2020. 2, 3, 5, 7, 8\", \"answer\": \"Volume 58, Issue 12\"}\n]\n```"}
{"uuid": "e253846a-71de-4252-b554-4da123b3a98e", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the preprint mentioned in the first line?\", \"context\": \"understanding via large vision and language models. arXiv preprint arXiv:2306.05424 , 2023. 2\", \"answer\": \"Understanding via large vision and language models\"},\n    {\"question\": \"What year was the arXiv preprint published?\", \"context\": \"understanding via large vision and language models. arXiv preprint arXiv:2306.05424 , 2023. 2\", \"answer\": \"2023\"},\n    {\"question\": \"Who published GPT-4's technical report?\", \"context\": \"[23] OpenAI. Gpt-4 technical report, 2023. 1\", \"answer\": \"OpenAI\"},\n    {\"question\": \"In which conference was the paper by Alec Radford and colleagues presented?\", \"context\": \"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning , pages 8748–8763. PMLR, 2021. 7\", \"answer\": \"International conference on machine learning\"},\n    {\"question\": \"What is the name of the conference where the paper 'Learning transferable visual models from natural language supervision' was published?\", \"context\": \"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning , pages 8748–8763. PMLR, 2021. 7\", \"answer\": \"International conference on machine learning\"},\n    {\"question\": \"What were the page numbers of the paper 'Learning transferable visual models from natural language supervision'?\", \"context\": \"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning , pages 8748–8763. PMLR, 2021. 7\", \"answer\": \"8748–8763\"},\n    {\"question\": \"Which organization published the paper 'Learning transferable visual models from natural language supervision'?\", \"context\": \"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning , pages 8748–8763. PMLR, 2021. 7\", \"answer\": \"PMLR\"},\n    {\"question\": \"In what year was the paper 'Learning transferable visual models from natural language supervision' published?\", \"context\": \"Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning , pages 8748–8763. PMLR, 2021. 7\", \"answer\": \"2021\"}\n]\n```"}
{"uuid": "774e7322-1726-41ac-ad7f-c62f0d698765", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the work by Maryam Rahnemoonfar and her co-authors?\", \"context\": \"[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, and Robin Murphy. Floodnet: A high resolution aerial imagery dataset for post flood scene understanding. arXiv preprint arXiv:2012.02951 , 2020. 3, 5\", \"answer\": \"Floodnet: A high resolution aerial imagery dataset for post flood scene understanding\"},\n    {\"question\": \"Where was the Floodnet paper published?\", \"context\": \"[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, and Robin Murphy. Floodnet: A high resolution aerial imagery dataset for post flood scene understanding. arXiv preprint arXiv:2012.02951 , 2020. 3, 5\", \"answer\": \"arXiv preprint arXiv:2012.02951\"},\n    {\"question\": \"Who are the authors of the Laion-400m dataset?\", \"context\": \"[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.\", \"answer\": \"Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki\"},\n    {\"question\": \"What is the name of the dataset described in reference [26]?\", \"context\": \"[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.\", \"answer\": \"Laion-400m\"},\n    {\"question\": \"How many image-text pairs does the Laion-400m dataset contain?\", \"context\": \"[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.\", \"answer\": \"400 million\"},\n    {\"question\": \"What is the main feature of the Laion-400m dataset?\", \"context\": \"[26] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.\", \"answer\": \"clip-filtered 400 million image-text pairs\"},\n    {\"question\": \"Which reference mentions the Floodnet dataset?\", \"context\": \"[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, and Robin Murphy. Floodnet: A high resolution aerial imagery dataset for post flood scene understanding. arXiv preprint arXiv:2012.02951 , 2020. 3, 5\", \"answer\": \"[25]\"},\n    {\"question\": \"When was the Floodnet paper published?\", \"context\": \"[25] Maryam Rahnemoonfar, Tashnim Chowdhury, Argho Sarkar, Debvrat Varshney, Masoud Yari, and Robin Murphy. Floodnet: A high resolution aerial imagery dataset for post flood scene understanding. arXiv preprint arXiv:2012.02951 , 2020. 3, 5\", \"answer\": \"2020\"}\n]\n```"}
{"uuid": "bae3d30e-8386-49ae-80e4-61f9e1036b21", "raw_resp": "```json\n[\n    {\"question\": \"Who are the authors of the paper titled 'Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery'?\", \"context\": \"[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun Fu. Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. IS-PRSS Journal of Photogrammetry and Remote Sensing , 184:116–130, 2022. 5\", \"answer\": \"Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun Fu\"},\n    {\"question\": \"In which journal was the paper 'Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery' published?\", \"context\": \"[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun Fu. Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. IS-PRSS Journal of Photogrammetry and Remote Sensing , 184:116–130, 2022. 5\", \"answer\": \"ISPRS Journal of Photogrammetry and Remote Sensing\"},\n    {\"question\": \"What is the title of the paper published by Xian Sun and colleagues in 2022?\", \"context\": \"[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun Fu. Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. IS-PRSS Journal of Photogrammetry and Remote Sensing , 184:116–130, 2022. 5\", \"answer\": \"Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery\"},\n    {\"question\": \"What is the volume number of the journal where the paper 'Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery' was published?\", \"context\": \"[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun Fu. Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. IS-PRSS Journal of Photogrammetry and Remote Sensing , 184:116–130, 2022. 5\", \"answer\": \"184\"},\n    {\"question\": \"Which year did the paper 'Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery' get published?\", \"context\": \"[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun Fu. Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. IS-PRSS Journal of Photogrammetry and Remote Sensing , 184:116–130, 2022. 5\", \"answer\": \"2022\"},\n    {\"question\": \"What are the page numbers of the publication 'Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery'?\", \"context\": \"[27] Xian Sun, Peijin Wang, Zhiyuan Yan, Feng Xu, Ruiping Wang, Wenhui Diao, Jin Chen, Jihao Li, Yingchao Feng, Tao Xu, Martin Weinmann, Stefan Hinz, Cheng Wang, and Kun Fu. Fair1m: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery. IS-PRSS Journal of Photogrammetry and Remote Sensing , 184:116–130, 2022. 5\", \"answer\": \"116–130\"},\n    {\"question\": \"Who are the authors of the paper 'Learning to rank question answer pairs with holographic dual'?\", \"context\": \"[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui. Learning to rank question answer pairs with holographic dual\", \"answer\": \"Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui\"},\n    {\"question\": \"What is the title of the paper authored by Yi Tay and co-authors?\", \"context\": \"[28] Yi Tay, Minh C Phan, Luu Anh Tuan, and Siu Cheung Hui. Learning to rank question answer pairs with holographic dual\", \"answer\": \"Learning to rank question answer pairs with holographic dual\"}\n]\n```"}
{"uuid": "24283ff2-74d3-43ed-89d6-d15a52483a26", "raw_resp": "```json\n[\n    {\"question\": \"What conference did the lstm architecture paper get published in?\", \"context\": \"lstm architecture. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 695–704. ACM, 2017. 3, 4, 5\", \"answer\": \"The 40th International ACM SIGIR Conference on Research and Development in Information Retrieval\"},\n    {\"question\": \"Which year was the lstm architecture paper published?\", \"context\": \"lstm architecture. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 695–704. ACM, 2017. 3, 4, 5\", \"answer\": \"2017\"},\n    {\"question\": \"Who are the authors of the Mobillama paper?\", \"context\": \"Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 , 2024. 2\", \"answer\": \"Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan\"},\n    {\"question\": \"What is the title of the paper authored by Omkar Thawakar et al.?\", \"context\": \"Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 , 2024. 2\", \"answer\": \"Mobillama: Towards accurate and lightweight fully transparent gpt\"},\n    {\"question\": \"Where was the Mobillama paper published?\", \"context\": \"Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 , 2024. 2\", \"answer\": \"arXiv preprint arXiv:2402.16840\"},\n    {\"question\": \"In what year was the Mobillama paper published?\", \"context\": \"Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 , 2024. 2\", \"answer\": \"2024\"},\n    {\"question\": \"What is the citation number for the lstm architecture paper?\", \"context\": \"lstm architecture. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval , pages 695–704. ACM, 2017. 3, 4, 5\", \"answer\": \"3, 4, 5\"},\n    {\"question\": \"What is the citation number for the Mobillama paper?\", \"context\": \"Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 , 2024. 2\", \"answer\": \"2\"}\n]\n```"}
{"uuid": "dffabc3e-22d7-4d85-aae0-508650d2a0e7", "raw_resp": "```json\n[\n    {\"question\": \"Who are the authors of the paper Xraygpt?\", \"context\": \"lappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using large medical vision-language models. arXiv: 2306.07971 , 2023. 2\", \"answer\": \"The authors of the paper Xraygpt are lappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Faham Shahbaz Khan.\"},\n    {\"question\": \"What is the title of the paper published by arXiv with the identifier 2306.07971?\", \"context\": \"lappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using large medical vision-language models. arXiv: 2306.07971 , 2023. 2\", \"answer\": \"The title of the paper is 'Xraygpt: Chest radiographs summarization using large medical vision-language models'.\"},\n    {\"question\": \"Which authors contributed to the paper titled 'Scaling-up remote sensing segmentation dataset with segment anything model'?\", \"context\": \"[31] Di Wang, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Scaling-up remote sensing segmentation dataset with segment anything model. In arxiv , 2023. 2, 5, 6, 8\", \"answer\": \"The authors who contributed to the paper titled 'Scaling-up remote sensing segmentation dataset with segment anything model' are Di Wang, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang.\"},\n    {\"question\": \"What year was the paper 'Scaling-up remote sensing segmentation dataset with segment anything model' published?\", \"context\": \"[31] Di Wang, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Scaling-up remote sensing segmentation dataset with segment anything model. In arxiv , 2023. 2, 5, 6, 8\", \"answer\": \"The paper 'Scaling-up remote sensing segmentation dataset with segment anything model' was published in 2023.\"},\n    {\"question\": \"Which authors worked on the paper that mentions 'Advancing plain vision'?\", \"context\": \"Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Advancing plain vision\", \"answer\": \"The authors who worked on the paper that mentions 'Advancing plain vision' are Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang.\"},\n    {\"question\": \"How many authors are listed for the paper that includes 'Advancing plain vision'?\", \"context\": \"Di Wang, Qiming Zhang, Yufei Xu, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Advancing plain vision\", \"answer\": \"There are seven authors listed for the paper that includes 'Advancing plain vision'.\"},\n    {\"question\": \"What is the reference number for the paper 'Scaling-up remote sensing segmentation dataset with segment anything model'?\", \"context\": \"[31] Di Wang, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Scaling-up remote sensing segmentation dataset with segment anything model. In arxiv , 2023. 2, 5, 6, 8\", \"answer\": \"The reference number for the paper 'Scaling-up remote sensing segmentation dataset with segment anything model' is [31].\"},\n    {\"question\": \"In which publication venue was the paper 'Scaling-up remote sensing segmentation dataset with segment anything model' presented?\", \"context\": \"[31] Di Wang, Jing Zhang, Bo Du, Dacheng Tao, and Liangpei Zhang. Scaling-up remote sensing segmentation dataset with segment anything model. In arxiv , 2023. 2, 5, 6, 8\", \"answer\": \"The paper 'Scaling-up remote sensing segmentation dataset with segment anything model' was presented in arxiv.\"}\n]\n```"}
{"uuid": "2bd164ad-af46-4f8c-b077-f21b75d1bc59", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the paper referenced by IEEE Transactions on Geoscience and Remote Sensing?\", \"context\": \"transformer toward remote sensing foundation model. IEEE Transactions on Geoscience and Remote Sensing , 61:1–15, 2023. 6\", \"answer\": \"transformer toward remote sensing foundation model\"},\n    {\"question\": \"Who are the authors of the paper 'LoveDA: A remote sensing land-cover dataset for domain adaptive semantic segmentation'?\", \"context\": \"[33] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong. LoveDA: A remote sensing land-cover dataset for domain adaptive semantic segmentation, 2021. 6\", \"answer\": \"Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong\"},\n    {\"question\": \"In which year was the paper 'LoveDA: A remote sensing land-cover dataset for domain adaptive semantic segmentation' published?\", \"context\": \"[33] Junjue Wang, Zhuo Zheng, Ailong Ma, Xiaoyan Lu, and Yanfei Zhong. LoveDA: A remote sensing land-cover dataset for domain adaptive semantic segmentation, 2021. 6\", \"answer\": \"2021\"},\n    {\"question\": \"What is the name of the benchmark data set introduced by Gui-Song Xia and co-authors?\", \"context\": \"[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid: A benchmark data set for performance evaluation of aerial\", \"answer\": \"Aid\"},\n    {\"question\": \"For what purpose is the Aid dataset designed?\", \"context\": \"[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid: A benchmark data set for performance evaluation of aerial\", \"answer\": \"performance evaluation of aerial\"},\n    {\"question\": \"List the authors who contributed to the creation of the Aid dataset.\", \"context\": \"[34] Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu. Aid: A benchmark data set for performance evaluation of aerial\", \"answer\": \"Gui-Song Xia, Jingwen Hu, Fan Hu, Baoguang Shi, Xiang Bai, Yanfei Zhong, Liangpei Zhang, and Xiaoqiang Lu\"},\n    {\"question\": \"Which journal published the paper 'transformer toward remote sensing foundation model'?\", \"context\": \"transformer toward remote sensing foundation model. IEEE Transactions on Geoscience and Remote Sensing , 61:1–15, 2023. 6\", \"answer\": \"IEEE Transactions on Geoscience and Remote Sensing\"},\n    {\"question\": \"In which year was the paper 'transformer toward remote sensing foundation model' published?\", \"context\": \"transformer toward remote sensing foundation model. IEEE Transactions on Geoscience and Remote Sensing , 61:1–15, 2023. 6\", \"answer\": \"2023\"}\n]\n```"}
{"uuid": "3556a521-8749-4771-96b3-fe1f968c2d6b", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the paper referenced by IEEE Transactions on Geoscience and Remote Sensing?\", \"context\": \"scene classification. IEEE Transactions on Geoscience and Remote Sensing , 55(7):3965–3981, 2017. 7\", \"answer\": \"The reference is about scene classification, but the full title of the paper is not provided in the given text.\"},\n    {\"question\": \"Who are the authors of the paper 'Dota: A large-scale dataset for object detection in aerial images'?\", \"context\": \"[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-pei Zhang. Dota: A large-scale dataset for object detection in aerial images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 5\", \"answer\": \"The authors are Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang.\"},\n    {\"question\": \"In which conference was the paper 'Dota: A large-scale dataset for object detection in aerial images' published?\", \"context\": \"[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-pei Zhang. Dota: A large-scale dataset for object detection in aerial images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 5\", \"answer\": \"The paper was published in The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) in 2018.\"},\n    {\"question\": \"Which year did the paper 'Dota: A large-scale dataset for object detection in aerial images' get published?\", \"context\": \"[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-pei Zhang. Dota: A large-scale dataset for object detection in aerial images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 5\", \"answer\": \"The paper was published in 2018.\"},\n    {\"question\": \"What is the name of the dataset mentioned in reference [35]?\", \"context\": \"[35] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Be-longie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liang-pei Zhang. Dota: A large-scale dataset for object detection in aerial images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 5\", \"answer\": \"The name of the dataset is Dota.\"},\n    {\"question\": \"What is the focus of the research described in the IEEE Transactions on Geoscience and Remote Sensing citation?\", \"context\": \"scene classification. IEEE Transactions on Geoscience and Remote Sensing , 55(7):3965–3981, 2017. 7\", \"answer\": \"The research focuses on scene classification.\"},\n    {\"question\": \"What journal published the research on scene classification in 2017?\", \"context\": \"scene classification. IEEE Transactions on Geoscience and Remote Sensing , 55(7):3965–3981, 2017. 7\", \"answer\": \"The research was published in IEEE Transactions on Geoscience and Remote Sensing.\"},\n    {\"question\": \"What is the volume and issue number of the IEEE Transactions on Geoscience and Remote Sensing where the scene classification paper was published?\", \"context\": \"scene classification. IEEE Transactions on Geoscience and Remote Sensing , 55(7):3965–3981, 2017. 7\", \"answer\": \"The paper was published in volume 55, issue 7.\"}\n]\n```"}
{"uuid": "7c42c969-4b9b-4d90-9428-492913954435", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the work referenced by [37]?\", \"context\": \"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.\", \"answer\": \"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions.\"},\n    {\"question\": \"Who are the authors of the work published in Transactions of the Association for Computational Linguistics in 2014?\", \"context\": \"Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.\", \"answer\": \"Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier\"},\n    {\"question\": \"In which journal was the work by Peter Young et al. published?\", \"context\": \"Transactions of the Association for Computational Linguistics, 2:67–78, 2014.\", \"answer\": \"Transactions of the Association for Computational Linguistics\"},\n    {\"question\": \"What are the page numbers for the article by Peter Young et al.?\", \"context\": \"Transactions of the Association for Computational Linguistics, 2:67–78, 2014.\", \"answer\": \"67–78\"},\n    {\"question\": \"When was the work by Peter Young et al. published?\", \"context\": \"Transactions of the Association for Computational Linguistics, 2:67–78, 2014.\", \"answer\": \"2014\"},\n    {\"question\": \"What is the name of the conference where the paper regarding land-use classification was presented?\", \"context\": \"the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270–279, 2010.\", \"answer\": \"the 18th SIGSPATIAL international conference on advances in geographic information systems\"},\n    {\"question\": \"On what pages was the paper about land-use classification presented at the conference?\", \"context\": \"the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270–279, 2010.\", \"answer\": \"pages 270–279\"},\n    {\"question\": \"In which year was the paper on land-use classification presented at the SIGSPATIAL conference?\", \"context\": \"the 18th SIGSPATIAL international conference on advances in geographic information systems, pages 270–279, 2010.\", \"answer\": \"2010\"}\n]\n```"}
{"uuid": "2c17ae39-4bfb-4c8f-ace5-b4e475dd5de2", "raw_resp": "```json\n[\n    {\"question\": \"Who are the authors of 'Modeling context in referring expressions'?\", \"context\": \"Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres-sions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.\", \"answer\": \"The authors are Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg.\"},\n    {\"question\": \"Where was the 14th European Conference on Computer Vision held?\", \"context\": \"Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres-sions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.\", \"answer\": \"The conference was held in Amsterdam, The Netherlands.\"},\n    {\"question\": \"When did the 14th European Conference on Computer Vision take place?\", \"context\": \"Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres-sions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.\", \"answer\": \"The conference took place from October 11-14, 2016.\"},\n    {\"question\": \"What is the title of the paper by Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu?\", \"context\": \"Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu. From easy to hard: Learning language-guided curricu-lum for visual question answering on remote sensing data.\", \"answer\": \"The title of the paper is 'From easy to hard: Learning language-guided curriculum for visual question answering on remote sensing data.'\"},\n    {\"question\": \"Which publisher published the proceedings where 'Modeling context in referring expressions' was included?\", \"context\": \"Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres-sions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.\", \"answer\": \"The proceedings were published by Springer.\"},\n    {\"question\": \"On which pages can 'Modeling context in referring expressions' be found in the proceedings?\", \"context\": \"Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres-sions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.\", \"answer\": \"The paper can be found on pages 69–85.\"},\n    {\"question\": \"In what year was the paper 'Modeling context in referring expressions' published?\", \"context\": \"Licheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expres-sions. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 , pages 69–85. Springer, 2016.\", \"answer\": \"The paper was published in 2016.\"},\n    {\"question\": \"What topic does the paper by Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu address?\", \"context\": \"Zhenghang Yuan, Lichao Mou, Qi Wang, and Xiao Xiang Zhu. From easy to hard: Learning language-guided curricu-lum for visual question answering on remote sensing data.\", \"answer\": \"The paper addresses learning language-guided curriculum for visual question answering on remote sensing data.\"}\n]\n```"}
{"uuid": "4dbd5e8d-b98a-403f-a776-1206374f67ae", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the paper authored by Yang Zhan, Zhitong Xiong, and Yuan Yuan?\", \"context\": \"IEEE Transactions on Geoscience and Remote Sensing , 61:1–13, 2023. 6 [40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring data and models for visual grounding on remote sensing data.\", \"answer\": \"Rsvg: Exploring data and models for visual grounding on remote sensing data.\"},\n    {\"question\": \"In which journal was the paper by Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua Chen, Fang Liu, Yuxuan Li, and Zhicheng Guo published?\", \"context\": \"[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua Chen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-tial hierarchical reasoning network for remote sensing visual question answering. IEEE Transactions on Geoscience and\", \"answer\": \"IEEE Transactions on Geoscience and Remote Sensing\"},\n    {\"question\": \"What is the name of the network developed by Zixiao Zhang et al. for remote sensing visual question answering?\", \"answer\": \"A spatial hierarchical reasoning network\"},\n    {\"question\": \"Which year did the paper 'Rsvg: Exploring data and models for visual grounding on remote sensing data' get published?\", \"context\": \"IEEE Transactions on Geoscience and Remote Sensing , 61:1–13, 2023. 6 [40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring data and models for visual grounding on remote sensing data.\", \"answer\": \"2023\"},\n    {\"question\": \"How many authors are listed for the paper 'A spatial hierarchical reasoning network for remote sensing visual question answering'?\", \"context\": \"[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua Chen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-tial hierarchical reasoning network for remote sensing visual question answering. IEEE Transactions on Geoscience and\", \"answer\": \"8\"},\n    {\"question\": \"What is the page range of the paper 'Rsvg: Exploring data and models for visual grounding on remote sensing data'?\", \"context\": \"IEEE Transactions on Geoscience and Remote Sensing , 61:1–13, 2023. 6 [40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring data and models for visual grounding on remote sensing data.\", \"answer\": \"1–13\"},\n    {\"question\": \"What volume number is associated with the publication of the paper 'Rsvg: Exploring data and models for visual grounding on remote sensing data'?\", \"context\": \"IEEE Transactions on Geoscience and Remote Sensing , 61:1–13, 2023. 6 [40] Yang Zhan, Zhitong Xiong, and Yuan Yuan. Rsvg: Exploring data and models for visual grounding on remote sensing data.\", \"answer\": \"61\"},\n    {\"question\": \"What is the main focus of the research described in the paper by Zixiao Zhang et al.?\", \"context\": \"[41] Zixiao Zhang, Licheng Jiao, Lingling Li, Xu Liu, Puhua Chen, Fang Liu, Yuxuan Li, and Zhicheng Guo. A spa-tial hierarchical reasoning network for remote sensing visual question answering. IEEE Transactions on Geoscience and\", \"answer\": \"remote sensing visual question answering\"}\n]\n```"}
{"uuid": "abd8593c-5af6-4e20-92c0-1eb2337190d2", "raw_resp": "```json\n[\n    {\"question\": \"What is the title of the paper authored by Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny?\", \"context\": \"[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023. 2\", \"answer\": \"Minigpt-4: Enhancing vision-language understanding with advanced large language models\"},\n    {\"question\": \"In which journal was the paper 'Transforming remote sensing images to textual descriptions' published?\", \"context\": \"[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transforming remote sensing images to textual descriptions. International Journal of Applied Earth Observation and Geoinformation , 108:102741, 2022. 2\", \"answer\": \"International Journal of Applied Earth Observation and Geoinformation\"},\n    {\"question\": \"Who are the authors of the paper 'Transforming remote sensing images to textual descriptions'?\", \"context\": \"[43] Usman Zia, M Mohsin Riaz, and Abdul Ghafoor. Transforming remote sensing images to textual descriptions. International Journal of Applied Earth Observation and Geoinformation , 108:102741, 2022. 2\", \"answer\": \"Usman Zia, M Mohsin Riaz, and Abdul Ghafoor\"},\n    {\"question\": \"What is the publication year of the paper 'Minigpt-4: Enhancing vision-language understanding with advanced large language models'?\", \"context\": \"[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023. 2\", \"answer\": \"2023\"},\n    {\"question\": \"Where was the paper 'Minigpt-4: Enhancing vision-language understanding with advanced large language models' published?\", \"context\": \"[42] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592 , 2023. 2\", \"answer\": \"arXiv preprint arXiv:2304.10592\"},\n    {\"question\": \"What is the volume number of the Remote Sensing journal mentioned in the text?\", \"context\": \"Remote Sensing , 61:1–15, 2023. 2, 7\", \"answer\": \"61\"},\n    {\"question\": \"What are the page numbers of the article in the Remote Sensing journal?\", \"context\": \"Remote Sensing , 61:1–15, 2023. 2, 7\", \"answer\": \"1–15\"},\n    {\"question\": \"In which year was the article published in the Remote Sensing journal?\", \"context\": \"Remote Sensing , 61:1–15, 2023. 2, 7\", \"answer\": \"2023\"}\n]\n```"}
